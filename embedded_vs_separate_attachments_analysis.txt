Embedded vs. Separate Saved Objects for Case Attachments
=========================================================

Analysis: Should case attachments be stored as an embedded array on the core case saved object, or as separate saved objects?


CONTEXT
-------

There are three approaches in play:

  1. Legacy (case-comments) - Separate saved object linked via references. Being deprecated.
  2. Embedded (current branch) - attachments[] array on the cases saved object. In progress (model v9).
  3. Separate v2 (case-attachments) - New standalone saved object with flexible schema. Defined but unused.

The current branch embeds attachments directly into the case document. This analysis evaluates whether that is the right approach versus using the already-defined case-attachments separate saved object.

All three approaches target the same backing index: .kibana_alerting_cases

This analysis also considers a variant of option 2 where the embedded attachments field uses the Elasticsearch "nested" mapping type with all attachment fields fully indexed, rather than dynamic: false with only id and type indexed.


WHAT IS THE CASES FEATURE?
--------------------------

Cases in Kibana are tools for opening and tracking issues directly within the platform. They function as issue management containers, allowing teams to systematize and monitor problems in one centralized location.

Cases support:
  - Metadata management: owners, tags, severity levels, status values, assignees
  - Attachments: alerts, user comments, files, visualizations, actions, external references
  - Custom fields and case templates
  - Human-readable incremental IDs
  - External connector integrations (Jira, ServiceNow, IBM Resilient, webhooks)
  - Space-level isolation (cases in one Kibana space are invisible to other spaces)

The Cases API supports creating, updating, deleting, searching, and pushing cases to external systems. Attachments (comments, alerts, files) are managed through dedicated sub-APIs on each case.


KEY LIMITS DEFINED IN THE CODEBASE
-----------------------------------

These constants from x-pack/platform/plugins/shared/cases/common/constants/index.ts bound the scale:

  MAX_ALERTS_PER_CASE:                              1,000
  MAX_USER_ACTIONS_PER_CASE:                       10,000
  MAX_COMMENT_LENGTH:                              30,000 characters
  MAX_DESCRIPTION_LENGTH:                          30,000 characters
  MAX_PERSISTABLE_STATE_AND_EXTERNAL_REFERENCES:      100
  MAX_FILES_PER_CASE:                                 100
  MAX_FILE_SIZE:                                   100 MB
  MAX_OBSERVABLES_PER_CASE:                            50
  MAX_CUSTOM_FIELDS_PER_CASE:                          10
  MAX_BULK_CREATE_ATTACHMENTS:                        100
  MAX_BULK_GET_ATTACHMENTS:                           100
  MAX_DOCS_PER_PAGE:                               10,000


HOW IT WORKS TODAY (LEGACY: case-comments)
------------------------------------------

The legacy approach stores each attachment as a separate saved object of type "case-comments", linked to its parent case via the saved object references array. The case saved object maintains denormalized counters (total_comments, total_alerts, total_events) that must be updated separately when attachments change.

Query patterns include:
  - Finding comments by case: hasReference filter on the case-comments type
  - Finding cases by alert: aggregation across case-comments to extract parent case IDs from references
  - Counting attachments: separate aggregation queries against case-comments

This works but requires multi-step writes (create comment + update counter) and cross-document joins for reverse lookups (find case by alert ID).


HOW THE EMBEDDED APPROACH WORKS (current branch)
-------------------------------------------------

Model version 9 adds an "attachments" field to the case saved object as an embedded array. Each attachment is an object with an id field plus all standard attachment attributes (type, comment, alertId, created_at, created_by, etc.).

The mapping currently uses dynamic: false with only id and type indexed:

  attachments: {
    dynamic: false,
    properties: {
      id: { type: 'keyword' },
      type: { type: 'keyword' },
    },
  }

The AttachmentService reads the case document, extracts the embedded array, and performs all filtering, sorting, and pagination in application memory. CRUD operations follow a read-modify-write pattern: read the case, modify the attachments array, write the case back with optimistic concurrency (version check).

Stats (total_comments, total_alerts, total_events) are recomputed from the full array on every write and persisted atomically alongside the attachments.

References belonging to attachments are stored on the case document's references array using a namespaced convention: attachment:{attachmentId}:{originalRefName}.


PROS OF EMBEDDING ATTACHMENTS ON THE CASE SAVED OBJECT
-------------------------------------------------------

1. Atomic Operations (Strong Consistency)

This is the single biggest advantage. A single unsecuredSavedObjectsClient.update() call writes both the attachment and updated stats (total_comments, total_alerts, total_events) atomically.

With separate saved objects, adding an attachment requires:
  - Creating the attachment saved object
  - Updating the case saved object counters
  - Hoping no crash occurs between steps 1 and 2

This eliminates an entire class of data consistency bugs where counters drift from actual attachment counts, a problem that likely exists with the legacy case-comments approach.


2. Reduced Query Overhead (Single Read)

getCaseWithAttachments() fetches a case and all its attachments in one Elasticsearch GET. With separate saved objects, this requires at minimum a get + a find (with hasReference filter), which is 2 round trips to Elasticsearch. For pages listing multiple cases with attachment counts, this difference multiplies.


3. Simplified Reference Management

The namespaced reference pattern (attachment:{id}:{refName}) keeps all references on a single document. Import/export of a case automatically includes its attachments with no need for multi-type traversal that the legacy approach requires.


4. Elimination of Cross-Document Joins

The find() method does filtering, sorting, and pagination entirely in-memory on the embedded array. This avoids the complex aggregation-based joins visible in the legacy getCaseIdsByAlertId() pattern, which had to aggregate across case-comments references to discover parent case IDs.


5. Optimistic Concurrency is Cleaner

With everything on one document, the version parameter on updates provides serialized writes naturally. The bulkDelete method correctly uses version: caseSO.version to prevent lost updates. With separate saved objects, you would need distributed coordination across multiple documents.


CONS OF EMBEDDING ATTACHMENTS ON THE CASE SAVED OBJECT
-------------------------------------------------------

1. Document Size Growth (Critical Risk)

This is the most significant concern.

Worst-case scenario: A case with 1,000 alert attachments (each containing alertId, index, rule, metadata), plus user comments, plus external references.

Conservative estimate:
  - 1,000 alerts x ~500 bytes each = ~500 KB
  - User comments (50 x 10KB average) = ~500 KB
  - Case base fields = ~5 KB
  - Total: ~1 MB+ for a heavily-used case

Elasticsearch's default http.max_content_length is 100 MB, so hard limits are unlikely to be hit. However:
  - Every single attachment operation rewrites the entire document. Adding the 1,000th alert re-serializes and re-indexes all 999 previous alerts plus the full case.
  - Indexing latency grows with document size. A 1 MB document takes meaningfully longer to index than a 5 KB one.
  - The Kibana saved objects framework has no document-level size validation. The first indication of a problem would be a mysterious Elasticsearch rejection.


2. Write Contention (Hot Document Problem)

All attachment operations serialize on a single document. The create method does read-modify-write with optimistic concurrency:

  read case -> append attachment -> write case (with version check)

If two users simultaneously add comments to the same case, one will get a version conflict. The saved objects framework retries automatically (with exponential backoff, maxTimeout: 3000, retries: 3, factor: 2), but under load (e.g., an automation bulk-attaching 1,000 alerts), this becomes a hot document bottleneck. Each retry requires re-reading the increasingly large document.

With separate saved objects, parallel writes create independent documents with no contention. Counter updates on the case saved object still contend, but the blast radius is a small integer update, not a full document rewrite.


3. In-Memory Filtering Does Not Scale

The find() implementation loads the entire attachment array into Node.js heap, then does filtering, sorting, and pagination in application memory. For a case with 1,000+ attachments, you are deserializing and iterating a large array on every page request.

With separate saved objects, Elasticsearch handles filtering, sorting, and pagination natively at the shard level, which is vastly more efficient.


4. No Independent Indexing of Attachment Fields (dynamic: false variant)

With the current mapping (dynamic: false, only id and type indexed), you cannot use Elasticsearch to:
  - Full-text search across comment bodies within a case
  - Filter attachments by created_at date ranges at the Elasticsearch level
  - Query by alertId or eventId without loading the entire document
  - Aggregate attachment statistics across multiple cases efficiently

With the separate case-attachments saved object, every indexed field (type, attachmentId, data.content as text, metadata.actionType, created_at, etc.) is independently queryable.


5. ES|QL Cannot Help with Embedded Arrays

ES|QL is designed for columnar analytics over indexed fields. Since the embedded attachment data is stored with dynamic: false (most fields not indexed), ES|QL queries against .kibana_alerting_cases cannot access attachment content. You would only be able to query the two indexed fields (attachments.id and attachments.type).

With separate saved objects, ES|QL becomes powerful. For example:

  FROM .kibana_alerting_cases
  | WHERE type == "case-attachments" AND case-attachments.created_at > NOW() - 7 days
  | STATS count = COUNT(*) BY case-attachments.type

This kind of cross-case attachment analytics (e.g., "how many alerts were attached across all cases this week?") is impossible with the embedded approach without loading every case document.


6. Migration Complexity for Existing Data

Migrating existing case-comments data into the embedded attachments array requires data reshaping (moving data from N separate documents into 1 parent document). This is more complex and riskier than a document-to-document transformation. If a case has 500 comments, the migration must:
  - Find all comments for that case
  - Build the embedded array
  - Write the enlarged case document
  - Clean up the old comment documents

Failure partway through leaves data in an inconsistent state. With a separate case-attachments saved object, migration is a simpler 1:1 document transformation.


7. Import/Export Payload Size

The saved objects import/export API serializes entire documents. A case with hundreds of embedded attachments produces a single large NDJSON line, which can cause memory pressure during import. The separate approach distributes this across many small lines.


WHAT IF ATTACHMENTS USED THE NESTED FIELD TYPE WITH FULL INDEXING?
-------------------------------------------------------------------

A natural follow-up question: if the embedded attachments field used the Elasticsearch "nested" mapping type with all attachment fields fully indexed (not just id and type), does that change the analysis?

This directly addresses Cons #3 and #4. The answer is: it helps meaningfully in one area, introduces a new blocker, and leaves the core problems unchanged.


What improves:

  Con #4 (No ES-Level Querying) is FIXED. Making attachments a nested type with all fields indexed enables real Elasticsearch-level queries. The saved objects find() API already supports nested filters. The cases plugin uses this pattern today for customFields and observables, constructing queries like:

    nested: {
      path: 'cases.observables',
      query: { term: { 'cases.observables.value': { value: search, case_insensitive: true } } }
    }

  Cross-case queries like "find all cases that have an alert attachment matching alertId X" become real ES nested queries instead of requiring cross-saved-object aggregation joins. This is a genuine improvement.


What is only partially fixed:

  Con #3 (In-Memory Filtering) is PARTIALLY FIXED. For cross-case queries (find cases matching attachment criteria), nested queries handle this at the Elasticsearch level.

  But for the primary UI use case, "show me page 3 of comments on case X", the situation is unchanged. The saved objects find() API paginates at the document level, not within nested arrays. There is no inner_hits support in the saved objects client. Paginating within a single case's attachment array still means:
    1. Load the entire case document (with all 1,000+ attachments)
    2. Filter, sort, and paginate in Node.js memory

  This is the exact same pattern the current embedded find() implementation uses. Elasticsearch does not offer a way to paginate the inner documents of a single parent independently through the standard search API.


What gets worse:

  Con #5 (ES|QL) becomes WORSE, not better. ES|QL completely does not support the nested field type. Per the official Elasticsearch limitations documentation and confirmed by open issue elastic/elasticsearch#107434:

    - Nested fields are completely invisible to ES|QL, not returned as null, not accessible at all
    - ES|QL behaves as if nested fields do not exist on the document
    - There is no timeline or milestone for adding nested support
    - The proposed EXPAND command to denormalize nested fields has no assignee or target release

  Switching from dynamic: false to nested actually eliminates the two fields (id and type) that ES|QL can currently see on the embedded array, because wrapping them in a nested type makes them invisible to ES|QL.

  The only documented workaround is include_in_root: true on the nested mapping, which duplicates attachment data as flat fields on the parent document. This doubles index storage for attachment data, loses field correlation (you cannot tell which alertId belongs to which created_at), and cannot be changed on an existing mapping without reindexing.

  Sources:
    - ES|QL limitations: https://www.elastic.co/guide/en/elasticsearch/reference/8.19/esql-limitations.html
    - Feature request (open, no milestone): https://github.com/elastic/elasticsearch/issues/107434
    - Nested fields not reported as unsupported: https://github.com/elastic/elasticsearch/issues/116413


New concerns introduced by nested:

  Lucene document explosion: Each nested object is internally stored as a separate hidden Lucene document. A case with 1,000 alert attachments creates 1,000+ hidden Lucene documents that must all be re-indexed on every case update. This increases index size and makes writes more expensive than the dynamic: false variant.

  Shared nested object limit: Elasticsearch defaults to index.mapping.nested_objects.limit = 10,000 nested objects per document, shared across ALL nested fields on the same document. The case saved object already uses nested for customFields and observables. With 1,000 alert attachments, the budget is consumed primarily by attachments, leaving less room for other nested fields if limits are tightened.

  Wider mapping: Since attachments are polymorphic (user comments, alerts, actions, external references, persistable state all have different fields), the nested mapping must be a union of all possible fields. This creates a wide mapping that grows with each new attachment type.


What stays the same:

  - Document size growth: Unchanged. The full document is still rewritten on every attachment operation.
  - Write contention: Unchanged or worse. The hot document problem remains, and nested re-indexing adds overhead.
  - Migration complexity: Unchanged. Still requires reshaping N documents into 1.
  - Import/export payload size: Unchanged. A case with hundreds of attachments is still one large NDJSON line.


Revised comparison (nested embedded vs. separate SOs):

  Dimension                                      | Nested Embedded          | Separate SOs
  -----------------------------------------------|--------------------------|----------------------------
  Cross-case queries (find cases with alert X)   | Nested query (good)      | Top-level query (simpler)
  Single-case pagination (page 3 of comments)    | Still in-memory (bad)    | ES-native pagination (good)
  ES|QL analytics                                | Broken (nested invisible)| Fully viable
  Write cost per attachment                      | Rewrite full doc + N nested Lucene docs | Create 1 small doc
  Index storage overhead                         | 1 parent + N hidden Lucene docs | N+1 independent docs


Bottom line on the nested variant: The nested type solves the "can't query attachment fields" problem for Query DSL, but it kills ES|QL compatibility entirely (which is worse than dynamic: false), does not fix the primary pagination use case, and makes writes more expensive. The fundamental mismatch is that nested fields are designed for querying parents by child criteria ("find cases that contain alert X"), not for independently paginating and managing child entities ("show me page 3 of attachments on case Y"). Case attachments are the latter pattern.


COMPARISON MATRIX (ALL THREE VARIANTS)
--------------------------------------

Dimension                       | Embedded (dynamic:false) | Embedded (nested+indexed) | Separate case-attachments SO
--------------------------------|--------------------------|---------------------------|-------------------------------
Read: single case + attachments | 1 ES read                | 1 ES read                 | 2+ ES reads (get + find)
Read: case list page            | N reads (one per case)   | N reads (one per case)    | N + N reads (cases + stats)
Write: add attachment           | Read-modify-write full doc| Read-modify-write full doc| Create independent doc + counter
Write contention                | High (hot document)      | Higher (nested reindex)   | Low (independent docs)
Consistency                     | Atomic                   | Atomic                    | Eventually consistent (2-step)
Document size                   | Grows with attachments   | Grows with attachments    | Fixed small size per doc
ES Query DSL search             | Only id and type         | Full nested query support | Full top-level query support
ES|QL analytics                 | id and type only         | Completely broken         | Fully viable
In-memory cost per request      | O(all attachments)       | O(all attachments)*       | O(page size)
Migration from legacy           | Complex (reshape)        | Complex (reshape)         | Simpler (rename/remap)
Max attachments per case        | ~thousands (doc size)    | ~thousands (doc size)     | ~millions (index size)
Lucene doc overhead per case    | 1 document               | 1 + N hidden documents    | N + 1 documents
Import/export                   | One large NDJSON line    | One large NDJSON line     | Many small NDJSON lines

* Pagination of attachments within a single case still requires loading the full document even with nested, because the saved objects API does not support inner_hits.


SUGGESTED SOLUTION: HYBRID APPROACH
------------------------------------

Use separate case-attachments saved objects as the primary store, but keep denormalized counters and a lightweight reference array on the case saved object.


What to keep on the case saved object (already present):

  - total_comments, total_alerts, total_events, total_observables: Denormalized counters for fast case list rendering without querying attachments at all.
  - attachments field with only id and type (current mapping, non-nested, dynamic: false): Useful for quick existence checks and type-based filtering without a second query. Remains visible to ES|QL for basic analytics.


What to store in separate case-attachments saved objects:

  - Full attachment data using the already-defined case-attachments type with its full field indexing.
  - Link to the parent case via saved object references (standard Kibana pattern).
  - Full-text search on data.content, date filtering on created_at, alert ID lookups: all handled natively by Elasticsearch at the shard level.
  - Independent pagination: "page 3 of comments on case X" is a standard saved objects find() call with page and perPage.


Counter consistency:

  - Update counters on the case saved object when attachments are created or deleted.
  - If counters drift (crash between steps), a background reconciliation task or on-read repair can fix them. This is the standard pattern used in distributed systems and is already partially implemented via the existing computeStats() method.
  - The lightweight attachments array (id + type only) on the case SO can serve as the reconciliation source: compare its length/composition against actual case-attachments SOs to detect and repair drift.


ES|QL benefits preserved:

  - Cross-case analytics become possible since each attachment is an independent indexed document with top-level fields (not nested).
  - Example: "Find all cases that received alert attachments in the last 24 hours" becomes a straightforward ES|QL query.
  - The lightweight attachments summary array on the case SO also remains visible to ES|QL (because it uses dynamic: false, not nested).


Migration path:

  - New attachments go to the case-attachments saved object.
  - Existing case-comments migrate 1:1 to case-attachments (simple field remapping).
  - No complex document reshaping required.
  - The lightweight attachments summary array on the case SO can be populated lazily or via a background migration.


When embedding still makes sense:

  If the total number of attachments per case is guaranteed to stay small (say, fewer than 50), and write contention is not a concern, embedding is simpler. But given that MAX_ALERTS_PER_CASE = 1,000 and real-world security cases can accumulate hundreds of comments, the separate saved object approach is safer for production scale.


SUMMARY
-------

Embedding gives you transactional consistency and simpler reads at the cost of document bloat, write contention, loss of Elasticsearch-level queryability, and inability to leverage ES|QL. Using the nested field type with full indexing partially addresses the queryability problem for Query DSL but makes ES|QL compatibility worse (nested fields are completely invisible to ES|QL with no support timeline), does not fix single-case attachment pagination, and increases write costs due to hidden Lucene document overhead.

For a system where cases can accumulate 1,000+ attachments and multiple users and automations write concurrently, separate saved objects with denormalized counters is the more scalable and operationally safer architecture.

The case-attachments saved object type already defined in the codebase is well-suited for this. Use it as the source of truth, and keep the lightweight attachments array on the case saved object as an optimization index rather than the canonical store. This preserves atomic-read benefits for the case list page, maintains ES|QL compatibility, enables Elasticsearch-native pagination and search on attachments, and provides a clean migration path from the legacy case-comments type.
