---
title: "Now available: The LLM safety assessment"
slug: "elastic-security-labs-releases-llm-safety-report"
date: "2023-05-06"
description: "Check out the newest report from Elastic Security Labs, which explores how you can protect your organization from LLM threats."
author:
  - slug: devon-kerr
image: "image1.png"
category:
  - slug: reports
tags:
  - slug: machine-learning
  - slug: generative-ai
---

Today Elastic Security Labs publishes our [LLM safety assessment report](https://www.elastic.co/security/llm-safety-report?utm_source=labshome), a research endeavor meant to collect and clarify information about practical threats to large language models. These forms of generative AI are likely to become ubiquitous in the near future-- but we need to consider the security of them __a little sooner__ than that.

One of the most immediate and significant challenges-- and this is true of every new data source-- is understanding the properties and characteristics of the data, if it exists. You can read more about that process in this [excellent](https://www.elastic.co/security-labs/embedding-security-in-llm-workflows) [pair](https://www.elastic.co/security-labs/elastic-advances-llm-security) of articles, which speak to a challenge many detection engineers are facing today.

New data sources are problematic in a unique way: with no visibility to rank malicious techniques by popularity, how does a detection engineer determine the most effective detections? Mapping fields and normalizing a data source is a good __initial__ step that makes it possible to begin investigating; it's exciting to be a little closer to the answer today than we were yesterday.

Check out the new report, browse our [prior research](https://www.elastic.co/security-labs/topics/generative-ai) on this topic, and join us in preparing for tomorrow.