name: "security.alert.validation"
enabled: true
description: "Security alert validation workflow. Author: Elastic"
version: "1"
tags:
  - security
  - alert_validation
triggers:
  - type: alert
consts:
  workflow_version: "v0.0.1" #Workflow version
  override_previous: true #When false, exits early if alert tags already exist 
  auto_close_enabled: true #Automatically close alerts exceeding FP threshold (default: false)
  auto_close_score_min_threshold: 0.1 #Minimum score for auto-close (0-100)
  auto_close_score_max_threshold: 0.39 #Maximum score for auto-close (0-100)
  auto_close_confidence_min_threshold: 0.85 #Minimum confidence for auto-close (0-1)
  auto_close_confidence_max_threshold: 1.0 #Maximum confidence for auto-close (0-1)
  closed_tag_suffix: "closed-by-agent" #Suffix for auto-closed false positives tag
steps:
- name: set_workflow_variabless
  type: data.set
  with:
    running_note: false
    normalized_name: "{{ workflow.name | downcase | replace: ' ', '_' }}"
    normalized_version: "{{ consts.workflow_version | downcase | replace: ' ', '_' }}"
    tag_prefix: "workflow.{{ workflow.name | downcase | replace: ' ', '_' }}"
- name: set_workflow_tags
  type: data.set
  with:
    workflow_tag: "{{ variables.tag_prefix }}"
    workflow_version_tag: "{{ variables.tag_prefix }}.version.{{ variables.normalized_version }}"
- name: loop_over_results
  type: foreach
  foreach: "{{ event.alerts | json }}"
  steps:
    - name: set_alert_id
      type: data.set
      with:
        alert_id: "{{ foreach.item._id }}"
        alert_index: "{{ foreach.item._index }}"
        rule_id: "{{event.rule.id}}"
    - name: build_already_validated_expr
      type: data.set
      with:
        already_validated_expr: 'tag == "{{ variables.workflow_tag }}"'
    - name: build_already_validated_condition
      type: data.set
      with:
        already_validated_condition: "${{ foreach.item.kibana.alert.workflow_tags | has_exp: 'tag', variables.already_validated_expr }}"
    - name: already_validated
      type: if
      condition: "${{ variables.already_validated_condition and consts.override_previous == false }}"
      steps:
        - name: log_already_validated
          type: console
          with:
            message: "Alert {{ variables.alert_id }} has already been validated. Please remove the \"workflow.security.alert.validation\" tag to re-run."
      else:
        - name: add_execution_note_to_alert
          type: kibana.request
          with:
            method: PATCH
            path: /api/note
            body:
              note:
                eventId: '{{ variables.alert_id }}'
                note: | 
                  ⏳ [{{workflow.name}}:{{execution.id}}]({{execution.url}})

                  ---
                timelineId: ''
            headers:
              kbn-xsrf: string
              Content-Type: application/json
        - name: set_note_id
          type: data.set
          with:
            note_id: "{{ steps.add_execution_note_to_alert.output.note.noteId }}"

        - name: security_getCloseHistory_step
          type: security.getCloseHistory
          with:
            ruleId: "{{variables.rule_id}}"
        - name: security_getGlobalPrevalence_step
          type: security.getGlobalPrevalence
          with:
            ruleId: "{{variables.rule_id}}"
        - name: security_getRelatedAlerts_step
          type: security.getRelatedAlerts
          with:
            alertId: "{{variables.alert_id}}"
            alertIndex: "{{variables.alert_index}}"
        - name: security_getRuleFireCount_step
          type: security.getRuleFireCount
          with:
            ruleId: "{{variables.rule_id}}"
        - name: security_getRuleMetadata_step
          type: security.getRuleMetadata
          with:
            ruleId: "{{variables.rule_id}}"
        - name: prepare_relatedAlerts_step
          type: data.map
          items: "${{steps.security_getRelatedAlerts_step.output.related_alerts}}"
          with:
            fields:
              severity: "${{ item.severity }}"
              rule_name: "${{ item.rule_name }}"
              alert_id: "${{ item.alert_id }}"
              related_by_entity_type: "${{ item.related_by_entity_type | join: '|' }}"
              alert_index: "${{ item.alert_index }}"
              timestamp: "${{ item.timestamp }}"

        - name: security_toonEncode_step
          type: security.toonEncode
          with: 
            data: "${{steps.prepare_relatedAlerts_step.output}}"
        - name: onechat_runAgent_step
          type: agentBuilder.runAgent
          agent_id: "elastic-ai-agent"
          with:
            message: |
              <role>
              You are a security analyst reasoning over a detection alert. Determine whether the alert is a true positive or a false positive.

              You may use tools to gather more context about this alert. Your verdict must be defensible from literal alert fields, the gathered context and general security knowledge. Do not invent facts; identify what is present, what is missing, and what matters.
              </role>

              <operational_constraints>
              Output only the required schema. Do not ask clarifying questions. Reflect uncertainty via confidence_score. Do not fabricate command syntax, tools, or unseen file paths.

              Missing Critical Fields reduce confidence only and must not change the classification.
              </operational_constraints>

              <verdict_semantics>
              true_positive: Observable behavior demonstrates or closely simulates expected rule firing expectations. Requires positive indicators—proved harm, confirmed attack techniques, or corroborated events that cohere into rule logic.
              false_positive: Observable behavior is consistent with routine legitimate operations and does not meet the expected rule firing expectations. When context is thin, confidence is lower but the classification remains likely_false_positive if the behavior itself is routine.
              inconclusive: Is is not possible to determine the classification of the alert based on the available information.

              </verdict_semantics>

              <epistemic_framework>
              Apply hypothesis testing: weigh observable evidence against competing explanations.

              Use available tools to gather required context such as details about related alerts or other events.

              Missing context lowers confidence but must not change classification.
              </epistemic_framework>

              <reasoning_process>

              1. Gather Context via search tools.
              2. Characterize the Event: Describe who did what, where, when, and how using literal fields. Explain why the rule fired.
              3. Establish Baseline & Evidence:
              4. Expected behavior (generic + historical)
              5. False positive anchors (observable routine behavior)
              6. Observable anomalies (must be present, not inferred)
              7. Missing critical fields (confidence impact only)
              8. Evaluate Hypotheses:

              Assess Attack Chain:

              Complete, Partial, or Absent based on observable evidence.

              Apply Decision Gates:
              Gate A (Proved Harm) → Gate B (Strong TP indicators) → Gate C (Routine ops) → Gate D (Insufficient evidence).

              Produce Rationale:
              Cite applied gate, key evidence, anomalies, missing fields, and confidence justification.
              </reasoning_process>

              ## Preliminary Context

              ### Global Prevalance
              {{steps.security_getGlobalPrevalence_step.output.message}}

              Total alerts: {{steps.security_getGlobalPrevalence_step.output.total_alerts}}
              Unique hosts: {{steps.security_getGlobalPrevalence_step.output.unique_hosts}}
              Unique users: {{steps.security_getGlobalPrevalence_step.output.unique_users}}

              {% if steps.security_getGlobalPrevalence_step.output.top_hosts and steps.security_getGlobalPrevalence_step.output.top_hosts.size > 0 -%}
              ##### Top Hosts
              {%- for host in steps.security_getGlobalPrevalence_step.output.top_hosts %}
              - {{ host.host_name }}: {{host.alert_count}} alerts
              {%- endfor %}
              {%- endif %}

              ### Related Alerts
              {{steps.security_getRelatedAlerts_step.output.message}}

              {{steps.security_toonEncode_step.output.toon}}

              ### Rule: {{steps.security_getRuleMetadata_step.output.metadata.rule_name}}
              {{steps.security_getRuleMetadata_step.output.metadata.rule_description}}
              {% capture metadata %}
              {% if steps.security_getRuleMetadata_step.output.metadata.severity -%}
              Severity: {{ steps.security_getRuleMetadata_step.output.metadata.severity }}
              {%- endif %}
              {% if steps.security_getRuleMetadata_step.output.metadata.rule_type -%}
              Type: {{ steps.security_getRuleMetadata_step.output.metadata.rule_type }}
              {%- endif %}
              {% if steps.security_getRuleMetadata_step.output.metadata.rule_id -%}
              Id: {{ steps.security_getRuleMetadata_step.output.metadata.rule_id }}
              {%- endif %}
              {% if steps.security_getRuleMetadata_step.output.metadata.rule_uuid -%}
              UUID: {{ steps.security_getRuleMetadata_step.output.metadata.rule_uuid }}
              {%- endif %}
              {% if steps.security_getRuleMetadata_step.output.metadata.threat_framework -%}
              Threat framework: {{ steps.security_getRuleMetadata_step.output.metadata.threat_framework }}
              {%- endif %}
              {% if steps.security_getRuleMetadata_step.output.metadata.threat_tactic and steps.security_getRuleMetadata_step.output.metadata.threat_tactic.name -%}
              Tactic: {{ steps.security_getRuleMetadata_step.output.metadata.threat_tactic.name }}
              {%- endif %}
              {% if steps.security_getRuleMetadata_step.output.metadata.threat_technique and steps.security_getRuleMetadata_step.output.metadata.threat_technique.name -%}
              Technique: {{ steps.security_getRuleMetadata_step.output.metadata.threat_technique.name }}
              {%- endif %}
              {% endcapture %}
              {{ metadata | strip }}

              ### Rule Fire Count
              {{steps.security_getRuleFireCount_step.output.message}}

            attachments:
              - type: "security.alert"
                data:
                  alert: "{{ foreach.item | json:2 }}"
            schema: | 
              {
                "type": "object",
                "properties": {
                  "classification": {
                    "type": "string",
                    "enum": ["false_positive", "true_positive", "inconclusive"],
                    "description": "Classification of the alert based on analysis"
                  },
                  "confidence_score": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 100,
                    "description": "Confidence score from 0-100 indicating certainty of the classification"
                  },
                  "rationale": {
                    "type": "string",
                    "description": "Human-readable explanation of the classification decision"
                  },
                  "contributing_factors": {
                    "type": "array",
                    "items": {
                      "type": "string"
                    },
                    "description": "List of factors that contributed to the classification decision"
                  },
                  "related_alert_ids": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "alert_id": {
                          "type": "string",
                          "description": "The alert ID"
                        },
                        "alert_index": {
                          "type": "string",
                          "description": "The alert index"
                        },
                        "related_by_entity_type": {
                          "type": "array",
                          "items": {
                            "type": "string",
                            "enum": ["host", "user", "service"]
                          },
                          "description": "Array of entity types that caused this alert to be related"
                        }
                      },
                      "required": ["alert_id", "alert_index", "related_by_entity_type"]
                    },
                    "description": "Array of alerts that are related to the same attach chain as the original attachment"
                  }
                },
                "required": [
                  "classification",
                  "confidence_score",
                  "rationale",
                  "contributing_factors",
                  "related_alert_ids"
                ]
              }
        - name: check_if_output_exists
          type: if
          condition: "not steps.onechat_runAgent_step.output.classification:*"
          steps:
            - name: add_no_data_note_to_alert
              type: kibana.request
              with:
                method: PATCH
                path: /api/note
                body:
                  noteId: "{{variables.note_id}}"
                  note:
                    eventId: '{{ variables.alert_id }}'
                    timelineId: ''
                    note: | 
                      ❌ [{{workflow.name}}:{{execution.id}}]({{execution.url}})

                      ---
                      Error: No classification returned.
            - name: fail_workflow
              type: http
              with:
                method: GET
                url: "---trigger-error---" 
        - name: add_verdict_note_to_alert
          type: kibana.request
          with:
            method: PATCH
            path: /api/note
            body:
              noteId: "{{variables.note_id}}"
              note:
                eventId: '{{ variables.alert_id }}'
                note: | 
                  ✅ [{{workflow.name}}:{{execution.id}}]({{execution.url}})

                  ---
                  ## Automated Alert Analysis
                  ### Classification
                  **{{ steps.onechat_runAgent_step.output.classification }}**

                  ### Confidence Score
                  {{ steps.onechat_runAgent_step.output.confidence_score }} / 100

                  ### Rationale
                  {{ steps.onechat_runAgent_step.output.rationale }}

                  ### Contributing Factors
                  {% for factor in steps.onechat_runAgent_step.output.contributing_factors %}
                  - {{ factor }}
                  {%- endfor %}

                  ### Related Alert IDs
                  {% for alert in steps.onechat_runAgent_step.output.related_alert_ids %}
                  - [{{ alert.alert_id }} ({{ alert.related_by_entity_type | join: ', ' }})](/app/security/alerts/redirect/{{ alert.alert_id }}?index={{ alert.alert_index }}&timestamp={{ alert.timestamp }})
                  {%- endfor %}


                  ### Timestamp
                  {{ execution.startedAt | date: '%Y-%m-%dT%H:%M:%S' }}

                timelineId: ''
            headers:
              kbn-xsrf: string
              Content-Type: application/json
        - name: build_tags_filter_expr
          type: data.set
          with:
            tags_filter_expr: 'tag contains "{{ variables.tag_prefix }}."'
        - name: set_tags
          type: data.set
          with:
            tags_to_add:
              - "{{ variables.workflow_tag }}"
              - "{{ variables.workflow_version_tag }}"
              - "{{ variables.tag_prefix }}.output.classification.{{ steps.onechat_runAgent_step.output.classification | downcase }}"
            tags_to_remove: "${{ foreach.item.kibana.alert.workflow_tags | where_exp: 'tag', variables.tags_filter_expr }}"
        
        - name: has_tags_to_remove
          type: if
          condition: "${{ variables.tags_to_remove | has_exp: 'item', 'true' }}"
          steps:
          # Seperately remove tags to avoid adding tags that are being removed
          - name: remove_workflow_tags
            type: kibana.SetAlertTags
            with:
              ids: 
                - '{{ variables.alert_id }}'
              tags: 
                tags_to_remove: "${{ variables.tags_to_remove }}"
                tags_to_add: []
                
        # Add new tags from this
        - name: add_result_tags
          type: kibana.SetAlertTags
          with:
            ids: 
            - '{{ variables.alert_id }}'
            tags: 
              tags_to_remove: []
              tags_to_add: "${{ variables.tags_to_add }}"
        # Autoclose
        - name: check_auto_close_conditions
          type: if
          condition: "consts.auto_close_enabled: true and steps.onechat_runAgent_step.output.classification: \"false_positive\" and steps.onechat_runAgent_step.output.confidence_score >= {{ consts.auto_close_score_min_threshold }} and steps.onechat_runAgent_step.output.confidence_score <= {{ consts.auto_close_score_max_threshold }}"          
          steps:
            - name: set_close_tags
              type: kibana.SetAlertTags
              with:
                ids: 
                - '{{ variables.alert_id }}'
                tags: 
                  tags_to_remove: []
                  tags_to_add:
                    - "{{variables.tag_prefix}}.{{consts.closed_tag_suffix}}"
            - name: close_alert_as_false_positive
              type: kibana.SetAlertsStatus
              with:
                status: 'closed'
                signal_ids:
                  - '{{ variables.alert_id }}'
                reason: 'false_positive'