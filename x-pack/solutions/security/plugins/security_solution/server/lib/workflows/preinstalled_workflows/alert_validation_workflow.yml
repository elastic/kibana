name: "security.alert.validation"
enabled: true
description: "Security alert validation workflow. Author: Elastic"
version: "1"
tags:
  - security
  - alert_validation
triggers:
  - type: alert
consts:
  workflow_version: "v0.0.1" #Workflow version
  auto_close_enabled: true #Automatically close alerts exceeding FP threshold (default: false)
  auto_close_score_min_threshold: 0.1 #Minimum score for auto-close (0-100)
  auto_close_score_max_threshold: 0.39 #Maximum score for auto-close (0-100)
  auto_close_confidence_min_threshold: 0.85 #Minimum confidence for auto-close (0-1)
  auto_close_confidence_max_threshold: 1.0 #Maximum confidence for auto-close (0-1)
  closed_tag_suffix: "closed-by-agent" #Suffix for auto-closed false positives tag
steps:
- name: set_workflow_variabless
  type: data.set
  with:
    running_note: false
    normalized_name: "{{ workflow.name | downcase | replace: ' ', '_' }}"
    normalized_version: "{{ consts.workflow_version | downcase | replace: ' ', '_' }}"
    tag_prefix: "workflow.{{ workflow.name | downcase | replace: ' ', '_' }}"
- name: set_workflow_tags
  type: data.set
  with:
    workflow_tag: "{{ variables.tag_prefix }}"
    workflow_version_tag: "{{ variables.tag_prefix }}.version.{{ variables.normalized_version }}"
- name: loop_over_results
  type: foreach
  foreach: "{{ event.alerts | json }}"
  steps:
    - name: set_alert_id
      type: data.set
      with:
        alert_id: "{{ foreach.item._id }}"
        alert_index: "{{ foreach.item._index }}"
    - name: build_already_validated_expr
      type: data.set
      with:
        already_validated_expr: 'tag == "{{ variables.workflow_tag }}"'
    - name: already_validated
      type: if
      condition: "${{ foreach.item.kibana.alert.workflow_tags | has_exp: 'tag', variables.already_validated_expr }}"
      steps:
        - name: log_already_validated
          type: console
          with:
            message: "Alert {{ variables.alert_id }} has already been validated. Please remove the \"workflow.security.alert.validation\" tag to re-run."
      else:
        - name: add_execution_note_to_alert
          type: kibana.request
          with:
            method: PATCH
            path: /api/note
            body:
              note:
                eventId: '{{ variables.alert_id }}'
                note: | 
                  ⏳ [{{workflow.name}}:{{execution.id}}]({{execution.url}})

                  ---
                timelineId: ''
            headers:
              kbn-xsrf: string
              Content-Type: application/json
        - name: set_note_id
          type: data.set
          with:
            note_id: "{{ steps.add_execution_note_to_alert.output.note.noteId }}"
        - name: onechat_runAgent_step
          type: agentBuilder.runAgent
          agent_id: "elastic-ai-agent"
          with:
            message: |
              <role>
              You are a security analyst reasoning over a detection alert. Determine whether the alert reflects malicious activity, benign activity, or requires human judgment.

              You have ONLY this alert and the firing rule. You may use tools to gather more context about this alert. Your verdict must be defensible from literal alert fields, the gathered context and general security knowledge. Do not invent facts; identify what is present, what is missing, and what matters.
              </role>

              <operational_constraints>
              Output only the required schema. Do not ask clarifying questions. Reflect uncertainty via confidence_score and calculated_score. In recommendations, describe generic acquisition actions (e.g., “collect process tree,” “retrieve file contents”) and quote literal artifacts (exact paths, hashes, PIDs) from fields. Do not fabricate command syntax, tools, or unseen file paths.

              Missing Critical Fields reduce confidence only and must not raise the verdict band.
              </operational_constraints>

              <verdict_semantics>
              Malicious: Observable behavior demonstrates or closely simulates adversary tradecraft. Requires positive indicators—proved harm, confirmed attack techniques, or corroborated anomalies that cohere into an attack pattern and contradict plausible legitimate explanations.
              Benign: Observable behavior is consistent with routine legitimate operations. Present anchors support this interpretation and no coherent attack path exists in the observable facts. When context is thin, confidence is lower but the verdict remains Benign if the behavior itself is routine.
              Suspicious: Observable anomalies or direct contradictions present in the fields prevent Gate C, yet the observable facts do not meet Gate B.

              </verdict_semantics>

              <epistemic_framework>
              You perform hypothesis testing, weighing observable evidence against competing explanations.

              H0 (benign): Activity is authorized, serves a legitimate purpose.
              H1 (malicious): Activity represents adversary tradecraft.

              Evaluate which hypothesis better explains observable facts. Do not treat missing fields or required assumptions as automatic evidence; assess the quality and coherence of what is present.

              Key concepts:

              Anchor: Observable evidence in alert fields that supports legitimate authorization or expected operation.
              Anomaly: Observable deviation from expected baseline that is present in alert fields.
              MCF (Missing Critical Field): A specific field whose value would resolve the verdict. Note these to guide follow-up; do not treat absence as evidence against H0.
              PHO (Proved Harmful Outcome): Alert fields document completed harm—confirmed exfiltration, credential theft logged, ransomware execution, system compromise.
              Assumption: A belief required to support a hypothesis without direct field evidence. Track qualitatively to expose inferential leaps, not to score verdicts.
              Evidence hierarchy: observation (literal field) > inference (observation + general knowledge) > assumption.

              Vendor metadata (names, severities, labels) provides context and is not proof. Quoted rule logic present in alert fields may be cited as observable facts when supported by field values.

              </epistemic_framework>

              <reasoning_process>
              1. Describe the event: WHO did WHAT, WHERE, WHEN, HOW. Produce one-sentence event summary and one-sentence explanation of why the rule fired, using literal field values.

              2. Establish baseline: What do generic OS/app norms for this activity typically show (locations, lineage, privilege, timing, scope)?

              3. Identify present evidence:
              - Anchors: what in the alert supports legitimacy?
              - Anomalies: what observable facts deviate from baseline?
              - MCFs: what missing fields would change the assessment?

              Focus on what is present. Distinguish observable anomalies from missing context. Absence of a field is an MCF, not an anomaly.

              4. State hypotheses plainly:
              - H0: "[actor] performing [legitimate function] via [mechanism]"
              - H1: "Attacker achieving [objective] via [technique]"

              5. Weigh evidence:
              - Does H0 explain all salient facts? What assumptions does it require?
              - Do observable anomalies cohere into H1? What assumptions does it require?
              - Are anomalies isolated signals or part of a coherent attack pattern?

              If H0 explains the observations with fewer and weaker assumptions than H1 and no attack chain is present, prefer Benign; reflect any uncertainty via confidence.

              6. Assess attack chain: Do observable facts constitute or strongly suggest an attack chain?
              - COMPLETE: PHO or multi-stage attack confirmed
              - PARTIAL: Attack-consistent behaviors present but unconfirmed
              - ABSENT: Behavior does not form an attack; anomalies are isolated or context-dependent

              7. Apply decision logic (see decision_policy).
              </reasoning_process>

              <decision_policy>
              Verdict follows from the strength and coherence of observable evidence.

              Gate A: Proved Harmful Outcome
              If alert fields document actual harm, verdict is Malicious regardless of other factors.
              Score 92-100, confidence 0.85-0.95.

              Gate B: Strong Observable Malicious Indicators
              Verdict is Malicious when observable evidence forms a compelling case for adversary activity:
              - Anomalies cohere into a recognizable attack pattern
              - Attack-specific techniques confirmed in fields
              - Observable behavior contradicts all plausible legitimate explanations

              Apply Gate B when field-level facts form a coherent, attack-specific constellation that requires fewer and weaker assumptions than any benign explanation. A multi-signal match can satisfy Gate B even without PHO, provided the observable facts contradict routine operation.
              Score 80-95, confidence 0.70-0.90.

              Gate C: Consistent with Legitimate Operations
              Verdict is Benign when:
              - Observable behavior is inherently routine
              - Present anchors support this interpretation
              - No coherent attack path exists in observable facts

              Apply Gate C when these conditions hold, even if context is thin. Missing provenance or contextual fields affect confidence, not verdict. When routine anchors are present and no coherent attack chain is observable, prefer Gate C. Do not elevate to Suspicious due to missing context; reflect uncertainty in confidence only.

              Score 10-39, confidence 0.75-0.95 (lower when key context is absent).

              Gate D: Insufficient Evidence
              Verdict is Suspicious only when observable anomalies exist but are insufficient to meet Gate B criteria, or when present evidence is contradictory.

              Do not apply Gate D solely because fields are missing. If behavior is routine and no attack path is observable, use Gate C regardless of missing context.

              Suspicious scoring reflects anomaly strength and coherence, not quantity of absent fields. Stronger anomalies with weaker anchors score higher within the 40-79 band; minor anomalies with decent anchors score lower.
              Confidence 0.40-0.80.
              </decision_policy>

              <scoring_and_confidence>
              Verdict bands:
              - Malicious: 80-100
              - Suspicious: 40-79
              - Benign: 0-39

              Score within band reflects evidence strength:
              - Malicious: PHO severity or strength of attack-indicator constellation
              - Suspicious: anomaly strength and coherence, not quantity of missing fields
              - Benign: strength of routine-operation explanation; thin context lowers score within band

              Confidence reflects certainty given available evidence. High confidence means clear case from available fields. Low confidence means verdict is best-supported but key context is missing.

              Do not choose scores by typical ranges. Derive the number from this alert’s evidence strength: start at the lower bound of the band and move upward with (a) number and coherence of anomalies, (b) contradiction of plausible benign paths, (c) presence and quality of anchors. Document this in score_rationale.

              Score rationale format: "Gate [A/B/C/D]: anchors=[concise summary] anomalies=[concise summary] assumptions H0=[summary] H1=[summary]"

              </scoring_and_confidence>

              <verification>
              Before finalizing:
              - Cited anomalies are present in alert fields, not inferred from absence
              - Cited anchors are literal field values
              - If vendor metadata conflicts with field evidence, rely on field evidence
              - Am I elevating the verdict due to missing fields alone? If yes, reduce confidence and apply Gate C if behavior is routine
              - Is there any coherent attack path in observable facts? If no and anchors exist, use Gate C (Benign)
              - Malicious: Gate A or B criteria met; observable indicators explicitly cited
              - Suspicious: observable anomalies present that don't meet Gate B; MCFs named
              - Score is within verdict band; confidence aligns with reasoning
              - No contradictions between verdict, score, confidence, and justification
              </verification>

              <final_principles>
              Observable anomalies are evidence. Apply uniform reasoning. Quote literals. Malicious needs positive attack indicators. Benign needs routine behavior with no attack path; missing context affects confidence, not verdict. Prefer Benign when anchors exist and no attack chain is observable; use Suspicious only when present anomalies or contradictions warrant it. Do not memorize patterns. Reason from principles.
              </final_principles>
            attachments:
              - type: "security.alert"
                data:
                  alert: "{{ foreach.item | json:2 }}"
            schema: | 
              {
                "type": "object",
                "properties": {
                  "summary": {
                    "type": "string",
                    "description": "2-3 sentences describing the event and the rule trigger"
                  },
                  "verdict": {
                    "type": "string",
                    "enum": ["Malicious", "Suspicious", "Benign"]
                  },
                  "detailed_justification": {
                    "type": "string",
                    "description": "Gate applied, evidence summary, assumption notes, and MCFs if Suspicious"
                  },
                  "evidence": {
                    "type": "array",
                    "items": {
                      "type": "string",
                      "maxLength": 200
                    },
                    "description": "Each entry <= 18 words, quoting literal fields"
                  },
                  "recommendations": {
                    "type": "string",
                    "description": "Single string, lines separated by \\n, each starting with '- '"
                  },
                  "confidence_score": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 1
                  },
                  "calculated_score": {
                    "type": "integer",
                    "minimum": 0,
                    "maximum": 100,
                    "description": "Must fall within the verdict band"
                  },
                  "score_rationale": {
                    "type": "string",
                    "description": "One-line explanation, format defined in scoring_and_confidence"
                  },
                  "reasoning_records": {
                    "type": "array",
                    "items": {
                      "type": "string"
                    },
                    "description": "Factual analysis only; no meta commentary"
                  }
                },
                "required": [
                  "summary",
                  "verdict",
                  "detailed_justification",
                  "evidence",
                  "recommendations",
                  "confidence_score",
                  "calculated_score",
                  "score_rationale",
                  "reasoning_records"
                ]
              }
        - name: check_if_output_exists
          type: if
          condition: "not steps.onechat_runAgent_step.output.verdict:*"
          steps:
            - name: add_no_data_note_to_alert
              type: kibana.request
              with:
                method: PATCH
                path: /api/note
                body:
                  noteId: "{{variables.note_id}}"
                  note:
                    eventId: '{{ variables.alert_id }}'
                    timelineId: ''
                    note: | 
                      ❌ [{{workflow.name}}:{{execution.id}}]({{execution.url}})

                      ---
                      Error: No verdict returned.
            - name: fail_workflow
              type: http
              with:
                method: GET
                url: "---trigger-error---" 
        - name: add_verdict_note_to_alert
          type: kibana.request
          with:
            method: PATCH
            path: /api/note
            body:
              noteId: "{{variables.note_id}}"
              note:
                eventId: '{{ variables.alert_id }}'
                note: | 
                  ✅ [{{workflow.name}}:{{execution.id}}]({{execution.url}})

                  ---
                  ## Automated Alert Analysis
                  ### Summary
                  {{ steps.onechat_runAgent_step.output.summary }}

                  ### Verdict
                  **{{ steps.onechat_runAgent_step.output.verdict }}**

                  ### Detailed Justification
                  {{ steps.onechat_runAgent_step.output.detailed_justification }}

                  ### Evidence
                  {{ steps.onechat_runAgent_step.output.evidence | join: '\n- ' | prepend: '- ' }}

                  ### Recommendations
                  {{ steps.onechat_runAgent_step.output.recommendations }}

                  ### Scoring & Confidence
                  - **Calculated Score:** {{ steps.onechat_runAgent_step.output.calculated_score }} / 100
                  - **Confidence Score:** {{ steps.onechat_runAgent_step.output.confidence_score }}

                  ### Score Rationale
                  {{ steps.onechat_runAgent_step.output.score_rationale }}

                  ### Reasoning Records
                  {{ steps.onechat_runAgent_step.output.reasoning_records | join: '\n- ' | prepend: '- ' }}
                timelineId: ''
            headers:
              kbn-xsrf: string
              Content-Type: application/json
        - name: build_tags_filter_expr
          type: data.set
          with:
            tags_filter_expr: 'tag contains "{{ variables.tag_prefix }}."'
        - name: set_tags
          type: data.set
          with:
            tags_to_add:
              - "{{ variables.workflow_tag }}"
              - "{{ variables.workflow_version_tag }}"
              - "{{ variables.tag_prefix }}.output.verdict.{{ steps.onechat_runAgent_step.output.verdict | downcase }}"
            tags_to_remove: "${{ foreach.item.kibana.alert.workflow_tags | where_exp: 'tag', variables.tags_filter_expr }}"
        
        - name: has_tags_to_remove
          type: if
          condition: "${{ variables.tags_to_remove | has_exp: 'item', 'true' }}"
          steps:
          # Seperately remove tags to avoid adding tags that are being removed
          - name: remove_workflow_tags
            type: kibana.SetAlertTags
            with:
              ids: 
                - '{{ variables.alert_id }}'
              tags: 
                tags_to_remove: "${{ variables.tags_to_remove }}"
                tags_to_add: []
                
        # Add new tags from this
        - name: add_result_tags
          type: kibana.SetAlertTags
          with:
            ids: 
            - '{{ variables.alert_id }}'
            tags: 
              tags_to_remove: []
              tags_to_add: "${{ variables.tags_to_add }}"
        # Autoclose
        - name: check_auto_close_conditions
          type: if
          condition: "consts.auto_close_enabled: true and steps.onechat_runAgent_step.output.verdict: \"Benign\" and steps.onechat_runAgent_step.output.calculated_score >= {{ consts.auto_close_score_min_threshold }} and steps.onechat_runAgent_step.output.calculated_score <= {{ consts.auto_close_score_max_threshold }} and steps.onechat_runAgent_step.output.confidence_score >= {{ consts.auto_close_confidence_min_threshold }} and steps.onechat_runAgent_step.output.confidence_score <= {{ consts.auto_close_confidence_max_threshold }}"          
          steps:
            - name: set_close_tags
              type: kibana.SetAlertTags
              with:
                ids: 
                - '{{ variables.alert_id }}'
                tags: 
                  tags_to_remove: []
                  tags_to_add:
                    - "{{variables.tag_prefix}}.{{consts.closed_tag_suffix}}"
            - name: close_alert_as_false_positive
              type: kibana.SetAlertsStatus
              with:
                status: 'closed'
                signal_ids:
                  - '{{ variables.alert_id }}'
                reason: 'false_positive'
                        