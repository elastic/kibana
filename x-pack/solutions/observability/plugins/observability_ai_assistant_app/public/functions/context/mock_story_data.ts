/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

import type { ContextToolResponseV2 } from '@kbn/observability-ai-assistant-plugin/server';

export const v2Response: ContextToolResponseV2 = {
  data: {
    queries: [
      {
        semantic: {
          query: 'how to set up an ingest pipeline',
          boost: 2,
        },
      },
      {
        semantic: {
          query: 'steps to configure an ingest pipeline',
          boost: 1,
        },
      },
    ],
    entries: [
      {
        selected: false,
        document: {
          author: {
            login: 'mukeshelastic',
          },
          issue_comments: [
            {
              author: {
                login: 'tbragin',
              },
              body: '@mukeshelastic Are there changes in 7.10 around Ingest Pipeline UI that are significant for us to highlight as part of the 7.10 launch?',
            },
          ],
          title: 'Ingest pipeline UI ',
          body: "<!-- Issue template for an Observability Product Brief, typically filed by a PM. --> Below is a brief summary describing why we wanted to create ingest node UI. Please find more detailed notes including some competitive screenshots [here](https://docs.google.com/document/d/1qgZl7Wib38Y5L2Psa2Z211PvR5wUwODNsZfbz7Av6qg/edit) **Business Justification** _Why now? Link back to overall product vision._ As outlined in [Logs OKRs and Roadmap document](https://docs.google.com/document/d/1FuT5yPj5iYZZfuxXFIwt6DmnjvcZoeHhWRvGLCXm0Os/edit#) easing the log ingest workflow reduces the total cost of ownership (TCO) of elastic platform significantly. Today our users rely on learning logstash filters or interacting with ingest apis for data transformation. Making this easy is going to reduce the time and skills required to ingest logs into elastic significantly and help increase adoption of elastic platform for log analytics. **Personas / User Stories** _Who will use it? What user-facing benefit will they try to accomplish?_ ### Persona and their responsibilities: **Elastic Administrator:** - Hardware sizing for ES clusters - ES cluster operations and SLAs - Install and setup tooling to ship data ( logs, metrics, traces etc) to ES - **Setting up pipeline workflow with filters, enrichment processors for structuring & enriching logs during ingest** - Setting up multi-tenant platform with tenant isolation through spaces, ECE etc - Governance and ChargeBack - Kibana onboarding for Elastic/Kibana users **Problems for Elastic Administrator:** - Not all operators are comfortable with invoking ingest APIs to create pipelines especially when the logstash provides pipeline management GUI. Without a GUI experience, - There is no easy way to CRUD pipelines - There is no easy way to disable/enable pipelines or filters within those pipelines - There is no easy way to version control the pipelines - Specifying processors as a json file with grok or other type of filters is an unpleasant UX that can be simplified to make it easy to parse, drop, enrich, route, obscure log data. **Competitive notes** _Briefly, how do competitors do it? What do users already expect?_ Datadog, Loggly who use elasticsearch for storing logs already provide UI experience for parsing logs. Splunk is schema on read platform so it doesn't require users to structure/transform logs at ingest. Users know the benefits of structuring logs but they expect a much smoother and simpler workflow to transform unstructured logs into structured ones. **Our positioning** _How will we position this in the market? Will this be a headline in a press release? If so, what would it say?_ it will be a headline. Getting unstructured logs ready for analysis in Elastic stack within minutes! **Licensing** _How will this feature be licensed? Link to the dev repo licensing issue, if it's a bigger discussion._ Basic+ Need to consult with ES UI team on the license **Telemetry / KPIs** _How will we measure success / adoption of this feature?_ Number of pipelines created using ingest pipeline UI Number of docs processed by ingest pipelines created by ingest pipeline UI",
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/917',
          number: 917,
          createdAt: '2020-06-08T09:34:30Z',
          assignees_list: [],
          labels_field: [
            {
              name: 'product-brief',
              description:
                'Brief description of justification, personas / user stories, acceptance criteria for the feature',
            },
            {
              name: 'v7.8.0',
              description: null,
            },
            {
              name: 'v7.9.0',
              description: null,
            },
            {
              name: 'v7.10.0',
              description: '',
            },
          ],
          state: 'OPEN',
          id: 'MDU6SXNzdWU2MzQ0NDQyMjg=',
          closedAt: null,
          _timestamp: '2023-06-10T01:11:25Z',
        },
        id: 'search-observability-dev/MDU6SXNzdWU2MzQ0NDQyMjg=',
        title: 'Ingest pipeline UI ',
        score: 173.26079,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"mukeshelastic"},"issue_comments":[{"author":{"login":"tbragin"},"body":"@mukeshelastic Are there changes in 7.10 around Ingest Pipeline UI that are significant for us to highlight as part of the 7.10 launch?"}],"title":"Ingest pipeline UI ","body":"<!-- Issue template for an Observability Product Brief, typically filed by a PM. --> Below is a brief summary describing why we wanted to create ingest node UI. Please find more detailed notes including some competitive screenshots [here](https://docs.google.com/document/d/1qgZl7Wib38Y5L2Psa2Z211PvR5wUwODNsZfbz7Av6qg/edit) **Business Justification** _Why now? Link back to overall product vision._ As outlined in [Logs OKRs and Roadmap document](https://docs.google.com/document/d/1FuT5yPj5iYZZfuxXFIwt6DmnjvcZoeHhWRvGLCXm0Os/edit#) easing the log ingest workflow reduces the total cost of ownership (TCO) of elastic platform significantly. Today our users rely on learning logstash filters or interacting with ingest apis for data transformation. Making this easy is going to reduce the time and skills required to ingest logs into elastic significantly and help increase adoption of elastic platform for log analytics. **Personas / User Stories** _Who will use it? What user-facing benefit will they try to accomplish?_ ### Persona and their responsibilities: **Elastic Administrator:** - Hardware sizing for ES clusters - ES cluster operations and SLAs - Install and setup tooling to ship data ( logs, metrics, traces etc) to ES - **Setting up pipeline workflow with filters, enrichment processors for structuring & enriching logs during ingest** - Setting up multi-tenant platform with tenant isolation through spaces, ECE etc - Governance and ChargeBack - Kibana onboarding for Elastic/Kibana users **Problems for Elastic Administrator:** - Not all operators are comfortable with invoking ingest APIs to create pipelines especially when the logstash provides pipeline management GUI. Without a GUI experience, - There is no easy way to CRUD pipelines - There is no easy way to disable/enable pipelines or filters within those pipelines - There is no easy way to version control the pipelines - Specifying processors as a json file with grok or other type of filters is an unpleasant UX that can be simplified to make it easy to parse, drop, enrich, route, obscure log data. **Competitive notes** _Briefly, how do competitors do it? What do users already expect?_ Datadog, Loggly who use elasticsearch for storing logs already provide UI experience for parsing logs. Splunk is schema on read platform so it doesn\'t require users to structure/transform logs at ingest. Users know the benefits of structuring logs but they expect a much smoother and simpler workflow to transform unstructured logs into structured ones. **Our positioning** _How will we position this in the market? Will this be a headline in a press release? If so, what would it say?_ it will be a headline. Getting unstructured logs ready for analysis in Elastic stack within minutes! **Licensing** _How will this feature be licensed? Link to the dev repo licensing issue, if it\'s a bigger discussion._ Basic+ Need to consult with ES UI team on the license **Telemetry / KPIs** _How will we measure success / adoption of this feature?_ Number of pipelines created using ingest pipeline UI Number of docs processed by ingest pipelines created by ingest pipeline UI","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/917","number":917,"createdAt":"2020-06-08T09:34:30Z","assignees_list":[],"labels_field":[{"name":"product-brief","description":"Brief description of justification, personas / user stories, acceptance criteria for the feature"},{"name":"v7.8.0","description":null},{"name":"v7.9.0","description":null},{"name":"v7.10.0","description":""}],"state":"OPEN","id":"MDU6SXNzdWU2MzQ0NDQyMjg=","closedAt":null,"_timestamp":"2023-06-10T01:11:25Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"mukeshelastic"},"issue_comments":[{"author":{"login":"tbragin"},"body":"@mukeshelastic Are there changes in 7.10 around Ingest Pipeline UI that are significant for us to highlight as part of the 7.10 launch?"}],"title":"Ingest pipeline UI ","body":"<!-- Issue template for an Observability Product Brief, typically filed by a PM. --> Below is a brief summary describing why we wanted to create ingest node UI. Please find more detailed notes including some competitive screenshots [here](https://docs.google.com/document/d/1qgZl7Wib38Y5L2Psa2Z211PvR5wUwODNsZfbz7Av6qg/edit) **Business Justification** _Why now? Link back to overall product vision._ As outlined in [Logs OKRs and Roadmap document](https://docs.google.com/document/d/1FuT5yPj5iYZZfuxXFIwt6DmnjvcZoeHhWRvGLCXm0Os/edit#) easing the log ingest workflow reduces the total cost of ownership (TCO) of elastic platform significantly. Today our users rely on learning logstash filters or interacting with ingest apis for data transformation. Making this easy is going to reduce the time and skills required to ingest logs into elastic significantly and help increase adoption of elastic platform for log analytics. **Personas / User Stories** _Who will use it? What user-facing benefit will they try to accomplish?_ ### Persona and their responsibilities: **Elastic Administrator:** - Hardware sizing for ES clusters - ES cluster operations and SLAs - Install and setup tooling to ship data ( logs, metrics, traces etc) to ES - **Setting up pipeline workflow with filters, enrichment processors for structuring & enriching logs during ingest** - Setting up multi-tenant platform with tenant isolation through spaces, ECE... <truncated>',
          originalTokenCount: 913,
          truncatedTokenCount: 400,
        },
        llmScore: 2,
      },
      {
        selected: true,
        document: {
          content:
            '## Ingest pipelines\n\nIngest pipelines let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.\n\nA pipeline consists of a series of configurable tasks called [processors](processors.html "Ingest processor reference"). Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, Elasticsearch adds the transformed documents to your data stream or index.\n\n![Ingest pipeline diagram](images/ingest/ingest-process.svg)\n\nYou can create and manage ingest pipelines using Kibana’s **Ingest Pipelines** feature or the [ingest APIs](ingest-apis.html "Ingest APIs"). Elasticsearch stores pipelines in the [cluster state](cluster-state.html "Cluster state API").\n\n### Prerequisites\n\n* Nodes with the [`ingest`](modules-node.html#node-ingest-node "Ingest node") node role handle pipeline processing. To use ingest pipelines, your cluster must have at least one node with the `ingest` role. For heavy ingest loads, we recommend creating [dedicated ingest nodes](modules-node.html#node-ingest-node "Ingest node").\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to manage ingest pipelines. To use Kibana’s **Ingest Pipelines** feature, you also need the `cluster:monitor/nodes/info` cluster privileges.\n* Pipelines including the `enrich` processor require additional setup. See [*Enrich your data*](ingest-enriching-data.html "Enrich your data").\n\n### Create and manage pipelines\n\nIn Kibana, open the main menu and click **Stack Management > Ingest Pipelines**. From the list view, you can:\n\n* View a list of your pipelines and drill down into details\n* Edit or clone existing pipelines\n* Delete pipelines\n\n![Kibana’s Ingest Pipelines list view](images/ingest/ingest-pipeline-list.png)\n\nTo create a pipeline, click **Create pipeline > New pipeline**. For an example tutorial, see [*Example: Parse logs*](common-log-format-example.html "Example: Parse logs in the Common Log Format").\n\nThe **New pipeline from CSV** option lets you use a CSV to create an ingest pipeline that maps custom data to the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16). Mapping your custom data to ECS makes the data easier to search and lets you reuse visualizations from other datasets. To get started, check [Map custom data to ECS](/guide/en/ecs/8.16/ecs-converting.html).\n\nYou can also use the [ingest APIs](ingest-apis.html "Ingest APIs") to create and manage pipelines. The following [create pipeline API](put-pipeline-api.html "Create or update pipeline API") request creates a pipeline containing two [`set`](set-processor.html "Set processor") processors followed by a [`lowercase`](lowercase-processor.html "Lowercase processor") processor. The processors run sequentially in the order specified.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    description="My optional pipeline description",\n    processors=[\n        {\n            "set": {\n                "description": "My optional processor description",\n                "field": "my-long-field",\n                "value": 10\n            }\n        },\n        {\n            "set": {\n                "description": "Set \'my-boolean-field\' to true",\n                "field": "my-boolean-field",\n                "value": True\n            }\n        },\n        {\n            "lowercase": {\n                "field": "my-keyword-field"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    description: \'My optional pipeline description\',\n    processors: [\n      {\n        set: {\n          description: \'My optional processor description\',\n          field: \'my-long-field\',\n          value: 10\n        }\n      },\n      {\n        set: {\n          description: "Set \'my-boolean-field\' to true",\n          field: \'my-boolean-field\',\n          value: true\n        }\n      },\n      {\n        lowercase: {\n          field: \'my-keyword-field\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  description: "My optional pipeline description",\n  processors: [\n    {\n      set: {\n        description: "My optional processor description",\n        field: "my-long-field",\n        value: 10,\n      },\n    },\n    {\n      set: {\n        description: "Set \'my-boolean-field\' to true",\n        field: "my-boolean-field",\n        value: true,\n      },\n    },\n    {\n      lowercase: {\n        field: "my-keyword-field",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "description": "My optional pipeline description",\n  "processors": [\n    {\n      "set": {\n        "description": "My optional processor description",\n        "field": "my-long-field",\n        "value": 10\n      }\n    },\n    {\n      "set": {\n        "description": "Set \'my-boolean-field\' to true",\n        "field": "my-boolean-field",\n        "value": true\n      }\n    },\n    {\n      "lowercase": {\n        "field": "my-keyword-field"\n      }\n    }\n  ]\n}\n```\n\n### Manage pipeline versions\n\nWhen you create or update a pipeline, you can specify an optional `version` integer. You can use this version number with the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params "Query parameters") parameter to conditionally update the pipeline. When the `if_version` parameter is specified, a successful update increments the pipeline’s version.\n\n```\nPUT _ingest/pipeline/my-pipeline-id\n{\n  "version": 1,\n  "processors": [ ... ]\n}\n```\n\nTo unset the `version` number using the API, replace or update the pipeline without specifying the `version` parameter.\n\n### Test a pipeline\n\nBefore using a pipeline in production, we recommend you test it using sample documents. When creating or editing a pipeline in Kibana, click **Add documents**. In the **Documents** tab, provide sample documents and click **Run the pipeline**.\n\n![Test a pipeline in Kibana](images/ingest/test-a-pipeline.png)\n\nYou can also test pipelines using the [simulate pipeline API](simulate-pipeline-api.html "Simulate pipeline API"). You can specify a configured pipeline in the request path. For example, the following request tests `my-pipeline`.\n\n```\nresp = client.ingest.simulate(\n    id="my-pipeline",\n    docs=[\n        {\n            "_source": {\n                "my-keyword-field": "FOO"\n            }\n        },\n        {\n            "_source": {\n                "my-keyword-field": "BAR"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.simulate(\n  id: \'my-pipeline\',\n  body: {\n    docs: [\n      {\n        _source: {\n          "my-keyword-field": \'FOO\'\n        }\n      },\n      {\n        _source: {\n          "my-keyword-field": \'BAR\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.simulate({\n  id: "my-pipeline",\n  docs: [\n    {\n      _source: {\n        "my-keyword-field": "FOO",\n      },\n    },\n    {\n      _source: {\n        "my-keyword-field": "BAR",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST _ingest/pipeline/my-pipeline/_simulate\n{\n  "docs": [\n    {\n      "_source": {\n        "my-keyword-field": "FOO"\n      }\n    },\n    {\n      "_source": {\n        "my-keyword-field": "BAR"\n      }\n    }\n  ]\n}\n```\n\nAlternatively, you can specify a pipeline and its processors in the request body.\n\n```\nresp = client.ingest.simulate(\n    pipeline={\n        "processors": [\n            {\n                "lowercase": {\n                    "field": "my-keyword-field"\n                }\n            }\n        ]\n    },\n    docs=[\n        {\n            "_source": {\n                "my-keyword-field": "FOO"\n            }\n        },\n        {\n            "_source": {\n                "my-keyword-field": "BAR"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.simulate(\n  body: {\n    pipeline: {\n      processors: [\n        {\n          lowercase: {\n            field: \'my-keyword-field\'\n          }\n        }\n      ]\n    },\n    docs: [\n      {\n        _source: {\n          "my-keyword-field": \'FOO\'\n        }\n      },\n      {\n        _source: {\n          "my-keyword-field": \'BAR\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.simulate({\n  pipeline: {\n    processors: [\n      {\n        lowercase: {\n          field: "my-keyword-field",\n        },\n      },\n    ],\n  },\n  docs: [\n    {\n      _source: {\n        "my-keyword-field": "FOO",\n      },\n    },\n    {\n      _source: {\n        "my-keyword-field": "BAR",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST _ingest/pipeline/_simulate\n{\n  "pipeline": {\n    "processors": [\n      {\n        "lowercase": {\n          "field": "my-keyword-field"\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_source": {\n        "my-keyword-field": "FOO"\n      }\n    },\n    {\n      "_source": {\n        "my-keyword-field": "BAR"\n      }\n    }\n  ]\n}\n```\n\nThe API returns transformed documents:\n\n```\n{\n  "docs": [\n    {\n      "doc": {\n        "_index": "_index",\n        "_id": "_id",\n        "_version": "-3",\n        "_source": {\n          "my-keyword-field": "foo"\n        },\n        "_ingest": {\n          "timestamp": "2099-03-07T11:04:03.000Z"\n        }\n      }\n    },\n    {\n      "doc": {\n        "_index": "_index",\n        "_id": "_id",\n        "_version": "-3",\n        "_source": {\n          "my-keyword-field": "bar"\n        },\n        "_ingest": {\n          "timestamp": "2099-03-07T11:04:04.000Z"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Add a pipeline to an indexing request\n\nUse the `pipeline` query parameter to apply a pipeline to documents in [individual](docs-index_.html "Index API") or [bulk](docs-bulk.html "Bulk API") indexing requests.\n\n```\nresp = client.index(\n    index="my-data-stream",\n    pipeline="my-pipeline",\n    document={\n        "@timestamp": "2099-03-07T11:04:05.000Z",\n        "my-keyword-field": "foo"\n    },\n)\nprint(resp)\n\nresp1 = client.bulk(\n    index="my-data-stream",\n    pipeline="my-pipeline",\n    operations=[\n        {\n            "create": {}\n        },\n        {\n            "@timestamp": "2099-03-07T11:04:06.000Z",\n            "my-keyword-field": "foo"\n        },\n        {\n            "create": {}\n        },\n        {\n            "@timestamp": "2099-03-07T11:04:07.000Z",\n            "my-keyword-field": "bar"\n        }\n    ],\n)\nprint(resp1)\n```\n\n```\nresponse = client.index(\n  index: \'my-data-stream\',\n  pipeline: \'my-pipeline\',\n  body: {\n    "@timestamp": \'2099-03-07T11:04:05.000Z\',\n    "my-keyword-field": \'foo\'\n  }\n)\nputs response\n\nresponse = client.bulk(\n  index: \'my-data-stream\',\n  pipeline: \'my-pipeline\',\n  body: [\n    {\n      create: {}\n    },\n    {\n      "@timestamp": \'2099-03-07T11:04:06.000Z\',\n      "my-keyword-field": \'foo\'\n    },\n    {\n      create: {}\n    },\n    {\n      "@timestamp": \'2099-03-07T11:04:07.000Z\',\n      "my-keyword-field": \'bar\'\n    }\n  ]\n)\nputs response\n```\n\n```\nconst response = await client.index({\n  index: "my-data-stream",\n  pipeline: "my-pipeline",\n  document: {\n    "@timestamp": "2099-03-07T11:04:05.000Z",\n    "my-keyword-field": "foo",\n  },\n});\nconsole.log(response);\n\nconst response1 = await client.bulk({\n  index: "my-data-stream",\n  pipeline: "my-pipeline",\n  operations: [\n    {\n      create: {},\n    },\n    {\n      "@timestamp": "2099-03-07T11:04:06.000Z",\n      "my-keyword-field": "foo",\n    },\n    {\n      create: {},\n    },\n    {\n      "@timestamp": "2099-03-07T11:04:07.000Z",\n      "my-keyword-field": "bar",\n    },\n  ],\n});\nconsole.log(response1);\n```\n\n```\nPOST my-data-stream/_doc?pipeline=my-pipeline\n{\n  "@timestamp": "2099-03-07T11:04:05.000Z",\n  "my-keyword-field": "foo"\n}\n\nPUT my-data-stream/_bulk?pipeline=my-pipeline\n{ "create":{ } }\n{ "@timestamp": "2099-03-07T11:04:06.000Z", "my-keyword-field": "foo" }\n{ "create":{ } }\n{ "@timestamp": "2099-03-07T11:04:07.000Z", "my-keyword-field": "bar" }\n```\n\nYou can also use the `pipeline` parameter with the [update by query](docs-update-by-query.html "Update By Query API") or [reindex](docs-reindex.html "Reindex API") APIs.\n\n```\nresp = client.update_by_query(\n    index="my-data-stream",\n    pipeline="my-pipeline",\n)\nprint(resp)\n\nresp1 = client.reindex(\n    source={\n        "index": "my-data-stream"\n    },\n    dest={\n        "index": "my-new-data-stream",\n        "op_type": "create",\n        "pipeline": "my-pipeline"\n    },\n)\nprint(resp1)\n```\n\n```\nresponse = client.update_by_query(\n  index: \'my-data-stream\',\n  pipeline: \'my-pipeline\'\n)\nputs response\n\nresponse = client.reindex(\n  body: {\n    source: {\n      index: \'my-data-stream\'\n    },\n    dest: {\n      index: \'my-new-data-stream\',\n      op_type: \'create\',\n      pipeline: \'my-pipeline\'\n    }\n  }\n)\nputs response\n```\n\n```\nconst response = await client.updateByQuery({\n  index: "my-data-stream",\n  pipeline: "my-pipeline",\n});\nconsole.log(response);\n\nconst response1 = await client.reindex({\n  source: {\n    index: "my-data-stream",\n  },\n  dest: {\n    index: "my-new-data-stream",\n    op_type: "create",\n    pipeline: "my-pipeline",\n  },\n});\nconsole.log(response1);\n```\n\n```\nPOST my-data-stream/_update_by_query?pipeline=my-pipeline\n\nPOST _reindex\n{\n  "source": {\n    "index": "my-data-stream"\n  },\n  "dest": {\n    "index": "my-new-data-stream",\n    "op_type": "create",\n    "pipeline": "my-pipeline"\n  }\n}\n```\n\n### Set a default pipeline\n\nUse the [`index.default_pipeline`](index-modules.html#index-default-pipeline) index setting to set a default pipeline. Elasticsearch applies this pipeline to indexing requests if no `pipeline` parameter is specified.\n\n### Set a final pipeline\n\nUse the [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting to set a final pipeline. Elasticsearch applies this pipeline after the request or default pipeline, even if neither is specified.\n\n### Pipelines for Beats\n\nTo add an ingest pipeline to an Elastic Beat, specify the `pipeline` parameter under `output.elasticsearch` in `<BEAT_NAME>.yml`. For example, for Filebeat, you’d specify `pipeline` in `filebeat.yml`.\n\n```\noutput.elasticsearch:\n  hosts: ["localhost:9200"]\n  pipeline: my-pipeline\n```\n\n### Pipelines for Fleet and Elastic Agent\n\nElastic Agent integrations ship with default ingest pipelines that preprocess and enrich data before indexing. [Fleet](/guide/en/fleet/8.17/index.html) applies these pipelines using [index templates](index-templates.html "Index templates") that include [pipeline index settings](ingest.html#set-default-pipeline "Set a default pipeline"). Elasticsearch matches these templates to your Fleet data streams based on the [stream’s naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme).\n\nEach default integration pipeline calls a nonexistent, unversioned `*@custom` ingest pipeline. If unaltered, this pipeline call has no effect on your data. However, you can modify this call to create custom pipelines for integrations that persist across upgrades. Refer to [Tutorial: Transform data with custom ingest pipelines](/guide/en/fleet/8.17/data-streams-pipeline-tutorial.html) to learn more.\n\nFleet doesn’t provide a default ingest pipeline for the **Custom logs** integration, but you can specify a pipeline for this integration using an [index template](ingest.html#pipeline-custom-logs-index-template) or a [custom configuration](ingest.html#pipeline-custom-logs-configuration).\n\n**Option 1: Index template**\n\n1. [Create](ingest.html#create-manage-ingest-pipelines "Create and manage pipelines") and [test](ingest.html#test-pipeline "Test a pipeline") your ingest pipeline. Name your pipeline `logs-<dataset-name>-default`. This makes tracking the pipeline for your integration easier.\n\n   For example, the following request creates a pipeline for the `my-app` dataset. The pipeline’s name is `logs-my_app-default`.\n\n   \n\n   ```\n   PUT _ingest/pipeline/logs-my_app-default\n   {\n     "description": "Pipeline for `my_app` dataset",\n     "processors": [ ... ]\n   }\n   ```\n\n2. Create an [index template](index-templates.html "Index templates") that includes your pipeline in the [`index.default_pipeline`](index-modules.html#index-default-pipeline) or [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting. Ensure the template is [data stream enabled](set-up-a-data-stream.html#create-index-template "Create an index template"). The template’s index pattern should match `logs-<dataset-name>-*`.\n\n   You can create this template using Kibana’s [**Index Management**](index-mgmt.html#manage-index-templates "Manage index templates") feature or the [create index template API](indices-put-template.html "Create or update index template API").\n\n   For example, the following request creates a template matching `logs-my_app-*`. The template uses a component template that contains the `index.default_pipeline` index setting.\n\n   ```\n   resp = client.cluster.put_component_template(\n       name="logs-my_app-settings",\n       template={\n           "settings": {\n               "index.default_pipeline": "logs-my_app-default",\n               "index.lifecycle.name": "logs"\n           }\n       },\n   )\n   print(resp)\n\n   resp1 = client.indices.put_index_template(\n       name="logs-my_app-template",\n       index_patterns=[\n           "logs-my_app-*"\n       ],\n       data_stream={},\n       priority=500,\n       composed_of=[\n           "logs-my_app-settings",\n           "logs-my_app-mappings"\n       ],\n   )\n   print(resp1)\n   ```\n\n   ```\n   const response = await client.cluster.putComponentTemplate({\n     name: "logs-my_app-settings",\n     template: {\n       settings: {\n         "index.default_pipeline": "logs-my_app-default",\n         "index.lifecycle.name": "logs",\n       },\n     },\n   });\n   console.log(response);\n\n   const response1 = await client.indices.putIndexTemplate({\n     name: "logs-my_app-template",\n     index_patterns: ["logs-my_app-*"],\n     data_stream: {},\n     priority: 500,\n     composed_of: ["logs-my_app-settings", "logs-my_app-mappings"],\n   });\n   console.log(response1);\n   ```\n\n   \n\n   ```\n   # Creates a component template for index settings\n   PUT _component_template/logs-my_app-settings\n   {\n     "template": {\n       "settings": {\n         "index.default_pipeline": "logs-my_app-default",\n         "index.lifecycle.name": "logs"\n       }\n     }\n   }\n\n   # Creates an index template matching `logs-my_app-*`\n   PUT _index_template/logs-my_app-template\n   {\n     "index_patterns": ["logs-my_app-*"],\n     "data_stream": { },\n     "priority": 500,\n     "composed_of": ["logs-my_app-settings", "logs-my_app-mappings"]\n   }\n   ```\n\n3. When adding or editing your **Custom logs** integration in Fleet, click **Configure integration > Custom log file > Advanced options**.\n\n4. In **Dataset name**, specify your dataset’s name. Fleet will add new data for the integration to the resulting `logs-<dataset-name>-default` data stream.\n\n   For example, if your dataset’s name was `my_app`, Fleet adds new data to the `logs-my_app-default` data stream.\n\n   ![Set up custom log integration in Fleet](images/ingest/custom-logs.png)\n\n5. Use the [rollover API](indices-rollover-index.html "Rollover API") to roll over your data stream. This ensures Elasticsearch applies the index template and its pipeline settings to any new data for the integration.\n\n   ```\n   resp = client.indices.rollover(\n       alias="logs-my_app-default",\n   )\n   print(resp)\n   ```\n\n   ```\n   response = client.indices.rollover(\n     alias: \'logs-my_app-default\'\n   )\n   puts response\n   ```\n\n   ```\n   const response = await client.indices.rollover({\n     alias: "logs-my_app-default",\n   });\n   console.log(response);\n   ```\n\n   \n\n   ```\n   POST logs-my_app-default/_rollover/\n   ```\n\n**Option 2: Custom configuration**\n\n1. [Create](ingest.html#create-manage-ingest-pipelines "Create and manage pipelines") and [test](ingest.html#test-pipeline "Test a pipeline") your ingest pipeline. Name your pipeline `logs-<dataset-name>-default`. This makes tracking the pipeline for your integration easier.\n\n   For example, the following request creates a pipeline for the `my-app` dataset. The pipeline’s name is `logs-my_app-default`.\n\n   \n\n   ```\n   PUT _ingest/pipeline/logs-my_app-default\n   {\n     "description": "Pipeline for `my_app` dataset",\n     "processors": [ ... ]\n   }\n   ```\n\n2. When adding or editing your **Custom logs** integration in Fleet, click **Configure integration > Custom log file > Advanced options**.\n\n3. In **Dataset name**, specify your dataset’s name. Fleet will add new data for the integration to the resulting `logs-<dataset-name>-default` data stream.\n\n   For example, if your dataset’s name was `my_app`, Fleet adds new data to the `logs-my_app-default` data stream.\n\n4. In **Custom Configurations**, specify your pipeline in the `pipeline` policy setting.\n\n   ![Custom pipeline configuration for custom log integration](images/ingest/custom-logs-pipeline.png)\n\n**Elastic Agent standalone**\n\nIf you run Elastic Agent standalone, you can apply pipelines using an [index template](index-templates.html "Index templates") that includes the [`index.default_pipeline`](index-modules.html#index-default-pipeline) or [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting. Alternatively, you can specify the `pipeline` policy setting in your `elastic-agent.yml` configuration. See [Install standalone Elastic Agents](/guide/en/fleet/8.17/install-standalone-elastic-agent.html).\n\n### Pipelines for search indices\n\nWhen you create Elasticsearch indices for search use cases, for example, using the [web crawler](/guide/en/enterprise-search/8.17/crawler.html) or [connectors](es-connectors.html "Ingest content with Elastic connectors"), these indices are automatically set up with specific ingest pipelines. These processors help optimize your content for search. See [*Ingest pipelines in Search*](ingest-pipeline-search.html "Ingest pipelines in Search") for more information.\n\n### Access source fields in a processor\n\nProcessors have read and write access to an incoming document’s source fields. To access a field key in a processor, use its field name. The following `set` processor accesses `my-long-field`.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "field": "my-long-field",\n                "value": 10\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          field: \'my-long-field\',\n          value: 10\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        field: "my-long-field",\n        value: 10,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "field": "my-long-field",\n        "value": 10\n      }\n    }\n  ]\n}\n```\n\nYou can also prepend the `_source` prefix.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "field": "_source.my-long-field",\n                "value": 10\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          field: \'_source.my-long-field\',\n          value: 10\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        field: "_source.my-long-field",\n        value: 10,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "field": "_source.my-long-field",\n        "value": 10\n      }\n    }\n  ]\n}\n```\n\nUse dot notation to access object fields.\n\nIf your document contains flattened objects, use the [`dot_expander`](dot-expand-processor.html "Dot expander processor") processor to expand them first. Other ingest processors cannot access flattened objects.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "dot_expander": {\n                "description": "Expand \'my-object-field.my-property\'",\n                "field": "my-object-field.my-property"\n            }\n        },\n        {\n            "set": {\n                "description": "Set \'my-object-field.my-property\' to 10",\n                "field": "my-object-field.my-property",\n                "value": 10\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        dot_expander: {\n          description: "Expand \'my-object-field.my-property\'",\n          field: \'my-object-field.my-property\'\n        }\n      },\n      {\n        set: {\n          description: "Set \'my-object-field.my-property\' to 10",\n          field: \'my-object-field.my-property\',\n          value: 10\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      dot_expander: {\n        description: "Expand \'my-object-field.my-property\'",\n        field: "my-object-field.my-property",\n      },\n    },\n    {\n      set: {\n        description: "Set \'my-object-field.my-property\' to 10",\n        field: "my-object-field.my-property",\n        value: 10,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "dot_expander": {\n        "description": "Expand \'my-object-field.my-property\'",\n        "field": "my-object-field.my-property"\n      }\n    },\n    {\n      "set": {\n        "description": "Set \'my-object-field.my-property\' to 10",\n        "field": "my-object-field.my-property",\n        "value": 10\n      }\n    }\n  ]\n}\n```\n\nSeveral processor parameters support [Mustache](https://mustache.github.io) template snippets. To access field values in a template snippet, enclose the field name in triple curly brackets:`{{{field-name}}}`. You can use template snippets to dynamically set field names.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Set dynamic \'<service>\' field to \'code\' value",\n                "field": "{{{service}}}",\n                "value": "{{{code}}}"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "Set dynamic \'<service>\' field to \'code\' value",\n          field: \'{{{service}}}\',\n          value: \'{{{code}}}\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Set dynamic \'<service>\' field to \'code\' value",\n        field: "{{{service}}}",\n        value: "{{{code}}}",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Set dynamic \'<service>\' field to \'code\' value",\n        "field": "{{{service}}}",\n        "value": "{{{code}}}"\n      }\n    }\n  ]\n}\n```\n\n### Access metadata fields in a processor\n\nProcessors can access the following metadata fields by name:\n\n* `_index`\n* `_id`\n* `_routing`\n* `_dynamic_templates`\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Set \'_routing\' to \'geoip.country_iso_code\' value",\n                "field": "_routing",\n                "value": "{{{geoip.country_iso_code}}}"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "Set \'_routing\' to \'geoip.country_iso_code\' value",\n          field: \'_routing\',\n          value: \'{{{geoip.country_iso_code}}}\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Set \'_routing\' to \'geoip.country_iso_code\' value",\n        field: "_routing",\n        value: "{{{geoip.country_iso_code}}}",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Set \'_routing\' to \'geoip.country_iso_code\' value",\n        "field": "_routing",\n        "value": "{{{geoip.country_iso_code}}}"\n      }\n    }\n  ]\n}\n```\n\nUse a Mustache template snippet to access metadata field values. For example, `{{{_routing}}}` retrieves a document’s routing value.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Use geo_point dynamic template for address field",\n                "field": "_dynamic_templates",\n                "value": {\n                    "address": "geo_point"\n                }\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: \'Use geo_point dynamic template for address field\',\n          field: \'_dynamic_templates\',\n          value: {\n            address: \'geo_point\'\n          }\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Use geo_point dynamic template for address field",\n        field: "_dynamic_templates",\n        value: {\n          address: "geo_point",\n        },\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Use geo_point dynamic template for address field",\n        "field": "_dynamic_templates",\n        "value": {\n          "address": "geo_point"\n        }\n      }\n    }\n  ]\n}\n```\n\nThe set processor above tells ES to use the dynamic template named `geo_point` for the field `address` if this field is not defined in the mapping of the index yet. This processor overrides the dynamic template for the field `address` if already defined in the bulk request, but has no effect on other dynamic templates defined in the bulk request.\n\nIf you [automatically generate](docs-index_.html#create-document-ids-automatically "Create document IDs automatically") document IDs, you cannot use `{{{_id}}}` in a processor. Elasticsearch assigns auto-generated `_id` values after ingest.\n\n### Access ingest metadata in a processor\n\nIngest processors can add and access ingest metadata using the `_ingest` key.\n\nUnlike source and metadata fields, Elasticsearch does not index ingest metadata fields by default. Elasticsearch also allows source fields that start with an `_ingest` key. If your data includes such source fields, use `_source._ingest` to access them.\n\nPipelines only create the `_ingest.timestamp` ingest metadata field by default. This field contains a timestamp of when Elasticsearch received the document’s indexing request. To index `_ingest.timestamp` or other ingest metadata fields, use the `set` processor.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Index the ingest timestamp as \'event.ingested\'",\n                "field": "event.ingested",\n                "value": "{{{_ingest.timestamp}}}"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "Index the ingest timestamp as \'event.ingested\'",\n          field: \'event.ingested\',\n          value: \'{{{_ingest.timestamp}}}\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Index the ingest timestamp as \'event.ingested\'",\n        field: "event.ingested",\n        value: "{{{_ingest.timestamp}}}",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Index the ingest timestamp as \'event.ingested\'",\n        "field": "event.ingested",\n        "value": "{{{_ingest.timestamp}}}"\n      }\n    }\n  ]\n}\n```\n\n### Handling pipeline failures\n\nA pipeline’s processors run sequentially. By default, pipeline processing stops when one of these processors fails or encounters an error.\n\nTo ignore a processor failure and run the pipeline’s remaining processors, set `ignore_failure` to `true`.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "rename": {\n                "description": "Rename \'provider\' to \'cloud.provider\'",\n                "field": "provider",\n                "target_field": "cloud.provider",\n                "ignore_failure": True\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        rename: {\n          description: "Rename \'provider\' to \'cloud.provider\'",\n          field: \'provider\',\n          target_field: \'cloud.provider\',\n          ignore_failure: true\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      rename: {\n        description: "Rename \'provider\' to \'cloud.provider\'",\n        field: "provider",\n        target_field: "cloud.provider",\n        ignore_failure: true,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "rename": {\n        "description": "Rename \'provider\' to \'cloud.provider\'",\n        "field": "provider",\n        "target_field": "cloud.provider",\n        "ignore_failure": true\n      }\n    }\n  ]\n}\n```\n\nUse the `on_failure` parameter to specify a list of processors to run immediately after a processor failure. If `on_failure` is specified, Elasticsearch afterward runs the pipeline’s remaining processors, even if the `on_failure` configuration is empty.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "rename": {\n                "description": "Rename \'provider\' to \'cloud.provider\'",\n                "field": "provider",\n                "target_field": "cloud.provider",\n                "on_failure": [\n                    {\n                        "set": {\n                            "description": "Set \'error.message\'",\n                            "field": "error.message",\n                            "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                            "override": False\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        rename: {\n          description: "Rename \'provider\' to \'cloud.provider\'",\n          field: \'provider\',\n          target_field: \'cloud.provider\',\n          on_failure: [\n            {\n              set: {\n                description: "Set \'error.message\'",\n                field: \'error.message\',\n                value: "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                override: false\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      rename: {\n        description: "Rename \'provider\' to \'cloud.provider\'",\n        field: "provider",\n        target_field: "cloud.provider",\n        on_failure: [\n          {\n            set: {\n              description: "Set \'error.message\'",\n              field: "error.message",\n              value:\n                "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              override: false,\n            },\n          },\n        ],\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "rename": {\n        "description": "Rename \'provider\' to \'cloud.provider\'",\n        "field": "provider",\n        "target_field": "cloud.provider",\n        "on_failure": [\n          {\n            "set": {\n              "description": "Set \'error.message\'",\n              "field": "error.message",\n              "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              "override": false\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\nNest a list of `on_failure` processors for nested error handling.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "rename": {\n                "description": "Rename \'provider\' to \'cloud.provider\'",\n                "field": "provider",\n                "target_field": "cloud.provider",\n                "on_failure": [\n                    {\n                        "set": {\n                            "description": "Set \'error.message\'",\n                            "field": "error.message",\n                            "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                            "override": False,\n                            "on_failure": [\n                                {\n                                    "set": {\n                                        "description": "Set \'error.message.multi\'",\n                                        "field": "error.message.multi",\n                                        "value": "Document encountered multiple ingest errors",\n                                        "override": True\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        rename: {\n          description: "Rename \'provider\' to \'cloud.provider\'",\n          field: \'provider\',\n          target_field: \'cloud.provider\',\n          on_failure: [\n            {\n              set: {\n                description: "Set \'error.message\'",\n                field: \'error.message\',\n                value: "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                override: false,\n                on_failure: [\n                  {\n                    set: {\n                      description: "Set \'error.message.multi\'",\n                      field: \'error.message.multi\',\n                      value: \'Document encountered multiple ingest errors\',\n                      override: true\n                    }\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      rename: {\n        description: "Rename \'provider\' to \'cloud.provider\'",\n        field: "provider",\n        target_field: "cloud.provider",\n        on_failure: [\n          {\n            set: {\n              description: "Set \'error.message\'",\n              field: "error.message",\n              value:\n                "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              override: false,\n              on_failure: [\n                {\n                  set: {\n                    description: "Set \'error.message.multi\'",\n                    field: "error.message.multi",\n                    value: "Document encountered multiple ingest errors",\n                    override: true,\n                  },\n                },\n              ],\n            },\n          },\n        ],\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "rename": {\n        "description": "Rename \'provider\' to \'cloud.provider\'",\n        "field": "provider",\n        "target_field": "cloud.provider",\n        "on_failure": [\n          {\n            "set": {\n              "description": "Set \'error.message\'",\n              "field": "error.message",\n              "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              "override": false,\n              "on_failure": [\n                {\n                  "set": {\n                    "description": "Set \'error.message.multi\'",\n                    "field": "error.message.multi",\n                    "value": "Document encountered multiple ingest errors",\n                    "override": true\n                  }\n                }\n              ]\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\nYou can also specify `on_failure` for a pipeline. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. Elasticsearch will not attempt to run the pipeline’s remaining processors.\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [ ... ],\n  "on_failure": [\n    {\n      "set": {\n        "description": "Index document to \'failed-<index>\'",\n        "field": "_index",\n        "value": "failed-{{{ _index }}}"\n      }\n    }\n  ]\n}\n```\n\nAdditional information about the pipeline failure may be available in the document metadata fields `on_failure_message`, `on_failure_processor_type`, `on_failure_processor_tag`, and `on_failure_pipeline`. These fields are accessible only from within an `on_failure` block.\n\nThe following example uses the metadata fields to include information about pipeline failures in documents.\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [ ... ],\n  "on_failure": [\n    {\n      "set": {\n        "description": "Record error information",\n        "field": "error_information",\n        "value": "Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}"\n      }\n    }\n  ]\n}\n```\n\n### Conditionally run a processor\n\nEach processor supports an optional `if` condition, written as a [Painless script](/guide/en/elasticsearch/painless/8.17/painless-guide.html). If provided, the processor only runs when the `if` condition is `true`.\n\n`if` condition scripts run in Painless’s [ingest processor context](/guide/en/elasticsearch/painless/8.17/painless-ingest-processor-context.html). In `if` conditions, `ctx` values are read-only.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents with \'network.name\' of \'Guest\'",\n                "if": "ctx?.network?.name == \'Guest\'"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        drop: {\n          description: "Drop documents with \'network.name\' of \'Guest\'",\n          if: "ctx?.network?.name == \'Guest\'"\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents with \'network.name\' of \'Guest\'",\n        if: "ctx?.network?.name == \'Guest\'",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents with \'network.name\' of \'Guest\'",\n        "if": "ctx?.network?.name == \'Guest\'"\n      }\n    }\n  ]\n}\n```\n\nIf the [`script.painless.regex.enabled`](circuit-breaker.html#script-painless-regex-enabled) cluster setting is enabled, you can use regular expressions in your `if` condition scripts. For supported syntax, see [Painless regular expressions](/guide/en/elasticsearch/painless/8.17/painless-regexes.html).\n\nIf possible, avoid using regular expressions. Expensive regular expressions can slow indexing speeds.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n                "if": "ctx.url?.scheme =~ /^http[^s]/",\n                "field": "url.insecure",\n                "value": True\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n          if: \'ctx.url?.scheme =~ /^http[^s]/\',\n          field: \'url.insecure\',\n          value: true\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n        if: "ctx.url?.scheme =~ /^http[^s]/",\n        field: "url.insecure",\n        value: true,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n        "if": "ctx.url?.scheme =~ /^http[^s]/",\n        "field": "url.insecure",\n        "value": true\n      }\n    }\n  ]\n}\n```\n\nYou must specify `if` conditions as valid JSON on a single line. However, you can use the [Kibana console](/guide/en/kibana/8.17/console-kibana.html#configuring-console)\'s triple quote syntax to write and debug larger scripts.\n\nIf possible, avoid using complex or expensive `if` condition scripts. Expensive condition scripts can slow indexing speeds.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents that don\'t contain \'prod\' tag",\n                "if": "\\n            Collection tags = ctx.tags;\\n            if(tags != null){\\n              for (String tag : tags) {\\n                if (tag.toLowerCase().contains(\'prod\')) {\\n                  return false;\\n                }\\n              }\\n            }\\n            return true;\\n        "\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents that don\'t contain \'prod\' tag",\n        if: "\\n            Collection tags = ctx.tags;\\n            if(tags != null){\\n              for (String tag : tags) {\\n                if (tag.toLowerCase().contains(\'prod\')) {\\n                  return false;\\n                }\\n              }\\n            }\\n            return true;\\n        ",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents that don\'t contain \'prod\' tag",\n        "if": """\n            Collection tags = ctx.tags;\n            if(tags != null){\n              for (String tag : tags) {\n                if (tag.toLowerCase().contains(\'prod\')) {\n                  return false;\n                }\n              }\n            }\n            return true;\n        """\n      }\n    }\n  ]\n}\n```\n\nYou can also specify a [stored script](modules-scripting-using.html#script-stored-scripts "Store and retrieve scripts") as the `if` condition.\n\n```\nresp = client.put_script(\n    id="my-prod-tag-script",\n    script={\n        "lang": "painless",\n        "source": "\\n      Collection tags = ctx.tags;\\n      if(tags != null){\\n        for (String tag : tags) {\\n          if (tag.toLowerCase().contains(\'prod\')) {\\n            return false;\\n          }\\n        }\\n      }\\n      return true;\\n    "\n    },\n)\nprint(resp)\n\nresp1 = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents that don\'t contain \'prod\' tag",\n                "if": {\n                    "id": "my-prod-tag-script"\n                }\n            }\n        }\n    ],\n)\nprint(resp1)\n```\n\n```\nconst response = await client.putScript({\n  id: "my-prod-tag-script",\n  script: {\n    lang: "painless",\n    source:\n      "\\n      Collection tags = ctx.tags;\\n      if(tags != null){\\n        for (String tag : tags) {\\n          if (tag.toLowerCase().contains(\'prod\')) {\\n            return false;\\n          }\\n        }\\n      }\\n      return true;\\n    ",\n  },\n});\nconsole.log(response);\n\nconst response1 = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents that don\'t contain \'prod\' tag",\n        if: {\n          id: "my-prod-tag-script",\n        },\n      },\n    },\n  ],\n});\nconsole.log(response1);\n```\n\n```\nPUT _scripts/my-prod-tag-script\n{\n  "script": {\n    "lang": "painless",\n    "source": """\n      Collection tags = ctx.tags;\n      if(tags != null){\n        for (String tag : tags) {\n          if (tag.toLowerCase().contains(\'prod\')) {\n            return false;\n          }\n        }\n      }\n      return true;\n    """\n  }\n}\n\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents that don\'t contain \'prod\' tag",\n        "if": { "id": "my-prod-tag-script" }\n      }\n    }\n  ]\n}\n```\n\nIncoming documents often contain object fields. If a processor script attempts to access a field whose parent object does not exist, Elasticsearch returns a `NullPointerException`. To avoid these exceptions, use [null safe operators](/guide/en/elasticsearch/painless/8.17/painless-operators-reference.html#null-safe-operator), such as `?.`, and write your scripts to be null safe.\n\nFor example, `ctx.network?.name.equalsIgnoreCase(\'Guest\')` is not null safe. `ctx.network?.name` can return null. Rewrite the script as `\'Guest\'.equalsIgnoreCase(ctx.network?.name)`, which is null safe because `Guest` is always non-null.\n\nIf you can’t rewrite a script to be null safe, include an explicit null check.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents that contain \'network.name\' of \'Guest\'",\n                "if": "ctx.network?.name != null && ctx.network.name.contains(\'Guest\')"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents that contain \'network.name\' of \'Guest\'",\n        if: "ctx.network?.name != null && ctx.network.name.contains(\'Guest\')",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents that contain \'network.name\' of \'Guest\'",\n        "if": "ctx.network?.name != null && ctx.network.name.contains(\'Guest\')"\n      }\n    }\n  ]\n}\n```\n\n### Conditionally apply pipelines\n\nCombine an `if` condition with the [`pipeline`](pipeline-processor.html "Pipeline processor") processor to apply other pipelines to documents based on your criteria. You can use this pipeline as the [default pipeline](ingest.html#set-default-pipeline "Set a default pipeline") in an [index template](index-templates.html "Index templates") used to configure multiple data streams or indices.\n\n```\nresp = client.ingest.put_pipeline(\n    id="one-pipeline-to-rule-them-all",\n    processors=[\n        {\n            "pipeline": {\n                "description": "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n                "if": "ctx.service?.name == \'apache_httpd\'",\n                "name": "httpd_pipeline"\n            }\n        },\n        {\n            "pipeline": {\n                "description": "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n                "if": "ctx.service?.name == \'syslog\'",\n                "name": "syslog_pipeline"\n            }\n        },\n        {\n            "fail": {\n                "description": "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n                "if": "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n                "message": "This pipeline requires service.name to be either `syslog` or `apache_httpd`"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'one-pipeline-to-rule-them-all\',\n  body: {\n    processors: [\n      {\n        pipeline: {\n          description: "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n          if: "ctx.service?.name == \'apache_httpd\'",\n          name: \'httpd_pipeline\'\n        }\n      },\n      {\n        pipeline: {\n          description: "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n          if: "ctx.service?.name == \'syslog\'",\n          name: \'syslog_pipeline\'\n        }\n      },\n      {\n        fail: {\n          description: "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n          if: "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n          message: \'This pipeline requires service.name to be either `syslog` or `apache_httpd`\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "one-pipeline-to-rule-them-all",\n  processors: [\n    {\n      pipeline: {\n        description:\n          "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n        if: "ctx.service?.name == \'apache_httpd\'",\n        name: "httpd_pipeline",\n      },\n    },\n    {\n      pipeline: {\n        description: "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n        if: "ctx.service?.name == \'syslog\'",\n        name: "syslog_pipeline",\n      },\n    },\n    {\n      fail: {\n        description:\n          "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n        if: "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n        message:\n          "This pipeline requires service.name to be either `syslog` or `apache_httpd`",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/one-pipeline-to-rule-them-all\n{\n  "processors": [\n    {\n      "pipeline": {\n        "description": "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n        "if": "ctx.service?.name == \'apache_httpd\'",\n        "name": "httpd_pipeline"\n      }\n    },\n    {\n      "pipeline": {\n        "description": "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n        "if": "ctx.service?.name == \'syslog\'",\n        "name": "syslog_pipeline"\n      }\n    },\n    {\n      "fail": {\n        "description": "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n        "if": "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n        "message": "This pipeline requires service.name to be either `syslog` or `apache_httpd`"\n      }\n    }\n  ]\n}\n```\n\n### Get pipeline usage statistics\n\nUse the [node stats](cluster-nodes-stats.html "Nodes stats API") API to get global and per-pipeline ingest statistics. Use these stats to determine which pipelines run most frequently or spend the most time processing.\n\n```\nresp = client.nodes.stats(\n    metric="ingest",\n    filter_path="nodes.*.ingest",\n)\nprint(resp)\n```\n\n```\nresponse = client.nodes.stats(\n  metric: \'ingest\',\n  filter_path: \'nodes.*.ingest\'\n)\nputs response\n```\n\n```\nconst response = await client.nodes.stats({\n  metric: "ingest",\n  filter_path: "nodes.*.ingest",\n});\nconsole.log(response);\n```\n\n```\nGET _nodes/stats/ingest?filter_path=nodes.*.ingest\n```\n',
          title: 'Ingest pipelines',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest.html',
          productName: 'elasticsearch',
          score: 157.81056,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest.html',
        title: 'Ingest pipelines',
        score: 157.81056,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Ingest pipelines\\n\\nIngest pipelines let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.\\n\\nA pipeline consists of a series of configurable tasks called [processors](processors.html \\"Ingest processor reference\\"). Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, Elasticsearch adds the transformed documents to your data stream or index.\\n\\n![Ingest pipeline diagram](images/ingest/ingest-process.svg)\\n\\nYou can create and manage ingest pipelines using Kibana’s **Ingest Pipelines** feature or the [ingest APIs](ingest-apis.html \\"Ingest APIs\\"). Elasticsearch stores pipelines in the [cluster state](cluster-state.html \\"Cluster state API\\").\\n\\n### Prerequisites\\n\\n* Nodes with the [`ingest`](modules-node.html#node-ingest-node \\"Ingest node\\") node role handle pipeline processing. To use ingest pipelines, your cluster must have at least one node with the `ingest` role. For heavy ingest loads, we recommend creating [dedicated ingest nodes](modules-node.html#node-ingest-node \\"Ingest node\\").\\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to manage ingest pipelines. To use Kibana’s **Ingest Pipelines** feature, you also need the `cluster:monitor/nodes/info` cluster privileges.\\n* Pipelines including the `enrich` processor require additional setup. See [*Enrich your data*](ingest-enriching-data.html \\"Enrich your data\\").\\n\\n### Create and manage pipelines\\n\\nIn Kibana, open the main menu and click **Stack Management > Ingest Pipelines**. From the list view, you can:\\n\\n* View a list of your pipelines and drill down into details\\n* Edit or clone existing pipelines\\n* Delete pipelines\\n\\n![Kibana’s Ingest Pipelines list view](images/ingest/ingest-pipeline-list.png)\\n\\nTo create a pipeline, click **Create pipeline > New pipeline**. For an example tutorial, see [*Example: Parse logs*](common-log-format-example.html \\"Example: Parse logs in the Common Log Format\\").\\n\\nThe **New pipeline from CSV** option lets you use a CSV to create an ingest pipeline that maps custom data to the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16). Mapping your custom data to ECS makes the data easier to search and lets you reuse visualizations from other datasets. To get started, check [Map custom data to ECS](/guide/en/ecs/8.16/ecs-converting.html).\\n\\nYou can also use the [ingest APIs](ingest-apis.html \\"Ingest APIs\\") to create and manage pipelines. The following [create pipeline API](put-pipeline-api.html \\"Create or update pipeline API\\") request creates a pipeline containing two [`set`](set-processor.html \\"Set processor\\") processors followed by a [`lowercase`](lowercase-processor.html \\"Lowercase processor\\") processor. The processors run sequentially in the order specified.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    description=\\"My optional pipeline description\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"My optional processor description\\",\\n                \\"field\\": \\"my-long-field\\",\\n                \\"value\\": 10\\n            }\\n        },\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"Set \'my-boolean-field\' to true\\",\\n                \\"field\\": \\"my-boolean-field\\",\\n                \\"value\\": True\\n            }\\n        },\\n        {\\n            \\"lowercase\\": {\\n                \\"field\\": \\"my-keyword-field\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    description: \'My optional pipeline description\',\\n    processors: [\\n      {\\n        set: {\\n          description: \'My optional processor description\',\\n          field: \'my-long-field\',\\n          value: 10\\n        }\\n      },\\n      {\\n        set: {\\n          description: \\"Set \'my-boolean-field\' to true\\",\\n          field: \'my-boolean-field\',\\n          value: true\\n        }\\n      },\\n      {\\n        lowercase: {\\n          field: \'my-keyword-field\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  description: \\"My optional pipeline description\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"My optional processor description\\",\\n        field: \\"my-long-field\\",\\n        value: 10,\\n      },\\n    },\\n    {\\n      set: {\\n        description: \\"Set \'my-boolean-field\' to true\\",\\n        field: \\"my-boolean-field\\",\\n        value: true,\\n      },\\n    },\\n    {\\n      lowercase: {\\n        field: \\"my-keyword-field\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"description\\": \\"My optional pipeline description\\",\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"My optional processor description\\",\\n        \\"field\\": \\"my-long-field\\",\\n        \\"value\\": 10\\n      }\\n    },\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Set \'my-boolean-field\' to true\\",\\n        \\"field\\": \\"my-boolean-field\\",\\n        \\"value\\": true\\n      }\\n    },\\n    {\\n      \\"lowercase\\": {\\n        \\"field\\": \\"my-keyword-field\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Manage pipeline versions\\n\\nWhen you create or update a pipeline, you can specify an optional `version` integer. You can use this version number with the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params \\"Query parameters\\") parameter to conditionally update the pipeline. When the `if_version` parameter is specified, a successful update increments the pipeline’s version.\\n\\n```\\nPUT _ingest/pipeline/my-pipeline-id\\n{\\n  \\"version\\": 1,\\n  \\"processors\\": [ ... ]\\n}\\n```\\n\\nTo unset the `version` number using the API, replace or update the pipeline without specifying the `version` parameter.\\n\\n### Test a pipeline\\n\\nBefore using a pipeline in production, we recommend you test it using sample documents. When creating or editing a pipeline in Kibana, click **Add documents**. In the **Documents** tab, provide sample documents and click **Run the pipeline**.\\n\\n![Test a pipeline in Kibana](images/ingest/test-a-pipeline.png)\\n\\nYou can also test pipelines using the [simulate pipeline API](simulate-pipeline-api.html \\"Simulate pipeline API\\"). You can specify a configured pipeline in the request path. For example, the following request tests `my-pipeline`.\\n\\n```\\nresp = client.ingest.simulate(\\n    id=\\"my-pipeline\\",\\n    docs=[\\n        {\\n            \\"_source\\": {\\n                \\"my-keyword-field\\": \\"FOO\\"\\n            }\\n        },\\n        {\\n            \\"_source\\": {\\n                \\"my-keyword-field\\": \\"BAR\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.simulate(\\n  id: \'my-pipeline\',\\n  body: {\\n    docs: [\\n      {\\n        _source: {\\n          \\"my-keyword-field\\": \'FOO\'\\n        }\\n      },\\n      {\\n        _source: {\\n          \\"my-keyword-field\\": \'BAR\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  id: \\"my-pipeline\\",\\n  docs: [\\n    {\\n      _source: {\\n        \\"my-keyword-field\\": \\"FOO\\",\\n      },\\n    },\\n    {\\n      _source: {\\n        \\"my-keyword-field\\": \\"BAR\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST _ingest/pipeline/my-pipeline/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"my-keyword-field\\": \\"FOO\\"\\n      }\\n    },\\n    {\\n      \\"_source\\": {\\n        \\"my-keyword-field\\": \\"BAR\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nAlternatively, you can specify a pipeline and its processors in the request body.\\n\\n```\\nresp = client.ingest.simulate(\\n    pipeline={\\n        \\"processors\\": [\\n            {\\n                \\"lowercase\\": {\\n                    \\"field\\": \\"my-keyword-field\\"\\n                }\\n            }\\n        ]\\n    },\\n    docs=[\\n        {\\n            \\"_source\\": {\\n                \\"my-keyword-field\\": \\"FOO\\"\\n            }\\n        },\\n        {\\n            \\"_source\\": {\\n                \\"my-keyword-field\\": \\"BAR\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.simulate(\\n  body: {\\n    pipeline: {\\n      processors: [\\n        {\\n          lowercase: {\\n            field: \'my-keyword-field\'\\n          }\\n        }\\n      ]\\n    },\\n    docs: [\\n      {\\n        _source: {\\n          \\"my-keyword-field\\": \'FOO\'\\n        }\\n      },\\n      {\\n        _source: {\\n          \\"my-keyword-field\\": \'BAR\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  pipeline: {\\n    processors: [\\n      {\\n        lowercase: {\\n          field: \\"my-keyword-field\\",\\n        },\\n      },\\n    ],\\n  },\\n  docs: [\\n    {\\n      _source: {\\n        \\"my-keyword-field\\": \\"FOO\\",\\n      },\\n    },\\n    {\\n      _source: {\\n        \\"my-keyword-field\\": \\"BAR\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST _ingest/pipeline/_simulate\\n{\\n  \\"pipeline\\": {\\n    \\"processors\\": [\\n      {\\n        \\"lowercase\\": {\\n          \\"field\\": \\"my-keyword-field\\"\\n        }\\n      }\\n    ]\\n  },\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"my-keyword-field\\": \\"FOO\\"\\n      }\\n    },\\n    {\\n      \\"_source\\": {\\n        \\"my-keyword-field\\": \\"BAR\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe API returns transformed documents:\\n\\n```\\n{\\n  \\"docs\\": [\\n    {\\n      \\"doc\\": {\\n        \\"_index\\": \\"_index\\",\\n        \\"_id\\": \\"_id\\",\\n        \\"_version\\": \\"-3\\",\\n        \\"_source\\": {\\n          \\"my-keyword-field\\": \\"foo\\"\\n        },\\n        \\"_ingest\\": {\\n          \\"timestamp\\": \\"2099-03-07T11:04:03.000Z\\"\\n        }\\n      }\\n    },\\n    {\\n      \\"doc\\": {\\n        \\"_index\\": \\"_index\\",\\n        \\"_id\\": \\"_id\\",\\n        \\"_version\\": \\"-3\\",\\n        \\"_source\\": {\\n          \\"my-keyword-field\\": \\"bar\\"\\n        },\\n        \\"_ingest\\": {\\n          \\"timestamp\\": \\"2099-03-07T11:04:04.000Z\\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Add a pipeline to an indexing request\\n\\nUse the `pipeline` query parameter to apply a pipeline to documents in [individual](docs-index_.html \\"Index API\\") or [bulk](docs-bulk.html \\"Bulk API\\") indexing requests.\\n\\n```\\nresp = client.index(\\n    index=\\"my-data-stream\\",\\n    pipeline=\\"my-pipeline\\",\\n    document={\\n        \\"@timestamp\\": \\"2099-03-07T11:04:05.000Z\\",\\n        \\"my-keyword-field\\": \\"foo\\"\\n    },\\n)\\nprint(resp)\\n\\nresp1 = client.bulk(\\n    index=\\"my-data-stream\\",\\n    pipeline=\\"my-pipeline\\",\\n    operations=[\\n        {\\n            \\"create\\": {}\\n        },\\n        {\\n            \\"@timestamp\\": \\"2099-03-07T11:04:06.000Z\\",\\n            \\"my-keyword-field\\": \\"foo\\"\\n        },\\n        {\\n            \\"create\\": {}\\n        },\\n        {\\n            \\"@timestamp\\": \\"2099-03-07T11:04:07.000Z\\",\\n            \\"my-keyword-field\\": \\"bar\\"\\n        }\\n    ],\\n)\\nprint(resp1)\\n```\\n\\n```\\nresponse = client.index(\\n  index: \'my-data-stream\',\\n  pipeline: \'my-pipeline\',\\n  body: {\\n    \\"@timestamp\\": \'2099-03-07T11:04:05.000Z\',\\n    \\"my-keyword-field\\": \'foo\'\\n  }\\n)\\nputs response\\n\\nresponse = client.bulk(\\n  index: \'my-data-stream\',\\n  pipeline: \'my-pipeline\',\\n  body: [\\n    {\\n      create: {}\\n    },\\n    {\\n      \\"@timestamp\\": \'2099-03-07T11:04:06.000Z\',\\n      \\"my-keyword-field\\": \'foo\'\\n    },\\n    {\\n      create: {}\\n    },\\n    {\\n      \\"@timestamp\\": \'2099-03-07T11:04:07.000Z\',\\n      \\"my-keyword-field\\": \'bar\'\\n    }\\n  ]\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.index({\\n  index: \\"my-data-stream\\",\\n  pipeline: \\"my-pipeline\\",\\n  document: {\\n    \\"@timestamp\\": \\"2099-03-07T11:04:05.000Z\\",\\n    \\"my-keyword-field\\": \\"foo\\",\\n  },\\n});\\nconsole.log(response);\\n\\nconst response1 = await client.bulk({\\n  index: \\"my-data-stream\\",\\n  pipeline: \\"my-pipeline\\",\\n  operations: [\\n    {\\n      create: {},\\n    },\\n    {\\n      \\"@timestamp\\": \\"2099-03-07T11:04:06.000Z\\",\\n      \\"my-keyword-field\\": \\"foo\\",\\n    },\\n    {\\n      create: {},\\n    },\\n    {\\n      \\"@timestamp\\": \\"2099-03-07T11:04:07.000Z\\",\\n      \\"my-keyword-field\\": \\"bar\\",\\n    },\\n  ],\\n});\\nconsole.log(response1);\\n```\\n\\n```\\nPOST my-data-stream/_doc?pipeline=my-pipeline\\n{\\n  \\"@timestamp\\": \\"2099-03-07T11:04:05.000Z\\",\\n  \\"my-keyword-field\\": \\"foo\\"\\n}\\n\\nPUT my-data-stream/_bulk?pipeline=my-pipeline\\n{ \\"create\\":{ } }\\n{ \\"@timestamp\\": \\"2099-03-07T11:04:06.000Z\\", \\"my-keyword-field\\": \\"foo\\" }\\n{ \\"create\\":{ } }\\n{ \\"@timestamp\\": \\"2099-03-07T11:04:07.000Z\\", \\"my-keyword-field\\": \\"bar\\" }\\n```\\n\\nYou can also use the `pipeline` parameter with the [update by query](docs-update-by-query.html \\"Update By Query API\\") or [reindex](docs-reindex.html \\"Reindex API\\") APIs.\\n\\n```\\nresp = client.update_by_query(\\n    index=\\"my-data-stream\\",\\n    pipeline=\\"my-pipeline\\",\\n)\\nprint(resp)\\n\\nresp1 = client.reindex(\\n    source={\\n        \\"index\\": \\"my-data-stream\\"\\n    },\\n    dest={\\n        \\"index\\": \\"my-new-data-stream\\",\\n        \\"op_type\\": \\"create\\",\\n        \\"pipeline\\": \\"my-pipeline\\"\\n    },\\n)\\nprint(resp1)\\n```\\n\\n```\\nresponse = client.update_by_query(\\n  index: \'my-data-stream\',\\n  pipeline: \'my-pipeline\'\\n)\\nputs response\\n\\nresponse = client.reindex(\\n  body: {\\n    source: {\\n      index: \'my-data-stream\'\\n    },\\n    dest: {\\n      index: \'my-new-data-stream\',\\n      op_type: \'create\',\\n      pipeline: \'my-pipeline\'\\n    }\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.updateByQuery({\\n  index: \\"my-data-stream\\",\\n  pipeline: \\"my-pipeline\\",\\n});\\nconsole.log(response);\\n\\nconst response1 = await client.reindex({\\n  source: {\\n    index: \\"my-data-stream\\",\\n  },\\n  dest: {\\n    index: \\"my-new-data-stream\\",\\n    op_type: \\"create\\",\\n    pipeline: \\"my-pipeline\\",\\n  },\\n});\\nconsole.log(response1);\\n```\\n\\n```\\nPOST my-data-stream/_update_by_query?pipeline=my-pipeline\\n\\nPOST _reindex\\n{\\n  \\"source\\": {\\n    \\"index\\": \\"my-data-stream\\"\\n  },\\n  \\"dest\\": {\\n    \\"index\\": \\"my-new-data-stream\\",\\n    \\"op_type\\": \\"create\\",\\n    \\"pipeline\\": \\"my-pipeline\\"\\n  }\\n}\\n```\\n\\n### Set a default pipeline\\n\\nUse the [`index.default_pipeline`](index-modules.html#index-default-pipeline) index setting to set a default pipeline. Elasticsearch applies this pipeline to indexing requests if no `pipeline` parameter is specified.\\n\\n### Set a final pipeline\\n\\nUse the [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting to set a final pipeline. Elasticsearch applies this pipeline after the request or default pipeline, even if neither is specified.\\n\\n### Pipelines for Beats\\n\\nTo add an ingest pipeline to an Elastic Beat, specify the `pipeline` parameter under `output.elasticsearch` in `<BEAT_NAME>.yml`. For example, for Filebeat, you’d specify `pipeline` in `filebeat.yml`.\\n\\n```\\noutput.elasticsearch:\\n  hosts: [\\"localhost:9200\\"]\\n  pipeline: my-pipeline\\n```\\n\\n### Pipelines for Fleet and Elastic Agent\\n\\nElastic Agent integrations ship with default ingest pipelines that preprocess and enrich data before indexing. [Fleet](/guide/en/fleet/8.17/index.html) applies these pipelines using [index templates](index-templates.html \\"Index templates\\") that include [pipeline index settings](ingest.html#set-default-pipeline \\"Set a default pipeline\\"). Elasticsearch matches these templates to your Fleet data streams based on the [stream’s naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme).\\n\\nEach default integration pipeline calls a nonexistent, unversioned `*@custom` ingest pipeline. If unaltered, this pipeline call has no effect on your data. However, you can modify this call to create custom pipelines for integrations that persist across upgrades. Refer to [Tutorial: Transform data with custom ingest pipelines](/guide/en/fleet/8.17/data-streams-pipeline-tutorial.html) to learn more.\\n\\nFleet doesn’t provide a default ingest pipeline for the **Custom logs** integration, but you can specify a pipeline for this integration using an [index template](ingest.html#pipeline-custom-logs-index-template) or a [custom configuration](ingest.html#pipeline-custom-logs-configuration).\\n\\n**Option 1: Index template**\\n\\n1. [Create](ingest.html#create-manage-ingest-pipelines \\"Create and manage pipelines\\") and [test](ingest.html#test-pipeline \\"Test a pipeline\\") your ingest pipeline. Name your pipeline `logs-<dataset-name>-default`. This makes tracking the pipeline for your integration easier.\\n\\n   For example, the following request creates a pipeline for the `my-app` dataset. The pipeline’s name is `logs-my_app-default`.\\n\\n   \\n\\n   ```\\n   PUT _ingest/pipeline/logs-my_app-default\\n   {\\n     \\"description\\": \\"Pipeline for `my_app` dataset\\",\\n     \\"processors\\": [ ... ]\\n   }\\n   ```\\n\\n2. Create an [index template](index-templates.html \\"Index templates\\") that includes your pipeline in the [`index.default_pipeline`](index-modules.html#index-default-pipeline) or [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting. Ensure the template is [data stream enabled](set-up-a-data-stream.html#create-index-template \\"Create an index template\\"). The template’s index pattern should match `logs-<dataset-name>-*`.\\n\\n   You can create this template using Kibana’s [**Index Management**](index-mgmt.html#manage-index-templates \\"Manage index templates\\") feature or the [create index template API](indices-put-template.html \\"Create or update index template API\\").\\n\\n   For example, the following request creates a template matching `logs-my_app-*`. The template uses a component template that contains the `index.default_pipeline` index setting.\\n\\n   ```\\n   resp = client.cluster.put_component_template(\\n       name=\\"logs-my_app-settings\\",\\n       template={\\n           \\"settings\\": {\\n               \\"index.default_pipeline\\": \\"logs-my_app-default\\",\\n               \\"index.lifecycle.name\\": \\"logs\\"\\n           }\\n       },\\n   )\\n   print(resp)\\n\\n   resp1 = client.indices.put_index_template(\\n       name=\\"logs-my_app-template\\",\\n       index_patterns=[\\n           \\"logs-my_app-*\\"\\n       ],\\n       data_stream={},\\n       priority=500,\\n       composed_of=[\\n           \\"logs-my_app-settings\\",\\n           \\"logs-my_app-mappings\\"\\n       ],\\n   )\\n   print(resp1)\\n   ```\\n\\n   ```\\n   const response = await client.cluster.putComponentTemplate({\\n     name: \\"logs-my_app-settings\\",\\n     template: {\\n       settings: {\\n         \\"index.default_pipeline\\": \\"logs-my_app-default\\",\\n         \\"index.lifecycle.name\\": \\"logs\\",\\n       },\\n     },\\n   });\\n   console.log(response);\\n\\n   const response1 = await client.indices.putIndexTemplate({\\n     name: \\"logs-my_app-template\\",\\n     index_patterns: [\\"logs-my_app-*\\"],\\n     data_stream: {},\\n     priority: 500,\\n     composed_of: [\\"logs-my_app-settings\\", \\"logs-my_app-mappings\\"],\\n   });\\n   console.log(response1);\\n   ```\\n\\n   \\n\\n   ```\\n   # Creates a component template for index settings\\n   PUT _component_template/logs-my_app-settings\\n   {\\n     \\"template\\": {\\n       \\"settings\\": {\\n         \\"index.default_pipeline\\": \\"logs-my_app-default\\",\\n         \\"index.lifecycle.name\\": \\"logs\\"\\n       }\\n     }\\n   }\\n\\n   # Creates an index template matching `logs-my_app-*`\\n   PUT _index_template/logs-my_app-template\\n   {\\n     \\"index_patterns\\": [\\"logs-my_app-*\\"],\\n     \\"data_stream\\": { },\\n     \\"priority\\": 500,\\n     \\"composed_of\\": [\\"logs-my_app-settings\\", \\"logs-my_app-mappings\\"]\\n   }\\n   ```\\n\\n3. When adding or editing your **Custom logs** integration in Fleet, click **Configure integration > Custom log file > Advanced options**.\\n\\n4. In **Dataset name**, specify your dataset’s name. Fleet will add new data for the integration to the resulting `logs-<dataset-name>-default` data stream.\\n\\n   For example, if your dataset’s name was `my_app`, Fleet adds new data to the `logs-my_app-default` data stream.\\n\\n   ![Set up custom log integration in Fleet](images/ingest/custom-logs.png)\\n\\n5. Use the [rollover API](indices-rollover-index.html \\"Rollover API\\") to roll over your data stream. This ensures Elasticsearch applies the index template and its pipeline settings to any new data for the integration.\\n\\n   ```\\n   resp = client.indices.rollover(\\n       alias=\\"logs-my_app-default\\",\\n   )\\n   print(resp)\\n   ```\\n\\n   ```\\n   response = client.indices.rollover(\\n     alias: \'logs-my_app-default\'\\n   )\\n   puts response\\n   ```\\n\\n   ```\\n   const response = await client.indices.rollover({\\n     alias: \\"logs-my_app-default\\",\\n   });\\n   console.log(response);\\n   ```\\n\\n   \\n\\n   ```\\n   POST logs-my_app-default/_rollover/\\n   ```\\n\\n**Option 2: Custom configuration**\\n\\n1. [Create](ingest.html#create-manage-ingest-pipelines \\"Create and manage pipelines\\") and [test](ingest.html#test-pipeline \\"Test a pipeline\\") your ingest pipeline. Name your pipeline `logs-<dataset-name>-default`. This makes tracking the pipeline for your integration easier.\\n\\n   For example, the following request creates a pipeline for the `my-app` dataset. The pipeline’s name is `logs-my_app-default`.\\n\\n   \\n\\n   ```\\n   PUT _ingest/pipeline/logs-my_app-default\\n   {\\n     \\"description\\": \\"Pipeline for `my_app` dataset\\",\\n     \\"processors\\": [ ... ]\\n   }\\n   ```\\n\\n2. When adding or editing your **Custom logs** integration in Fleet, click **Configure integration > Custom log file > Advanced options**.\\n\\n3. In **Dataset name**, specify your dataset’s name. Fleet will add new data for the integration to the resulting `logs-<dataset-name>-default` data stream.\\n\\n   For example, if your dataset’s name was `my_app`, Fleet adds new data to the `logs-my_app-default` data stream.\\n\\n4. In **Custom Configurations**, specify your pipeline in the `pipeline` policy setting.\\n\\n   ![Custom pipeline configuration for custom log integration](images/ingest/custom-logs-pipeline.png)\\n\\n**Elastic Agent standalone**\\n\\nIf you run Elastic Agent standalone, you can apply pipelines using an [index template](index-templates.html \\"Index templates\\") that includes the [`index.default_pipeline`](index-modules.html#index-default-pipeline) or [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting. Alternatively, you can specify the `pipeline` policy setting in your `elastic-agent.yml` configuration. See [Install standalone Elastic Agents](/guide/en/fleet/8.17/install-standalone-elastic-agent.html).\\n\\n### Pipelines for search indices\\n\\nWhen you create Elasticsearch indices for search use cases, for example, using the [web crawler](/guide/en/enterprise-search/8.17/crawler.html) or [connectors](es-connectors.html \\"Ingest content with Elastic connectors\\"), these indices are automatically set up with specific ingest pipelines. These processors help optimize your content for search. See [*Ingest pipelines in Search*](ingest-pipeline-search.html \\"Ingest pipelines in Search\\") for more information.\\n\\n### Access source fields in a processor\\n\\nProcessors have read and write access to an incoming document’s source fields. To access a field key in a processor, use its field name. The following `set` processor accesses `my-long-field`.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"field\\": \\"my-long-field\\",\\n                \\"value\\": 10\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          field: \'my-long-field\',\\n          value: 10\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        field: \\"my-long-field\\",\\n        value: 10,\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"field\\": \\"my-long-field\\",\\n        \\"value\\": 10\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYou can also prepend the `_source` prefix.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"field\\": \\"_source.my-long-field\\",\\n                \\"value\\": 10\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          field: \'_source.my-long-field\',\\n          value: 10\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        field: \\"_source.my-long-field\\",\\n        value: 10,\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"field\\": \\"_source.my-long-field\\",\\n        \\"value\\": 10\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nUse dot notation to access object fields.\\n\\nIf your document contains flattened objects, use the [`dot_expander`](dot-expand-processor.html \\"Dot expander processor\\") processor to expand them first. Other ingest processors cannot access flattened objects.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"dot_expander\\": {\\n                \\"description\\": \\"Expand \'my-object-field.my-property\'\\",\\n                \\"field\\": \\"my-object-field.my-property\\"\\n            }\\n        },\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"Set \'my-object-field.my-property\' to 10\\",\\n                \\"field\\": \\"my-object-field.my-property\\",\\n                \\"value\\": 10\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        dot_expander: {\\n          description: \\"Expand \'my-object-field.my-property\'\\",\\n          field: \'my-object-field.my-property\'\\n        }\\n      },\\n      {\\n        set: {\\n          description: \\"Set \'my-object-field.my-property\' to 10\\",\\n          field: \'my-object-field.my-property\',\\n          value: 10\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      dot_expander: {\\n        description: \\"Expand \'my-object-field.my-property\'\\",\\n        field: \\"my-object-field.my-property\\",\\n      },\\n    },\\n    {\\n      set: {\\n        description: \\"Set \'my-object-field.my-property\' to 10\\",\\n        field: \\"my-object-field.my-property\\",\\n        value: 10,\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"dot_expander\\": {\\n        \\"description\\": \\"Expand \'my-object-field.my-property\'\\",\\n        \\"field\\": \\"my-object-field.my-property\\"\\n      }\\n    },\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Set \'my-object-field.my-property\' to 10\\",\\n        \\"field\\": \\"my-object-field.my-property\\",\\n        \\"value\\": 10\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nSeveral processor parameters support [Mustache](https://mustache.github.io) template snippets. To access field values in a template snippet, enclose the field name in triple curly brackets:`{{{field-name}}}`. You can use template snippets to dynamically set field names.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"Set dynamic \'<service>\' field to \'code\' value\\",\\n                \\"field\\": \\"{{{service}}}\\",\\n                \\"value\\": \\"{{{code}}}\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          description: \\"Set dynamic \'<service>\' field to \'code\' value\\",\\n          field: \'{{{service}}}\',\\n          value: \'{{{code}}}\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"Set dynamic \'<service>\' field to \'code\' value\\",\\n        field: \\"{{{service}}}\\",\\n        value: \\"{{{code}}}\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Set dynamic \'<service>\' field to \'code\' value\\",\\n        \\"field\\": \\"{{{service}}}\\",\\n        \\"value\\": \\"{{{code}}}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Access metadata fields in a processor\\n\\nProcessors can access the following metadata fields by name:\\n\\n* `_index`\\n* `_id`\\n* `_routing`\\n* `_dynamic_templates`\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"Set \'_routing\' to \'geoip.country_iso_code\' value\\",\\n                \\"field\\": \\"_routing\\",\\n                \\"value\\": \\"{{{geoip.country_iso_code}}}\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          description: \\"Set \'_routing\' to \'geoip.country_iso_code\' value\\",\\n          field: \'_routing\',\\n          value: \'{{{geoip.country_iso_code}}}\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"Set \'_routing\' to \'geoip.country_iso_code\' value\\",\\n        field: \\"_routing\\",\\n        value: \\"{{{geoip.country_iso_code}}}\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Set \'_routing\' to \'geoip.country_iso_code\' value\\",\\n        \\"field\\": \\"_routing\\",\\n        \\"value\\": \\"{{{geoip.country_iso_code}}}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nUse a Mustache template snippet to access metadata field values. For example, `{{{_routing}}}` retrieves a document’s routing value.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"Use geo_point dynamic template for address field\\",\\n                \\"field\\": \\"_dynamic_templates\\",\\n                \\"value\\": {\\n                    \\"address\\": \\"geo_point\\"\\n                }\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          description: \'Use geo_point dynamic template for address field\',\\n          field: \'_dynamic_templates\',\\n          value: {\\n            address: \'geo_point\'\\n          }\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"Use geo_point dynamic template for address field\\",\\n        field: \\"_dynamic_templates\\",\\n        value: {\\n          address: \\"geo_point\\",\\n        },\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Use geo_point dynamic template for address field\\",\\n        \\"field\\": \\"_dynamic_templates\\",\\n        \\"value\\": {\\n          \\"address\\": \\"geo_point\\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe set processor above tells ES to use the dynamic template named `geo_point` for the field `address` if this field is not defined in the mapping of the index yet. This processor overrides the dynamic template for the field `address` if already defined in the bulk request, but has no effect on other dynamic templates defined in the bulk request.\\n\\nIf you [automatically generate](docs-index_.html#create-document-ids-automatically \\"Create document IDs automatically\\") document IDs, you cannot use `{{{_id}}}` in a processor. Elasticsearch assigns auto-generated `_id` values after ingest.\\n\\n### Access ingest metadata in a processor\\n\\nIngest processors can add and access ingest metadata using the `_ingest` key.\\n\\nUnlike source and metadata fields, Elasticsearch does not index ingest metadata fields by default. Elasticsearch also allows source fields that start with an `_ingest` key. If your data includes such source fields, use `_source._ingest` to access them.\\n\\nPipelines only create the `_ingest.timestamp` ingest metadata field by default. This field contains a timestamp of when Elasticsearch received the document’s indexing request. To index `_ingest.timestamp` or other ingest metadata fields, use the `set` processor.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"Index the ingest timestamp as \'event.ingested\'\\",\\n                \\"field\\": \\"event.ingested\\",\\n                \\"value\\": \\"{{{_ingest.timestamp}}}\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          description: \\"Index the ingest timestamp as \'event.ingested\'\\",\\n          field: \'event.ingested\',\\n          value: \'{{{_ingest.timestamp}}}\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"Index the ingest timestamp as \'event.ingested\'\\",\\n        field: \\"event.ingested\\",\\n        value: \\"{{{_ingest.timestamp}}}\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Index the ingest timestamp as \'event.ingested\'\\",\\n        \\"field\\": \\"event.ingested\\",\\n        \\"value\\": \\"{{{_ingest.timestamp}}}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Handling pipeline failures\\n\\nA pipeline’s processors run sequentially. By default, pipeline processing stops when one of these processors fails or encounters an error.\\n\\nTo ignore a processor failure and run the pipeline’s remaining processors, set `ignore_failure` to `true`.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"rename\\": {\\n                \\"description\\": \\"Rename \'provider\' to \'cloud.provider\'\\",\\n                \\"field\\": \\"provider\\",\\n                \\"target_field\\": \\"cloud.provider\\",\\n                \\"ignore_failure\\": True\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        rename: {\\n          description: \\"Rename \'provider\' to \'cloud.provider\'\\",\\n          field: \'provider\',\\n          target_field: \'cloud.provider\',\\n          ignore_failure: true\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      rename: {\\n        description: \\"Rename \'provider\' to \'cloud.provider\'\\",\\n        field: \\"provider\\",\\n        target_field: \\"cloud.provider\\",\\n        ignore_failure: true,\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"rename\\": {\\n        \\"description\\": \\"Rename \'provider\' to \'cloud.provider\'\\",\\n        \\"field\\": \\"provider\\",\\n        \\"target_field\\": \\"cloud.provider\\",\\n        \\"ignore_failure\\": true\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nUse the `on_failure` parameter to specify a list of processors to run immediately after a processor failure. If `on_failure` is specified, Elasticsearch afterward runs the pipeline’s remaining processors, even if the `on_failure` configuration is empty.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"rename\\": {\\n                \\"description\\": \\"Rename \'provider\' to \'cloud.provider\'\\",\\n                \\"field\\": \\"provider\\",\\n                \\"target_field\\": \\"cloud.provider\\",\\n                \\"on_failure\\": [\\n                    {\\n                        \\"set\\": {\\n                            \\"description\\": \\"Set \'error.message\'\\",\\n                            \\"field\\": \\"error.message\\",\\n                            \\"value\\": \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n                            \\"override\\": False\\n                        }\\n                    }\\n                ]\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        rename: {\\n          description: \\"Rename \'provider\' to \'cloud.provider\'\\",\\n          field: \'provider\',\\n          target_field: \'cloud.provider\',\\n          on_failure: [\\n            {\\n              set: {\\n                description: \\"Set \'error.message\'\\",\\n                field: \'error.message\',\\n                value: \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n                override: false\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      rename: {\\n        description: \\"Rename \'provider\' to \'cloud.provider\'\\",\\n        field: \\"provider\\",\\n        target_field: \\"cloud.provider\\",\\n        on_failure: [\\n          {\\n            set: {\\n              description: \\"Set \'error.message\'\\",\\n              field: \\"error.message\\",\\n              value:\\n                \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n              override: false,\\n            },\\n          },\\n        ],\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"rename\\": {\\n        \\"description\\": \\"Rename \'provider\' to \'cloud.provider\'\\",\\n        \\"field\\": \\"provider\\",\\n        \\"target_field\\": \\"cloud.provider\\",\\n        \\"on_failure\\": [\\n          {\\n            \\"set\\": {\\n              \\"description\\": \\"Set \'error.message\'\\",\\n              \\"field\\": \\"error.message\\",\\n              \\"value\\": \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n              \\"override\\": false\\n            }\\n          }\\n        ]\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nNest a list of `on_failure` processors for nested error handling.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"rename\\": {\\n                \\"description\\": \\"Rename \'provider\' to \'cloud.provider\'\\",\\n                \\"field\\": \\"provider\\",\\n                \\"target_field\\": \\"cloud.provider\\",\\n                \\"on_failure\\": [\\n                    {\\n                        \\"set\\": {\\n                            \\"description\\": \\"Set \'error.message\'\\",\\n                            \\"field\\": \\"error.message\\",\\n                            \\"value\\": \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n                            \\"override\\": False,\\n                            \\"on_failure\\": [\\n                                {\\n                                    \\"set\\": {\\n                                        \\"description\\": \\"Set \'error.message.multi\'\\",\\n                                        \\"field\\": \\"error.message.multi\\",\\n                                        \\"value\\": \\"Document encountered multiple ingest errors\\",\\n                                        \\"override\\": True\\n                                    }\\n                                }\\n                            ]\\n                        }\\n                    }\\n                ]\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        rename: {\\n          description: \\"Rename \'provider\' to \'cloud.provider\'\\",\\n          field: \'provider\',\\n          target_field: \'cloud.provider\',\\n          on_failure: [\\n            {\\n              set: {\\n                description: \\"Set \'error.message\'\\",\\n                field: \'error.message\',\\n                value: \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n                override: false,\\n                on_failure: [\\n                  {\\n                    set: {\\n                      description: \\"Set \'error.message.multi\'\\",\\n                      field: \'error.message.multi\',\\n                      value: \'Document encountered multiple ingest errors\',\\n                      override: true\\n                    }\\n                  }\\n                ]\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      rename: {\\n        description: \\"Rename \'provider\' to \'cloud.provider\'\\",\\n        field: \\"provider\\",\\n        target_field: \\"cloud.provider\\",\\n        on_failure: [\\n          {\\n            set: {\\n              description: \\"Set \'error.message\'\\",\\n              field: \\"error.message\\",\\n              value:\\n                \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n              override: false,\\n              on_failure: [\\n                {\\n                  set: {\\n                    description: \\"Set \'error.message.multi\'\\",\\n                    field: \\"error.message.multi\\",\\n                    value: \\"Document encountered multiple ingest errors\\",\\n                    override: true,\\n                  },\\n                },\\n              ],\\n            },\\n          },\\n        ],\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"rename\\": {\\n        \\"description\\": \\"Rename \'provider\' to \'cloud.provider\'\\",\\n        \\"field\\": \\"provider\\",\\n        \\"target_field\\": \\"cloud.provider\\",\\n        \\"on_failure\\": [\\n          {\\n            \\"set\\": {\\n              \\"description\\": \\"Set \'error.message\'\\",\\n              \\"field\\": \\"error.message\\",\\n              \\"value\\": \\"Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'\\",\\n              \\"override\\": false,\\n              \\"on_failure\\": [\\n                {\\n                  \\"set\\": {\\n                    \\"description\\": \\"Set \'error.message.multi\'\\",\\n                    \\"field\\": \\"error.message.multi\\",\\n                    \\"value\\": \\"Document encountered multiple ingest errors\\",\\n                    \\"override\\": true\\n                  }\\n                }\\n              ]\\n            }\\n          }\\n        ]\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYou can also specify `on_failure` for a pipeline. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. Elasticsearch will not attempt to run the pipeline’s remaining processors.\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [ ... ],\\n  \\"on_failure\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Index document to \'failed-<index>\'\\",\\n        \\"field\\": \\"_index\\",\\n        \\"value\\": \\"failed-{{{ _index }}}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nAdditional information about the pipeline failure may be available in the document metadata fields `on_failure_message`, `on_failure_processor_type`, `on_failure_processor_tag`, and `on_failure_pipeline`. These fields are accessible only from within an `on_failure` block.\\n\\nThe following example uses the metadata fields to include information about pipeline failures in documents.\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [ ... ],\\n  \\"on_failure\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"Record error information\\",\\n        \\"field\\": \\"error_information\\",\\n        \\"value\\": \\"Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Conditionally run a processor\\n\\nEach processor supports an optional `if` condition, written as a [Painless script](/guide/en/elasticsearch/painless/8.17/painless-guide.html). If provided, the processor only runs when the `if` condition is `true`.\\n\\n`if` condition scripts run in Painless’s [ingest processor context](/guide/en/elasticsearch/painless/8.17/painless-ingest-processor-context.html). In `if` conditions, `ctx` values are read-only.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"drop\\": {\\n                \\"description\\": \\"Drop documents with \'network.name\' of \'Guest\'\\",\\n                \\"if\\": \\"ctx?.network?.name == \'Guest\'\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        drop: {\\n          description: \\"Drop documents with \'network.name\' of \'Guest\'\\",\\n          if: \\"ctx?.network?.name == \'Guest\'\\"\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      drop: {\\n        description: \\"Drop documents with \'network.name\' of \'Guest\'\\",\\n        if: \\"ctx?.network?.name == \'Guest\'\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"drop\\": {\\n        \\"description\\": \\"Drop documents with \'network.name\' of \'Guest\'\\",\\n        \\"if\\": \\"ctx?.network?.name == \'Guest\'\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nIf the [`script.painless.regex.enabled`](circuit-breaker.html#script-painless-regex-enabled) cluster setting is enabled, you can use regular expressions in your `if` condition scripts. For supported syntax, see [Painless regular expressions](/guide/en/elasticsearch/painless/8.17/painless-regexes.html).\\n\\nIf possible, avoid using regular expressions. Expensive regular expressions can slow indexing speeds.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"If \'url.scheme\' is \'http\', set \'url.insecure\' to true\\",\\n                \\"if\\": \\"ctx.url?.scheme =~ /^http[^s]/\\",\\n                \\"field\\": \\"url.insecure\\",\\n                \\"value\\": True\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline\',\\n  body: {\\n    processors: [\\n      {\\n        set: {\\n          description: \\"If \'url.scheme\' is \'http\', set \'url.insecure\' to true\\",\\n          if: \'ctx.url?.scheme =~ /^http[^s]/\',\\n          field: \'url.insecure\',\\n          value: true\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"If \'url.scheme\' is \'http\', set \'url.insecure\' to true\\",\\n        if: \\"ctx.url?.scheme =~ /^http[^s]/\\",\\n        field: \\"url.insecure\\",\\n        value: true,\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"set\\": {\\n        \\"description\\": \\"If \'url.scheme\' is \'http\', set \'url.insecure\' to true\\",\\n        \\"if\\": \\"ctx.url?.scheme =~ /^http[^s]/\\",\\n        \\"field\\": \\"url.insecure\\",\\n        \\"value\\": true\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYou must specify `if` conditions as valid JSON on a single line. However, you can use the [Kibana console](/guide/en/kibana/8.17/console-kibana.html#configuring-console)\'s triple quote syntax to write and debug larger scripts.\\n\\nIf possible, avoid using complex or expensive `if` condition scripts. Expensive condition scripts can slow indexing speeds.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"drop\\": {\\n                \\"description\\": \\"Drop documents that don\'t contain \'prod\' tag\\",\\n                \\"if\\": \\"\\\\n            Collection tags = ctx.tags;\\\\n            if(tags != null){\\\\n              for (String tag : tags) {\\\\n                if (tag.toLowerCase().contains(\'prod\')) {\\\\n                  return false;\\\\n                }\\\\n              }\\\\n            }\\\\n            return true;\\\\n        \\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      drop: {\\n        description: \\"Drop documents that don\'t contain \'prod\' tag\\",\\n        if: \\"\\\\n            Collection tags = ctx.tags;\\\\n            if(tags != null){\\\\n              for (String tag : tags) {\\\\n                if (tag.toLowerCase().contains(\'prod\')) {\\\\n                  return false;\\\\n                }\\\\n              }\\\\n            }\\\\n            return true;\\\\n        \\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"drop\\": {\\n        \\"description\\": \\"Drop documents that don\'t contain \'prod\' tag\\",\\n        \\"if\\": \\"\\"\\"\\n            Collection tags = ctx.tags;\\n            if(tags != null){\\n              for (String tag : tags) {\\n                if (tag.toLowerCase().contains(\'prod\')) {\\n                  return false;\\n                }\\n              }\\n            }\\n            return true;\\n        \\"\\"\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYou can also specify a [stored script](modules-scripting-using.html#script-stored-scripts \\"Store and retrieve scripts\\") as the `if` condition.\\n\\n```\\nresp = client.put_script(\\n    id=\\"my-prod-tag-script\\",\\n    script={\\n        \\"lang\\": \\"painless\\",\\n        \\"source\\": \\"\\\\n      Collection tags = ctx.tags;\\\\n      if(tags != null){\\\\n        for (String tag : tags) {\\\\n          if (tag.toLowerCase().contains(\'prod\')) {\\\\n            return false;\\\\n          }\\\\n        }\\\\n      }\\\\n      return true;\\\\n    \\"\\n    },\\n)\\nprint(resp)\\n\\nresp1 = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"drop\\": {\\n                \\"description\\": \\"Drop documents that don\'t contain \'prod\' tag\\",\\n                \\"if\\": {\\n                    \\"id\\": \\"my-prod-tag-script\\"\\n                }\\n            }\\n        }\\n    ],\\n)\\nprint(resp1)\\n```\\n\\n```\\nconst response = await client.putScript({\\n  id: \\"my-prod-tag-script\\",\\n  script: {\\n    lang: \\"painless\\",\\n    source:\\n      \\"\\\\n      Collection tags = ctx.tags;\\\\n      if(tags != null){\\\\n        for (String tag : tags) {\\\\n          if (tag.toLowerCase().contains(\'prod\')) {\\\\n            return false;\\\\n          }\\\\n        }\\\\n      }\\\\n      return true;\\\\n    \\",\\n  },\\n});\\nconsole.log(response);\\n\\nconst response1 = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      drop: {\\n        description: \\"Drop documents that don\'t contain \'prod\' tag\\",\\n        if: {\\n          id: \\"my-prod-tag-script\\",\\n        },\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response1);\\n```\\n\\n```\\nPUT _scripts/my-prod-tag-script\\n{\\n  \\"script\\": {\\n    \\"lang\\": \\"painless\\",\\n    \\"source\\": \\"\\"\\"\\n      Collection tags = ctx.tags;\\n      if(tags != null){\\n        for (String tag : tags) {\\n          if (tag.toLowerCase().contains(\'prod\')) {\\n            return false;\\n          }\\n        }\\n      }\\n      return true;\\n    \\"\\"\\"\\n  }\\n}\\n\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"drop\\": {\\n        \\"description\\": \\"Drop documents that don\'t contain \'prod\' tag\\",\\n        \\"if\\": { \\"id\\": \\"my-prod-tag-script\\" }\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nIncoming documents often contain object fields. If a processor script attempts to access a field whose parent object does not exist, Elasticsearch returns a `NullPointerException`. To avoid these exceptions, use [null safe operators](/guide/en/elasticsearch/painless/8.17/painless-operators-reference.html#null-safe-operator), such as `?.`, and write your scripts to be null safe.\\n\\nFor example, `ctx.network?.name.equalsIgnoreCase(\'Guest\')` is not null safe. `ctx.network?.name` can return null. Rewrite the script as `\'Guest\'.equalsIgnoreCase(ctx.network?.name)`, which is null safe because `Guest` is always non-null.\\n\\nIf you can’t rewrite a script to be null safe, include an explicit null check.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline\\",\\n    processors=[\\n        {\\n            \\"drop\\": {\\n                \\"description\\": \\"Drop documents that contain \'network.name\' of \'Guest\'\\",\\n                \\"if\\": \\"ctx.network?.name != null && ctx.network.name.contains(\'Guest\')\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline\\",\\n  processors: [\\n    {\\n      drop: {\\n        description: \\"Drop documents that contain \'network.name\' of \'Guest\'\\",\\n        if: \\"ctx.network?.name != null && ctx.network.name.contains(\'Guest\')\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline\\n{\\n  \\"processors\\": [\\n    {\\n      \\"drop\\": {\\n        \\"description\\": \\"Drop documents that contain \'network.name\' of \'Guest\'\\",\\n        \\"if\\": \\"ctx.network?.name != null && ctx.network.name.contains(\'Guest\')\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Conditionally apply pipelines\\n\\nCombine an `if` condition with the [`pipeline`](pipeline-processor.html \\"Pipeline processor\\") processor to apply other pipelines to documents based on your criteria. You can use this pipeline as the [default pipeline](ingest.html#set-default-pipeline \\"Set a default pipeline\\") in an [index template](index-templates.html \\"Index templates\\") used to configure multiple data streams or indices.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"one-pipeline-to-rule-them-all\\",\\n    processors=[\\n        {\\n            \\"pipeline\\": {\\n                \\"description\\": \\"If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'\\",\\n                \\"if\\": \\"ctx.service?.name == \'apache_httpd\'\\",\\n                \\"name\\": \\"httpd_pipeline\\"\\n            }\\n        },\\n        {\\n            \\"pipeline\\": {\\n                \\"description\\": \\"If \'service.name\' is \'syslog\', use \'syslog_pipeline\'\\",\\n                \\"if\\": \\"ctx.service?.name == \'syslog\'\\",\\n                \\"name\\": \\"syslog_pipeline\\"\\n            }\\n        },\\n        {\\n            \\"fail\\": {\\n                \\"description\\": \\"If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message\\",\\n                \\"if\\": \\"ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'\\",\\n                \\"message\\": \\"This pipeline requires service.name to be either `syslog` or `apache_httpd`\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'one-pipeline-to-rule-them-all\',\\n  body: {\\n    processors: [\\n      {\\n        pipeline: {\\n          description: \\"If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'\\",\\n          if: \\"ctx.service?.name == \'apache_httpd\'\\",\\n          name: \'httpd_pipeline\'\\n        }\\n      },\\n      {\\n        pipeline: {\\n          description: \\"If \'service.name\' is \'syslog\', use \'syslog_pipeline\'\\",\\n          if: \\"ctx.service?.name == \'syslog\'\\",\\n          name: \'syslog_pipeline\'\\n        }\\n      },\\n      {\\n        fail: {\\n          description: \\"If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message\\",\\n          if: \\"ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'\\",\\n          message: \'This pipeline requires service.name to be either `syslog` or `apache_httpd`\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"one-pipeline-to-rule-them-all\\",\\n  processors: [\\n    {\\n      pipeline: {\\n        description:\\n          \\"If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'\\",\\n        if: \\"ctx.service?.name == \'apache_httpd\'\\",\\n        name: \\"httpd_pipeline\\",\\n      },\\n    },\\n    {\\n      pipeline: {\\n        description: \\"If \'service.name\' is \'syslog\', use \'syslog_pipeline\'\\",\\n        if: \\"ctx.service?.name == \'syslog\'\\",\\n        name: \\"syslog_pipeline\\",\\n      },\\n    },\\n    {\\n      fail: {\\n        description:\\n          \\"If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message\\",\\n        if: \\"ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'\\",\\n        message:\\n          \\"This pipeline requires service.name to be either `syslog` or `apache_httpd`\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/one-pipeline-to-rule-them-all\\n{\\n  \\"processors\\": [\\n    {\\n      \\"pipeline\\": {\\n        \\"description\\": \\"If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'\\",\\n        \\"if\\": \\"ctx.service?.name == \'apache_httpd\'\\",\\n        \\"name\\": \\"httpd_pipeline\\"\\n      }\\n    },\\n    {\\n      \\"pipeline\\": {\\n        \\"description\\": \\"If \'service.name\' is \'syslog\', use \'syslog_pipeline\'\\",\\n        \\"if\\": \\"ctx.service?.name == \'syslog\'\\",\\n        \\"name\\": \\"syslog_pipeline\\"\\n      }\\n    },\\n    {\\n      \\"fail\\": {\\n        \\"description\\": \\"If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message\\",\\n        \\"if\\": \\"ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'\\",\\n        \\"message\\": \\"This pipeline requires service.name to be either `syslog` or `apache_httpd`\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Get pipeline usage statistics\\n\\nUse the [node stats](cluster-nodes-stats.html \\"Nodes stats API\\") API to get global and per-pipeline ingest statistics. Use these stats to determine which pipelines run most frequently or spend the most time processing.\\n\\n```\\nresp = client.nodes.stats(\\n    metric=\\"ingest\\",\\n    filter_path=\\"nodes.*.ingest\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.nodes.stats(\\n  metric: \'ingest\',\\n  filter_path: \'nodes.*.ingest\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.nodes.stats({\\n  metric: \\"ingest\\",\\n  filter_path: \\"nodes.*.ingest\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nGET _nodes/stats/ingest?filter_path=nodes.*.ingest\\n```\\n","title":"Ingest pipelines","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest.html","productName":"elasticsearch","score":157.81056}',
        truncated: {
          truncatedText:
            '{"content":"## Ingest pipelines\\n\\nIngest pipelines let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.\\n\\nA pipeline consists of a series of configurable tasks called [processors](processors.html \\"Ingest processor reference\\"). Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, Elasticsearch adds the transformed documents to your data stream or index.\\n\\n![Ingest pipeline diagram](images/ingest/ingest-process.svg)\\n\\nYou can create and manage ingest pipelines using Kibana’s **Ingest Pipelines** feature or the [ingest APIs](ingest-apis.html \\"Ingest APIs\\"). Elasticsearch stores pipelines in the [cluster state](cluster-state.html \\"Cluster state API\\").\\n\\n### Prerequisites\\n\\n* Nodes with the [`ingest`](modules-node.html#node-ingest-node \\"Ingest node\\") node role handle pipeline processing. To use ingest pipelines, your cluster must have at least one node with the `ingest` role. For heavy ingest loads, we recommend creating [dedicated ingest nodes](modules-node.html#node-ingest-node \\"Ingest node\\").\\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to manage ingest pipelines. To use Kibana’s **Ingest Pipelines** feature, you also need the `cluster:monitor/nodes/info` cluster privileges.\\n* Pipelines including the `enrich` processor require additional setup. See [*Enrich your data*](ingest-enriching-data.html \\"Enrich your data\\").\\n\\n### Create and manage pipelines\\n\\nIn Kibana, open the main menu and click **Stack Management > Ingest Pipelines**. From the list view... <truncated>',
          originalTokenCount: 16056,
          truncatedTokenCount: 400,
        },
        llmScore: 7,
      },
      {
        selected: true,
        document: {
          content:
            '## Ingest pipelines in Search\n\nYou can manage ingest pipelines through Elasticsearch APIs or Kibana UIs.\n\nThe **Content** UI under **Search** has a set of tools for creating and managing indices optimized for search use cases (non time series data). You can also manage your ingest pipelines in this UI.\n\n### Find pipelines in Content UI\n\nTo work with ingest pipelines using these UI tools, you’ll be using the **Pipelines** tab on your search-optimized Elasticsearch index.\n\nTo find this tab in the Kibana UI:\n\n1. Go to **Search > Content > Elasticsearch indices**.\n2. Select the index you want to work with. For example, `search-my-index`.\n3. On the index’s overview page, open the **Pipelines** tab.\n4. From here, you can follow the instructions to create custom pipelines, and set up ML inference pipelines.\n\nThe tab is highlighted in this screenshot:\n\n![ingest pipeline ent search ui](images/ingest/ingest-pipeline-ent-search-ui.png)\n\n### Overview\n\nThese tools can be particularly helpful by providing a layer of customization and post-processing of documents. For example:\n\n* providing consistent extraction of text from binary data types\n* ensuring consistent formatting\n* providing consistent sanitization steps (removing PII like phone numbers or SSN’s)\n\nIt can be a lot of work to set up and manage production-ready pipelines from scratch. Considerations such as error handling, conditional execution, sequencing, versioning, and modularization must all be taken into account.\n\nTo this end, when you create indices for search use cases, (including [Elastic web crawler](/guide/en/enterprise-search/8.17/crawler.html), [connectors](es-connectors.html "Ingest content with Elastic connectors"). , and API indices), each index already has a pipeline set up with several processors that optimize your content for search.\n\nThis pipeline is called `ent-search-generic-ingestion`. While it is a "managed" pipeline (meaning it should not be tampered with), you can view its details via the Kibana UI or the Elasticsearch API. You can also [read more about its contents below](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference").\n\nYou can control whether you run some of these processors. While all features are enabled by default, they are eligible for opt-out. For [Elastic crawler](/guide/en/enterprise-search/8.17/crawler.html) and [connectors](es-connectors.html "Ingest content with Elastic connectors"). , you can opt out (or back in) per index, and your choices are saved. For API indices, you can opt out (or back in) by including specific fields in your documents. [See below for details](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings-using-the-api "Using the API").\n\nAt the deployment level, you can change the default settings for all new indices. This will not effect existing indices.\n\nEach index also provides the capability to easily create index-specific ingest pipelines with customizable processing. If you need that extra flexibility, you can create a custom pipeline by going to your pipeline settings and choosing to "copy and customize". This will replace the index’s use of `ent-search-generic-ingestion` with 3 newly generated pipelines:\n\n1. `<index-name>`\n2. `<index-name>@custom`\n3. `<index-name>@ml-inference`\n\nLike `ent-search-generic-ingestion`, the first of these is "managed", but the other two can and should be modified to fit your needs. You can view these pipelines using the platform tools (Kibana UI, Elasticsearch API), and can also [read more about their content below](ingest-pipeline-search.html#ingest-pipeline-search-details-specific "Index-specific ingest pipelines").\n\n### Pipeline Settings\n\nAside from the pipeline itself, you have a few configuration options which control individual features of the pipelines.\n\n* **Extract Binary Content** - This controls whether or not binary documents should be processed and any textual content should be extracted.\n* **Reduce Whitespace** - This controls whether or not consecutive, leading, and trailing whitespaces should be removed. This can help to display more content in some search experiences.\n* **Run ML Inference** - Only available on index-specific pipelines. This controls whether or not the optional `<index-name>@ml-inference` pipeline will be run. Enabled by default.\n\nFor Elastic web crawler and connectors, you can opt in or out per index. These settings are stored in Elasticsearch in the `.elastic-connectors` index, in the document that corresponds to the specific index. These settings can be changed there directly, or through the Kibana UI at **Search > Content > Indices > \\<your index> > Pipelines > Settings**.\n\nYou can also change the deployment wide defaults. These settings are stored in the Elasticsearch mapping for `.elastic-connectors` in the `_meta` section. These settings can be changed there directly, or from the Kibana UI at **Search > Content > Settings** tab. Changing the deployment wide defaults will not impact any existing indices, but will only impact any newly created indices defaults. Those defaults will still be able to be overriden by the index-specific settings.\n\n#### Using the API\n\nThese settings are not persisted for indices that "Use the API". Instead, changing these settings will, in real time, change the example cURL request displayed. Notice that the example document in the cURL request contains three underscore-prefixed fields:\n\n```\n{\n  ...\n  "_extract_binary_content": true,\n  "_reduce_whitespace": true,\n  "_run_ml_inference": true\n}\n```\n\nOmitting one of these special fields is the same as specifying it with the value `false`.\n\nYou must also specify the pipeline in your indexing request. This is also shown in the example cURL request.\n\nIf the pipeline is not specified, the underscore-prefixed fields will actually be indexed, and will not impact any processing behaviors.\n\n### Details\n\n#### `ent-search-generic-ingestion` Reference\n\nYou can access this pipeline with the [Elasticsearch Ingest Pipelines API](get-pipeline-api.html "Get pipeline API") or via Kibana’s [Stack Management > Ingest Pipelines](ingest.html#create-manage-ingest-pipelines "Create and manage pipelines") UI.\n\nThis pipeline is a "managed" pipeline. That means that it is not intended to be edited. Editing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future. If you want to make customizations, we recommend you utilize index-specific pipelines (see below), specifically [the `<index-name>@custom` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-custom-reference "<index-name>@custom Reference").\n\n##### Processors\n\n1. `attachment` - this uses the [Attachment](attachment.html "Attachment processor") processor to convert any binary data stored in a document’s `_attachment` field to a nested object of plain text and metadata.\n2. `set_body` - this uses the [Set](set-processor.html "Set processor") processor to copy any plain text extracted from the previous step and persist it on the document in the `body` field.\n3. `remove_replacement_chars` - this uses the [Gsub](gsub-processor.html "Gsub processor") processor to remove characters like "�" from the `body` field.\n4. `remove_extra_whitespace` - this uses the [Gsub](gsub-processor.html "Gsub processor") processor to replace consecutive whitespace characters with single spaces in the `body` field. While not perfect for every use case (see below for how to disable), this can ensure that search experiences display more content and highlighting and less empty space for your search results.\n5. `trim` - this uses the [Trim](trim-processor.html "Trim processor") processor to remove any remaining leading or trailing whitespace from the `body` field.\n6. `remove_meta_fields` - this final step of the pipeline uses the [Remove](remove-processor.html "Remove processor") processor to remove special fields that may have been used elsewhere in the pipeline, whether as temporary storage or as control flow parameters.\n\n##### Control flow parameters\n\nThe `ent-search-generic-ingestion` pipeline does not always run all processors. It utilizes a feature of ingest pipelines to [conditionally run processors](ingest.html#conditionally-run-processor "Conditionally run a processor") based on the contents of each individual document.\n\n* `_extract_binary_content` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `attachment`, `set_body`, and `remove_replacement_chars` processors. Note that the document will also need an `_attachment` field populated with base64-encoded binary data in order for the `attachment` processor to have any output. If the `_extract_binary_content` field is missing or `false` on a source document, these processors will be skipped.\n* `_reduce_whitespace` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `remove_extra_whitespace` and `trim` processors. These processors only apply to the `body` field. If the `_reduce_whitespace` field is missing or `false` on a source document, these processors will be skipped.\n\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index’s Pipeline tab. To control what settings any new indices will have upon creation, see the deployment wide content settings. See [Pipeline Settings](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings").\n\n#### Index-specific ingest pipelines\n\nIn the Kibana UI for your index, by clicking on the Pipelines tab, then **Settings > Copy and customize**, you can quickly generate 3 pipelines which are specific to your index. These 3 pipelines replace `ent-search-generic-ingestion` for the index. There is nothing lost in this action, as the `<index-name>` pipeline is a superset of functionality over the `ent-search-generic-ingestion` pipeline.\n\nThe "copy and customize" button is not available at all Elastic subscription levels. Refer to the Elastic subscriptions pages for [Elastic Cloud](/subscriptions/cloud) and [self-managed](/subscriptions) deployments.\n\n##### `<index-name>` Reference\n\nThis pipeline looks and behaves a lot like the [`ent-search-generic-ingestion` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference"), but with [two additional processors](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-reference-processors "Processors").\n\nYou should not rename this pipeline.\n\nThis pipeline is a "managed" pipeline. That means that it is not intended to be edited. Editing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future. If you want to make customizations, we recommend you utilize [the `<index-name>@custom` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-custom-reference "<index-name>@custom Reference").\n\n###### Processors\n\nIn addition to the processors inherited from the [`ent-search-generic-ingestion` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference"), the index-specific pipeline also defines:\n\n* `index_ml_inference_pipeline` - this uses the [Pipeline](pipeline-processor.html "Pipeline processor") processor to run the `<index-name>@ml-inference` pipeline. This processor will only be run if the source document includes a `_run_ml_inference` field with the value `true`.\n* `index_custom_pipeline` - this uses the [Pipeline](pipeline-processor.html "Pipeline processor") processor to run the `<index-name>@custom` pipeline.\n\n###### Control flow parameters\n\nLike the `ent-search-generic-ingestion` pipeline, the `<index-name>` pipeline does not always run all processors. In addition to the `_extract_binary_content` and `_reduce_whitespace` control flow parameters, the `<index-name>` pipeline also supports:\n\n* `_run_ml_inference` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `index_ml_inference_pipeline` processor. If the `_run_ml_inference` field is missing or `false` on a source document, this processor will be skipped.\n\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index’s Pipeline tab. To control what settings any new indices will have upon creation, see the deployment wide content settings. See [Pipeline Settings](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings").\n\n##### `<index-name>@ml-inference` Reference\n\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the **Stack Management > Ingest Pipelines** page. Unlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT "managed".\n\nIt’s possible to add one or more ML inference pipelines to an index in the **Content** UI. This pipeline will serve as a container for all of the ML inference pipelines configured for the index. Each ML inference pipeline added to the index is referenced within `<index-name>@ml-inference` using a `pipeline` processor.\n\nYou should not rename this pipeline.\n\nThe `monitor_ml` Elasticsearch cluster permission is required in order to manage ML models and ML inference pipelines which use those models.\n\n##### `<index-name>@custom` Reference\n\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the **Stack Management > Ingest Pipelines** page. Unlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT "managed".\n\nYou are encouraged to make additions and edits to this pipeline, provided its name remains the same. This provides a convenient hook from which to add custom processing and transformations for your data. Be sure to read the [docs for ingest pipelines](ingest.html "Ingest pipelines") to see what options are available.\n\nYou should not rename this pipeline.\n\n### Upgrading notes\n\nExpand to see upgrading notes\n\n* `app_search_crawler` - Since 8.3, App Search web crawler has utilized this pipeline to power its binary content extraction. You can read more about this pipeline and its usage in the [App Search Guide](/guide/en/app-search/8.17/web-crawler-reference.html#web-crawler-reference-binary-content-extraction). When upgrading from 8.3 to 8.5+, be sure to note any changes that you made to the `app_search_crawler` pipeline. These changes should be re-applied to each index’s `<index-name>@custom` pipeline in order to ensure a consistent data processing experience. In 8.5+, the [index setting to enable binary content](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings") is required **in addition** to the configurations mentioned in the [App Search Guide](/guide/en/app-search/8.17/web-crawler-reference.html#web-crawler-reference-binary-content-extraction).\n* `ent_search_crawler` - Since 8.4, the Elastic web crawler has utilized this pipeline to power its binary content extraction. You can read more about this pipeline and its usage in the [Elastic web crawler Guide](/guide/en/enterprise-search/8.17/crawler-managing.html#crawler-managing-binary-content). When upgrading from 8.4 to 8.5+, be sure to note any changes that you made to the `ent_search_crawler` pipeline. These changes should be re-applied to each index’s `<index-name>@custom` pipeline in order to ensure a consistent data processing experience. In 8.5+, the [index setting to enable binary content](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings") is required **in addition** to the configurations mentioned in the [Elastic web crawler Guide](/guide/en/enterprise-search/8.17/crawler-managing.html#crawler-managing-binary-content).\n* `ent-search-generic-ingestion` - Since 8.5, Native Connectors, Connector Clients, and new (>8.4) Elastic web crawler indices will all make use of this pipeline by default. You can [read more about this pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference") above. As this pipeline is "managed", any modifications that were made to `app_search_crawler` and/or `ent_search_crawler` should NOT be made to `ent-search-generic-ingestion`. Instead, if such customizations are desired, you should utilize [Index-specific ingest pipelines](ingest-pipeline-search.html#ingest-pipeline-search-details-specific "Index-specific ingest pipelines"), placing all modifications in the `<index-name>@custom` pipeline(s).\n',
          title: 'Ingest pipelines in Search',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-pipeline-search.html',
          productName: 'elasticsearch',
          score: 154.87169,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-pipeline-search.html',
        title: 'Ingest pipelines in Search',
        score: 154.87169,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Ingest pipelines in Search\\n\\nYou can manage ingest pipelines through Elasticsearch APIs or Kibana UIs.\\n\\nThe **Content** UI under **Search** has a set of tools for creating and managing indices optimized for search use cases (non time series data). You can also manage your ingest pipelines in this UI.\\n\\n### Find pipelines in Content UI\\n\\nTo work with ingest pipelines using these UI tools, you’ll be using the **Pipelines** tab on your search-optimized Elasticsearch index.\\n\\nTo find this tab in the Kibana UI:\\n\\n1. Go to **Search > Content > Elasticsearch indices**.\\n2. Select the index you want to work with. For example, `search-my-index`.\\n3. On the index’s overview page, open the **Pipelines** tab.\\n4. From here, you can follow the instructions to create custom pipelines, and set up ML inference pipelines.\\n\\nThe tab is highlighted in this screenshot:\\n\\n![ingest pipeline ent search ui](images/ingest/ingest-pipeline-ent-search-ui.png)\\n\\n### Overview\\n\\nThese tools can be particularly helpful by providing a layer of customization and post-processing of documents. For example:\\n\\n* providing consistent extraction of text from binary data types\\n* ensuring consistent formatting\\n* providing consistent sanitization steps (removing PII like phone numbers or SSN’s)\\n\\nIt can be a lot of work to set up and manage production-ready pipelines from scratch. Considerations such as error handling, conditional execution, sequencing, versioning, and modularization must all be taken into account.\\n\\nTo this end, when you create indices for search use cases, (including [Elastic web crawler](/guide/en/enterprise-search/8.17/crawler.html), [connectors](es-connectors.html \\"Ingest content with Elastic connectors\\"). , and API indices), each index already has a pipeline set up with several processors that optimize your content for search.\\n\\nThis pipeline is called `ent-search-generic-ingestion`. While it is a \\"managed\\" pipeline (meaning it should not be tampered with), you can view its details via the Kibana UI or the Elasticsearch API. You can also [read more about its contents below](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference \\"ent-search-generic-ingestion Reference\\").\\n\\nYou can control whether you run some of these processors. While all features are enabled by default, they are eligible for opt-out. For [Elastic crawler](/guide/en/enterprise-search/8.17/crawler.html) and [connectors](es-connectors.html \\"Ingest content with Elastic connectors\\"). , you can opt out (or back in) per index, and your choices are saved. For API indices, you can opt out (or back in) by including specific fields in your documents. [See below for details](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings-using-the-api \\"Using the API\\").\\n\\nAt the deployment level, you can change the default settings for all new indices. This will not effect existing indices.\\n\\nEach index also provides the capability to easily create index-specific ingest pipelines with customizable processing. If you need that extra flexibility, you can create a custom pipeline by going to your pipeline settings and choosing to \\"copy and customize\\". This will replace the index’s use of `ent-search-generic-ingestion` with 3 newly generated pipelines:\\n\\n1. `<index-name>`\\n2. `<index-name>@custom`\\n3. `<index-name>@ml-inference`\\n\\nLike `ent-search-generic-ingestion`, the first of these is \\"managed\\", but the other two can and should be modified to fit your needs. You can view these pipelines using the platform tools (Kibana UI, Elasticsearch API), and can also [read more about their content below](ingest-pipeline-search.html#ingest-pipeline-search-details-specific \\"Index-specific ingest pipelines\\").\\n\\n### Pipeline Settings\\n\\nAside from the pipeline itself, you have a few configuration options which control individual features of the pipelines.\\n\\n* **Extract Binary Content** - This controls whether or not binary documents should be processed and any textual content should be extracted.\\n* **Reduce Whitespace** - This controls whether or not consecutive, leading, and trailing whitespaces should be removed. This can help to display more content in some search experiences.\\n* **Run ML Inference** - Only available on index-specific pipelines. This controls whether or not the optional `<index-name>@ml-inference` pipeline will be run. Enabled by default.\\n\\nFor Elastic web crawler and connectors, you can opt in or out per index. These settings are stored in Elasticsearch in the `.elastic-connectors` index, in the document that corresponds to the specific index. These settings can be changed there directly, or through the Kibana UI at **Search > Content > Indices > \\\\<your index> > Pipelines > Settings**.\\n\\nYou can also change the deployment wide defaults. These settings are stored in the Elasticsearch mapping for `.elastic-connectors` in the `_meta` section. These settings can be changed there directly, or from the Kibana UI at **Search > Content > Settings** tab. Changing the deployment wide defaults will not impact any existing indices, but will only impact any newly created indices defaults. Those defaults will still be able to be overriden by the index-specific settings.\\n\\n#### Using the API\\n\\nThese settings are not persisted for indices that \\"Use the API\\". Instead, changing these settings will, in real time, change the example cURL request displayed. Notice that the example document in the cURL request contains three underscore-prefixed fields:\\n\\n```\\n{\\n  ...\\n  \\"_extract_binary_content\\": true,\\n  \\"_reduce_whitespace\\": true,\\n  \\"_run_ml_inference\\": true\\n}\\n```\\n\\nOmitting one of these special fields is the same as specifying it with the value `false`.\\n\\nYou must also specify the pipeline in your indexing request. This is also shown in the example cURL request.\\n\\nIf the pipeline is not specified, the underscore-prefixed fields will actually be indexed, and will not impact any processing behaviors.\\n\\n### Details\\n\\n#### `ent-search-generic-ingestion` Reference\\n\\nYou can access this pipeline with the [Elasticsearch Ingest Pipelines API](get-pipeline-api.html \\"Get pipeline API\\") or via Kibana’s [Stack Management > Ingest Pipelines](ingest.html#create-manage-ingest-pipelines \\"Create and manage pipelines\\") UI.\\n\\nThis pipeline is a \\"managed\\" pipeline. That means that it is not intended to be edited. Editing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future. If you want to make customizations, we recommend you utilize index-specific pipelines (see below), specifically [the `<index-name>@custom` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-custom-reference \\"<index-name>@custom Reference\\").\\n\\n##### Processors\\n\\n1. `attachment` - this uses the [Attachment](attachment.html \\"Attachment processor\\") processor to convert any binary data stored in a document’s `_attachment` field to a nested object of plain text and metadata.\\n2. `set_body` - this uses the [Set](set-processor.html \\"Set processor\\") processor to copy any plain text extracted from the previous step and persist it on the document in the `body` field.\\n3. `remove_replacement_chars` - this uses the [Gsub](gsub-processor.html \\"Gsub processor\\") processor to remove characters like \\"�\\" from the `body` field.\\n4. `remove_extra_whitespace` - this uses the [Gsub](gsub-processor.html \\"Gsub processor\\") processor to replace consecutive whitespace characters with single spaces in the `body` field. While not perfect for every use case (see below for how to disable), this can ensure that search experiences display more content and highlighting and less empty space for your search results.\\n5. `trim` - this uses the [Trim](trim-processor.html \\"Trim processor\\") processor to remove any remaining leading or trailing whitespace from the `body` field.\\n6. `remove_meta_fields` - this final step of the pipeline uses the [Remove](remove-processor.html \\"Remove processor\\") processor to remove special fields that may have been used elsewhere in the pipeline, whether as temporary storage or as control flow parameters.\\n\\n##### Control flow parameters\\n\\nThe `ent-search-generic-ingestion` pipeline does not always run all processors. It utilizes a feature of ingest pipelines to [conditionally run processors](ingest.html#conditionally-run-processor \\"Conditionally run a processor\\") based on the contents of each individual document.\\n\\n* `_extract_binary_content` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `attachment`, `set_body`, and `remove_replacement_chars` processors. Note that the document will also need an `_attachment` field populated with base64-encoded binary data in order for the `attachment` processor to have any output. If the `_extract_binary_content` field is missing or `false` on a source document, these processors will be skipped.\\n* `_reduce_whitespace` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `remove_extra_whitespace` and `trim` processors. These processors only apply to the `body` field. If the `_reduce_whitespace` field is missing or `false` on a source document, these processors will be skipped.\\n\\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index’s Pipeline tab. To control what settings any new indices will have upon creation, see the deployment wide content settings. See [Pipeline Settings](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings \\"Pipeline Settings\\").\\n\\n#### Index-specific ingest pipelines\\n\\nIn the Kibana UI for your index, by clicking on the Pipelines tab, then **Settings > Copy and customize**, you can quickly generate 3 pipelines which are specific to your index. These 3 pipelines replace `ent-search-generic-ingestion` for the index. There is nothing lost in this action, as the `<index-name>` pipeline is a superset of functionality over the `ent-search-generic-ingestion` pipeline.\\n\\nThe \\"copy and customize\\" button is not available at all Elastic subscription levels. Refer to the Elastic subscriptions pages for [Elastic Cloud](/subscriptions/cloud) and [self-managed](/subscriptions) deployments.\\n\\n##### `<index-name>` Reference\\n\\nThis pipeline looks and behaves a lot like the [`ent-search-generic-ingestion` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference \\"ent-search-generic-ingestion Reference\\"), but with [two additional processors](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-reference-processors \\"Processors\\").\\n\\nYou should not rename this pipeline.\\n\\nThis pipeline is a \\"managed\\" pipeline. That means that it is not intended to be edited. Editing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future. If you want to make customizations, we recommend you utilize [the `<index-name>@custom` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-custom-reference \\"<index-name>@custom Reference\\").\\n\\n###### Processors\\n\\nIn addition to the processors inherited from the [`ent-search-generic-ingestion` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference \\"ent-search-generic-ingestion Reference\\"), the index-specific pipeline also defines:\\n\\n* `index_ml_inference_pipeline` - this uses the [Pipeline](pipeline-processor.html \\"Pipeline processor\\") processor to run the `<index-name>@ml-inference` pipeline. This processor will only be run if the source document includes a `_run_ml_inference` field with the value `true`.\\n* `index_custom_pipeline` - this uses the [Pipeline](pipeline-processor.html \\"Pipeline processor\\") processor to run the `<index-name>@custom` pipeline.\\n\\n###### Control flow parameters\\n\\nLike the `ent-search-generic-ingestion` pipeline, the `<index-name>` pipeline does not always run all processors. In addition to the `_extract_binary_content` and `_reduce_whitespace` control flow parameters, the `<index-name>` pipeline also supports:\\n\\n* `_run_ml_inference` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `index_ml_inference_pipeline` processor. If the `_run_ml_inference` field is missing or `false` on a source document, this processor will be skipped.\\n\\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index’s Pipeline tab. To control what settings any new indices will have upon creation, see the deployment wide content settings. See [Pipeline Settings](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings \\"Pipeline Settings\\").\\n\\n##### `<index-name>@ml-inference` Reference\\n\\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the **Stack Management > Ingest Pipelines** page. Unlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT \\"managed\\".\\n\\nIt’s possible to add one or more ML inference pipelines to an index in the **Content** UI. This pipeline will serve as a container for all of the ML inference pipelines configured for the index. Each ML inference pipeline added to the index is referenced within `<index-name>@ml-inference` using a `pipeline` processor.\\n\\nYou should not rename this pipeline.\\n\\nThe `monitor_ml` Elasticsearch cluster permission is required in order to manage ML models and ML inference pipelines which use those models.\\n\\n##### `<index-name>@custom` Reference\\n\\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the **Stack Management > Ingest Pipelines** page. Unlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT \\"managed\\".\\n\\nYou are encouraged to make additions and edits to this pipeline, provided its name remains the same. This provides a convenient hook from which to add custom processing and transformations for your data. Be sure to read the [docs for ingest pipelines](ingest.html \\"Ingest pipelines\\") to see what options are available.\\n\\nYou should not rename this pipeline.\\n\\n### Upgrading notes\\n\\nExpand to see upgrading notes\\n\\n* `app_search_crawler` - Since 8.3, App Search web crawler has utilized this pipeline to power its binary content extraction. You can read more about this pipeline and its usage in the [App Search Guide](/guide/en/app-search/8.17/web-crawler-reference.html#web-crawler-reference-binary-content-extraction). When upgrading from 8.3 to 8.5+, be sure to note any changes that you made to the `app_search_crawler` pipeline. These changes should be re-applied to each index’s `<index-name>@custom` pipeline in order to ensure a consistent data processing experience. In 8.5+, the [index setting to enable binary content](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings \\"Pipeline Settings\\") is required **in addition** to the configurations mentioned in the [App Search Guide](/guide/en/app-search/8.17/web-crawler-reference.html#web-crawler-reference-binary-content-extraction).\\n* `ent_search_crawler` - Since 8.4, the Elastic web crawler has utilized this pipeline to power its binary content extraction. You can read more about this pipeline and its usage in the [Elastic web crawler Guide](/guide/en/enterprise-search/8.17/crawler-managing.html#crawler-managing-binary-content). When upgrading from 8.4 to 8.5+, be sure to note any changes that you made to the `ent_search_crawler` pipeline. These changes should be re-applied to each index’s `<index-name>@custom` pipeline in order to ensure a consistent data processing experience. In 8.5+, the [index setting to enable binary content](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings \\"Pipeline Settings\\") is required **in addition** to the configurations mentioned in the [Elastic web crawler Guide](/guide/en/enterprise-search/8.17/crawler-managing.html#crawler-managing-binary-content).\\n* `ent-search-generic-ingestion` - Since 8.5, Native Connectors, Connector Clients, and new (>8.4) Elastic web crawler indices will all make use of this pipeline by default. You can [read more about this pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference \\"ent-search-generic-ingestion Reference\\") above. As this pipeline is \\"managed\\", any modifications that were made to `app_search_crawler` and/or `ent_search_crawler` should NOT be made to `ent-search-generic-ingestion`. Instead, if such customizations are desired, you should utilize [Index-specific ingest pipelines](ingest-pipeline-search.html#ingest-pipeline-search-details-specific \\"Index-specific ingest pipelines\\"), placing all modifications in the `<index-name>@custom` pipeline(s).\\n","title":"Ingest pipelines in Search","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-pipeline-search.html","productName":"elasticsearch","score":154.87169}',
        truncated: {
          truncatedText:
            '{"content":"## Ingest pipelines in Search\\n\\nYou can manage ingest pipelines through Elasticsearch APIs or Kibana UIs.\\n\\nThe **Content** UI under **Search** has a set of tools for creating and managing indices optimized for search use cases (non time series data). You can also manage your ingest pipelines in this UI.\\n\\n### Find pipelines in Content UI\\n\\nTo work with ingest pipelines using these UI tools, you’ll be using the **Pipelines** tab on your search-optimized Elasticsearch index.\\n\\nTo find this tab in the Kibana UI:\\n\\n1. Go to **Search > Content > Elasticsearch indices**.\\n2. Select the index you want to work with. For example, `search-my-index`.\\n3. On the index’s overview page, open the **Pipelines** tab.\\n4. From here, you can follow the instructions to create custom pipelines, and set up ML inference pipelines.\\n\\nThe tab is highlighted in this screenshot:\\n\\n![ingest pipeline ent search ui](images/ingest/ingest-pipeline-ent-search-ui.png)\\n\\n### Overview\\n\\nThese tools can be particularly helpful by providing a layer of customization and post-processing of documents. For example:\\n\\n* providing consistent extraction of text from binary data types\\n* ensuring consistent formatting\\n* providing consistent sanitization steps (removing PII like phone numbers or SSN’s)\\n\\nIt can be a lot of work to set up and manage production-ready pipelines from scratch. Considerations such as error handling, conditional execution, sequencing, versioning, and modularization must all be taken into account.\\n\\nTo this end, when you create indices for search use cases, (including [Elastic web crawler](/guide/en/enterprise-search/8.17/crawler.html), [connectors](es-connectors.html \\"Ingest content with Elastic connectors\\"). , and API indices),... <truncated>',
          originalTokenCount: 3795,
          truncatedTokenCount: 400,
        },
        llmScore: 6,
      },
      {
        selected: false,
        document: {
          author: {
            login: 'jonathan-buttner',
          },
          issue_comments: [
            {
              author: {
                login: 'ruflin',
              },
              body: 'There is a config option to define a pipeline in the dataset: https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/manifest.yml#L9 This will use https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/elasticsearch/ingest-pipeline/default.json I wonder if this already solves what you are looking for?',
            },
            {
              author: {
                login: 'jonathan-buttner',
              },
              body: "> There is a config option to define a pipeline in the dataset: https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/manifest.yml#L9 This will use https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/elasticsearch/ingest-pipeline/default.json I wonder if this already solves what you are looking for?\r\n\r\nOh you're right! I totally missed this too:\r\nhttps://github.com/elastic/kibana/blob/master/x-pack/plugins/ingest_manager/server/services/epm/elasticsearch/template/template.ts#L34\r\n\r\nLooks like this is already implemented 👍 \r\n\r\ncc: @elastic/endpoint-response \r\n\r\n![image](https://user-images.githubusercontent.com/56361221/78898841-7524d600-7a42-11ea-8170-860b7df73154.png)\r\n",
            },
            {
              author: {
                login: 'jonathan-buttner',
              },
              body: 'Closing as it is already implemented.',
            },
          ],
          title: '[Ingest Manager][Discuss] Support defining a default pipeline in a template',
          body: "The Endpoint team has a use case where it'd like move alert events to a new index on ingest. We'd like to leverage a pipeline to do this. It would also be helpful if we could define a default pipeline in a template so that we don't have to tell the endpoint which pipeline to use when sending events. Currently the template that is created by the Ingest Manager does not include the [default_pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules.html#dynamic-index-settings) field when creating the template: https://github.com/elastic/kibana/blob/master/x-pack/plugins/ingest_manager/server/services/epm/elasticsearch/template/template.ts#L145 I think we could support this by adding a field to the manifest file of a dataset that indicates which ingest pipeline should be the default and have the Ingest Manager use that when building the template. @elastic/ingest-management cc: @elastic/endpoint-response",
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/768',
          number: 768,
          createdAt: '2020-04-03T14:42:41Z',
          assignees_list: [],
          labels_field: [
            {
              name: 'Team:Ingest Management',
              description: 'Label for the Ingest Management team',
            },
          ],
          state: 'CLOSED',
          id: 'MDU6SXNzdWU1OTM0MzIyMDU=',
          closedAt: '2020-04-09T13:14:48Z',
          _timestamp: '2020-04-09T13:14:48Z',
        },
        id: 'search-observability-dev/MDU6SXNzdWU1OTM0MzIyMDU=',
        title: '[Ingest Manager][Discuss] Support defining a default pipeline in a template',
        score: 153.60333,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"jonathan-buttner"},"issue_comments":[{"author":{"login":"ruflin"},"body":"There is a config option to define a pipeline in the dataset: https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/manifest.yml#L9 This will use https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/elasticsearch/ingest-pipeline/default.json I wonder if this already solves what you are looking for?"},{"author":{"login":"jonathan-buttner"},"body":"> There is a config option to define a pipeline in the dataset: https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/manifest.yml#L9 This will use https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/elasticsearch/ingest-pipeline/default.json I wonder if this already solves what you are looking for?\\r\\n\\r\\nOh you\'re right! I totally missed this too:\\r\\nhttps://github.com/elastic/kibana/blob/master/x-pack/plugins/ingest_manager/server/services/epm/elasticsearch/template/template.ts#L34\\r\\n\\r\\nLooks like this is already implemented 👍 \\r\\n\\r\\ncc: @elastic/endpoint-response \\r\\n\\r\\n![image](https://user-images.githubusercontent.com/56361221/78898841-7524d600-7a42-11ea-8170-860b7df73154.png)\\r\\n"},{"author":{"login":"jonathan-buttner"},"body":"Closing as it is already implemented."}],"title":"[Ingest Manager][Discuss] Support defining a default pipeline in a template","body":"The Endpoint team has a use case where it\'d like move alert events to a new index on ingest. We\'d like to leverage a pipeline to do this. It would also be helpful if we could define a default pipeline in a template so that we don\'t have to tell the endpoint which pipeline to use when sending events. Currently the template that is created by the Ingest Manager does not include the [default_pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules.html#dynamic-index-settings) field when creating the template: https://github.com/elastic/kibana/blob/master/x-pack/plugins/ingest_manager/server/services/epm/elasticsearch/template/template.ts#L145 I think we could support this by adding a field to the manifest file of a dataset that indicates which ingest pipeline should be the default and have the Ingest Manager use that when building the template. @elastic/ingest-management cc: @elastic/endpoint-response","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/768","number":768,"createdAt":"2020-04-03T14:42:41Z","assignees_list":[],"labels_field":[{"name":"Team:Ingest Management","description":"Label for the Ingest Management team"}],"state":"CLOSED","id":"MDU6SXNzdWU1OTM0MzIyMDU=","closedAt":"2020-04-09T13:14:48Z","_timestamp":"2020-04-09T13:14:48Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"jonathan-buttner"},"issue_comments":[{"author":{"login":"ruflin"},"body":"There is a config option to define a pipeline in the dataset: https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/manifest.yml#L9 This will use https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/elasticsearch/ingest-pipeline/default.json I wonder if this already solves what you are looking for?"},{"author":{"login":"jonathan-buttner"},"body":"> There is a config option to define a pipeline in the dataset: https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/manifest.yml#L9 This will use https://github.com/elastic/package-registry/blob/master/dev/packages/example/nginx-1.2.0/dataset/access/elasticsearch/ingest-pipeline/default.json I wonder if this already solves what you are looking for?\\r\\n\\r\\nOh you\'re right! I totally missed this too:\\r\\nhttps://github.com/elastic/kibana/blob/master/x-pack/plugins/ingest_manager/server/services/epm/elasticsearch/template/template.ts#L34\\r\\n\\r\\nLooks like this is already implemented 👍 \\r\\n\\r\\ncc: @elastic/endpoint-response \\r\\n\\r\\n![image](https://user-images.githubusercontent.com/56361221/78898841-7524d600-7a42-11ea-8170-860b7df73154.png)\\r\\n"},{"author":{"login":"jonathan-buttner"},"body":"Closing as it is already implemented."}],"title":"[Ingest Manager][Discuss] Support defining a default pipeline in a template","body":"The Endpoint team has a use case where it\'d like move alert events to a new index... <truncated>',
          originalTokenCount: 705,
          truncatedTokenCount: 400,
        },
        llmScore: 2,
      },
      {
        selected: true,
        document: {
          content:
            '## Ingest APIs\n\nUse ingest APIs to manage tasks and resources related to [ingest pipelines](ingest.html "Ingest pipelines") and processors.\n\n### Ingest pipeline APIs\n\nUse the following APIs to create, manage, and test ingest pipelines:\n\n* [Create or update pipeline](put-pipeline-api.html "Create or update pipeline API") to create or update a pipeline\n* [Get pipeline](get-pipeline-api.html "Get pipeline API") to retrieve a pipeline configuration\n* [Delete pipeline](delete-pipeline-api.html "Delete pipeline API") to delete a pipeline\n* [Simulate pipeline](simulate-pipeline-api.html "Simulate pipeline API") and [Simulate ingest](simulate-ingest-api.html "Simulate ingest API") to test ingest pipelines\n\n### Stat APIs\n\nUse the following APIs to get statistics about ingest processing:\n\n* [GeoIP stats](geoip-stats-api.html "GeoIP stats API") to get download statistics for IP geolocation databases used with the [`geoip` processor](geoip-processor.html "GeoIP processor").\n\n### Ingest IP Location Database APIs\n\nUse the following APIs to configure and manage commercial IP geolocation database downloads:\n\n* [Create or update IP geolocation database configuration](put-ip-location-database-api.html "Create or update IP geolocation database configuration API") to create or update a database configuration\n* [Get IP geolocation database configuration](get-ip-location-database-api.html "Get IP geolocation database configuration API") to retrieve a database configuration\n* [Delete IP geolocation database configuration](delete-ip-location-database-api.html "Delete IP geolocation database configuration API") to delete a database configuration\n',
          title: 'Ingest APIs',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-apis.html',
          productName: 'elasticsearch',
          score: 147.50217,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-apis.html',
        title: 'Ingest APIs',
        score: 147.50217,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Ingest APIs\\n\\nUse ingest APIs to manage tasks and resources related to [ingest pipelines](ingest.html \\"Ingest pipelines\\") and processors.\\n\\n### Ingest pipeline APIs\\n\\nUse the following APIs to create, manage, and test ingest pipelines:\\n\\n* [Create or update pipeline](put-pipeline-api.html \\"Create or update pipeline API\\") to create or update a pipeline\\n* [Get pipeline](get-pipeline-api.html \\"Get pipeline API\\") to retrieve a pipeline configuration\\n* [Delete pipeline](delete-pipeline-api.html \\"Delete pipeline API\\") to delete a pipeline\\n* [Simulate pipeline](simulate-pipeline-api.html \\"Simulate pipeline API\\") and [Simulate ingest](simulate-ingest-api.html \\"Simulate ingest API\\") to test ingest pipelines\\n\\n### Stat APIs\\n\\nUse the following APIs to get statistics about ingest processing:\\n\\n* [GeoIP stats](geoip-stats-api.html \\"GeoIP stats API\\") to get download statistics for IP geolocation databases used with the [`geoip` processor](geoip-processor.html \\"GeoIP processor\\").\\n\\n### Ingest IP Location Database APIs\\n\\nUse the following APIs to configure and manage commercial IP geolocation database downloads:\\n\\n* [Create or update IP geolocation database configuration](put-ip-location-database-api.html \\"Create or update IP geolocation database configuration API\\") to create or update a database configuration\\n* [Get IP geolocation database configuration](get-ip-location-database-api.html \\"Get IP geolocation database configuration API\\") to retrieve a database configuration\\n* [Delete IP geolocation database configuration](delete-ip-location-database-api.html \\"Delete IP geolocation database configuration API\\") to delete a database configuration\\n","title":"Ingest APIs","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-apis.html","productName":"elasticsearch","score":147.50217}',
        truncated: {
          truncatedText:
            '{"content":"## Ingest APIs\\n\\nUse ingest APIs to manage tasks and resources related to [ingest pipelines](ingest.html \\"Ingest pipelines\\") and processors.\\n\\n### Ingest pipeline APIs\\n\\nUse the following APIs to create, manage, and test ingest pipelines:\\n\\n* [Create or update pipeline](put-pipeline-api.html \\"Create or update pipeline API\\") to create or update a pipeline\\n* [Get pipeline](get-pipeline-api.html \\"Get pipeline API\\") to retrieve a pipeline configuration\\n* [Delete pipeline](delete-pipeline-api.html \\"Delete pipeline API\\") to delete a pipeline\\n* [Simulate pipeline](simulate-pipeline-api.html \\"Simulate pipeline API\\") and [Simulate ingest](simulate-ingest-api.html \\"Simulate ingest API\\") to test ingest pipelines\\n\\n### Stat APIs\\n\\nUse the following APIs to get statistics about ingest processing:\\n\\n* [GeoIP stats](geoip-stats-api.html \\"GeoIP stats API\\") to get download statistics for IP geolocation databases used with the [`geoip` processor](geoip-processor.html \\"GeoIP processor\\").\\n\\n### Ingest IP Location Database APIs\\n\\nUse the following APIs to configure and manage commercial IP geolocation database downloads:\\n\\n* [Create or update IP geolocation database configuration](put-ip-location-database-api.html \\"Create or update IP geolocation database configuration API\\") to create or update a database configuration\\n* [Get IP geolocation database configuration](get-ip-location-database-api.html \\"Get IP geolocation database configuration API\\") to retrieve a database configuration\\n* [Delete IP geolocation database configuration](delete-ip-location-database-api.html \\"Delete IP geolocation database configuration API\\") to delete a database configuration\\n","title":"Ingest APIs","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-apis.html","productName":"elasticsearch","score":147.... <truncated>',
          originalTokenCount: 403,
          truncatedTokenCount: 400,
        },
        llmScore: 5,
      },
      {
        selected: false,
        document: {
          content:
            '## Set up an enrich processor\n\nTo set up an enrich processor, follow these steps:\n\n1. Check the [prerequisites](enrich-setup.html#enrich-prereqs "Prerequisites").\n2. [Add enrich data](enrich-setup.html#create-enrich-source-index "Add enrich data").\n3. [Create an enrich policy](enrich-setup.html#create-enrich-policy "Create an enrich policy").\n4. [Execute the enrich policy](enrich-setup.html#execute-enrich-policy "Execute the enrich policy").\n5. [Add an enrich processor to an ingest pipeline](enrich-setup.html#add-enrich-processor "Add an enrich processor to an ingest pipeline").\n6. [Ingest and enrich documents](enrich-setup.html#ingest-enrich-docs "Ingest and enrich documents").\n\nOnce you have an enrich processor set up, you can [update your enrich data](enrich-setup.html#update-enrich-data "Update an enrich index") and [update your enrich policies](enrich-setup.html#update-enrich-policies "Update an enrich policy").\n\nThe enrich processor performs several operations and may impact the speed of your ingest pipeline.\n\nWe strongly recommend testing and benchmarking your enrich processors before deploying them in production.\n\nWe do not recommend using the enrich processor to append real-time data. The enrich processor works best with reference data that doesn’t change frequently.\n\n#### Prerequisites\n\nTo use enrich policies, you must have:\n\n* `read` index privileges for any indices used\n* The `enrich_user` [built-in role](built-in-roles.html "Built-in roles")\n\n### Add enrich data\n\nTo begin, add documents to one or more source indices. These documents should contain the enrich data you eventually want to add to incoming data.\n\nYou can manage source indices just like regular Elasticsearch indices using the [document](docs.html "Document APIs") and [index](indices.html "Index APIs") APIs.\n\nYou also can set up [Beats](/guide/en/beats/libbeat/8.17/getting-started.html), such as a [Filebeat](/guide/en/beats/filebeat/8.17/filebeat-installation-configuration.html), to automatically send and index documents to your source indices. See [Getting started with Beats](/guide/en/beats/libbeat/8.17/getting-started.html).\n\n### Create an enrich policy\n\nAfter adding enrich data to your source indices, use the [create enrich policy API](put-enrich-policy-api.html "Create enrich policy API") or [Index Management in Kibana](index-mgmt.html#manage-enrich-policies "Manage enrich policies") to create an enrich policy.\n\nOnce created, you can’t update or change an enrich policy. See [Update an enrich policy](enrich-setup.html#update-enrich-policies "Update an enrich policy").\n\n### Execute the enrich policy\n\nOnce the enrich policy is created, you need to execute it using the [execute enrich policy API](execute-enrich-policy-api.html "Execute enrich policy API") or [Index Management in Kibana](index-mgmt.html#manage-enrich-policies "Manage enrich policies") to create an [enrich index](ingest-enriching-data.html#enrich-index).\n\n![enrich policy index](images/ingest/enrich/enrich-policy-index.svg)\n\nThe *enrich index* contains documents from the policy’s source indices. Enrich indices always begin with `.enrich-*`, are read-only, and are [force merged](indices-forcemerge.html "Force merge API").\n\nEnrich indices should only be used by the [enrich processor](enrich-processor.html "Enrich processor") or the [ES|QL `ENRICH` command](esql-commands.html#esql-enrich "ENRICH"). Avoid using enrich indices for other purposes.\n\n### Add an enrich processor to an ingest pipeline\n\nOnce you have source indices, an enrich policy, and the related enrich index in place, you can set up an ingest pipeline that includes an enrich processor for your policy.\n\n![enrich processor](images/ingest/enrich/enrich-processor.svg)\n\nDefine an [enrich processor](enrich-processor.html "Enrich processor") and add it to an ingest pipeline using the [create or update pipeline API](put-pipeline-api.html "Create or update pipeline API").\n\nWhen defining the enrich processor, you must include at least the following:\n\n* The enrich policy to use.\n* The field used to match incoming documents to the documents in your enrich index.\n* The target field to add to incoming documents. This target field contains the match and enrich fields specified in your enrich policy.\n\nYou also can use the `max_matches` option to set the number of enrich documents an incoming document can match. If set to the default of `1`, data is added to an incoming document’s target field as a JSON object. Otherwise, the data is added as an array.\n\nSee [Enrich](enrich-processor.html "Enrich processor") for a full list of configuration options.\n\nYou also can add other [processors](processors.html "Ingest processor reference") to your ingest pipeline.\n\n### Ingest and enrich documents\n\nYou can now use your ingest pipeline to enrich and index documents.\n\n![enrich process](images/ingest/enrich/enrich-process.svg)\n\nBefore implementing the pipeline in production, we recommend indexing a few test documents first and verifying enrich data was added correctly using the [get API](docs-get.html "Get API").\n\n### Update an enrich index\n\nOnce created, you cannot update or index documents to an enrich index. Instead, update your source indices and [execute](execute-enrich-policy-api.html "Execute enrich policy API") the enrich policy again. This creates a new enrich index from your updated source indices. The previous enrich index will deleted with a delayed maintenance job. By default this is done every 15 minutes.\n\nIf wanted, you can [reindex](docs-reindex.html "Reindex API") or [update](docs-update-by-query.html "Update By Query API") any already ingested documents using your ingest pipeline.\n\n### Update an enrich policy\n\nOnce created, you can’t update or change an enrich policy. Instead, you can:\n\n1. Create and [execute](execute-enrich-policy-api.html "Execute enrich policy API") a new enrich policy.\n2. Replace the previous enrich policy with the new enrich policy in any in-use enrich processors or ES|QL queries.\n3. Use the [delete enrich policy](delete-enrich-policy-api.html "Delete enrich policy API") API or [Index Management in Kibana](index-mgmt.html#manage-enrich-policies "Manage enrich policies") to delete the previous enrich policy.\n\n### Enrich components\n\nThe enrich coordinator is a component that manages and performs the searches required to enrich documents on each ingest node. It combines searches from all enrich processors in all pipelines into bulk [multi-searches](search-multi-search.html "Multi search API").\n\nThe enrich policy executor is a component that manages the executions of all enrich policies. When an enrich policy is executed, this component creates a new enrich index and removes the previous enrich index. The enrich policy executions are managed from the elected master node. The execution of these policies occurs on a different node.\n\n### Node Settings\n\nThe `enrich` processor has node settings for enrich coordinator and enrich policy executor.\n\nThe enrich coordinator supports the following node settings:\n\n* `enrich.cache_size`\n\n  Maximum size of the cache that caches searches for enriching documents. The size can be specified in three units: the raw number of cached searches (e.g. `1000`), an absolute size in bytes (e.g. `100Mb`), or a percentage of the max heap space of the node (e.g. `1%`). Both for the absolute byte size and the percentage of heap space, Elasticsearch does not guarantee that the enrich cache size will adhere exactly to that maximum, as Elasticsearch uses the byte size of the serialized search response which is is a good representation of the used space on the heap, but not an exact match. Defaults to `1%`. There is a single cache for all enrich processors in the cluster.\n\n* `enrich.coordinator_proxy.max_concurrent_requests`\n\n  Maximum number of concurrent [multi-search requests](search-multi-search.html "Multi search API") to run when enriching documents. Defaults to `8`.\n\n* `enrich.coordinator_proxy.max_lookups_per_request`\n\n  Maximum number of searches to include in a [multi-search request](search-multi-search.html "Multi search API") when enriching documents. Defaults to `128`.\n\nThe enrich policy executor supports the following node settings:\n\n* `enrich.fetch_size`\n\n  Maximum batch size when reindexing a source index into an enrich index. Defaults to `10000`.\n\n* `enrich.max_force_merge_attempts`\n\n  Maximum number of [force merge](indices-forcemerge.html "Force merge API") attempts allowed on an enrich index. Defaults to `3`.\n\n* `enrich.cleanup_period`\n\n  How often Elasticsearch checks whether unused enrich indices can be deleted. Defaults to `15m`.\n\n* `enrich.max_concurrent_policy_executions`\n\n  Maximum number of enrich policies to execute concurrently. Defaults to `50`.\n',
          title: 'Set up an enrich processor',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/enrich-setup.html',
          productName: 'elasticsearch',
          score: 144.76215,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/enrich-setup.html',
        title: 'Set up an enrich processor',
        score: 144.76215,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Set up an enrich processor\\n\\nTo set up an enrich processor, follow these steps:\\n\\n1. Check the [prerequisites](enrich-setup.html#enrich-prereqs \\"Prerequisites\\").\\n2. [Add enrich data](enrich-setup.html#create-enrich-source-index \\"Add enrich data\\").\\n3. [Create an enrich policy](enrich-setup.html#create-enrich-policy \\"Create an enrich policy\\").\\n4. [Execute the enrich policy](enrich-setup.html#execute-enrich-policy \\"Execute the enrich policy\\").\\n5. [Add an enrich processor to an ingest pipeline](enrich-setup.html#add-enrich-processor \\"Add an enrich processor to an ingest pipeline\\").\\n6. [Ingest and enrich documents](enrich-setup.html#ingest-enrich-docs \\"Ingest and enrich documents\\").\\n\\nOnce you have an enrich processor set up, you can [update your enrich data](enrich-setup.html#update-enrich-data \\"Update an enrich index\\") and [update your enrich policies](enrich-setup.html#update-enrich-policies \\"Update an enrich policy\\").\\n\\nThe enrich processor performs several operations and may impact the speed of your ingest pipeline.\\n\\nWe strongly recommend testing and benchmarking your enrich processors before deploying them in production.\\n\\nWe do not recommend using the enrich processor to append real-time data. The enrich processor works best with reference data that doesn’t change frequently.\\n\\n#### Prerequisites\\n\\nTo use enrich policies, you must have:\\n\\n* `read` index privileges for any indices used\\n* The `enrich_user` [built-in role](built-in-roles.html \\"Built-in roles\\")\\n\\n### Add enrich data\\n\\nTo begin, add documents to one or more source indices. These documents should contain the enrich data you eventually want to add to incoming data.\\n\\nYou can manage source indices just like regular Elasticsearch indices using the [document](docs.html \\"Document APIs\\") and [index](indices.html \\"Index APIs\\") APIs.\\n\\nYou also can set up [Beats](/guide/en/beats/libbeat/8.17/getting-started.html), such as a [Filebeat](/guide/en/beats/filebeat/8.17/filebeat-installation-configuration.html), to automatically send and index documents to your source indices. See [Getting started with Beats](/guide/en/beats/libbeat/8.17/getting-started.html).\\n\\n### Create an enrich policy\\n\\nAfter adding enrich data to your source indices, use the [create enrich policy API](put-enrich-policy-api.html \\"Create enrich policy API\\") or [Index Management in Kibana](index-mgmt.html#manage-enrich-policies \\"Manage enrich policies\\") to create an enrich policy.\\n\\nOnce created, you can’t update or change an enrich policy. See [Update an enrich policy](enrich-setup.html#update-enrich-policies \\"Update an enrich policy\\").\\n\\n### Execute the enrich policy\\n\\nOnce the enrich policy is created, you need to execute it using the [execute enrich policy API](execute-enrich-policy-api.html \\"Execute enrich policy API\\") or [Index Management in Kibana](index-mgmt.html#manage-enrich-policies \\"Manage enrich policies\\") to create an [enrich index](ingest-enriching-data.html#enrich-index).\\n\\n![enrich policy index](images/ingest/enrich/enrich-policy-index.svg)\\n\\nThe *enrich index* contains documents from the policy’s source indices. Enrich indices always begin with `.enrich-*`, are read-only, and are [force merged](indices-forcemerge.html \\"Force merge API\\").\\n\\nEnrich indices should only be used by the [enrich processor](enrich-processor.html \\"Enrich processor\\") or the [ES|QL `ENRICH` command](esql-commands.html#esql-enrich \\"ENRICH\\"). Avoid using enrich indices for other purposes.\\n\\n### Add an enrich processor to an ingest pipeline\\n\\nOnce you have source indices, an enrich policy, and the related enrich index in place, you can set up an ingest pipeline that includes an enrich processor for your policy.\\n\\n![enrich processor](images/ingest/enrich/enrich-processor.svg)\\n\\nDefine an [enrich processor](enrich-processor.html \\"Enrich processor\\") and add it to an ingest pipeline using the [create or update pipeline API](put-pipeline-api.html \\"Create or update pipeline API\\").\\n\\nWhen defining the enrich processor, you must include at least the following:\\n\\n* The enrich policy to use.\\n* The field used to match incoming documents to the documents in your enrich index.\\n* The target field to add to incoming documents. This target field contains the match and enrich fields specified in your enrich policy.\\n\\nYou also can use the `max_matches` option to set the number of enrich documents an incoming document can match. If set to the default of `1`, data is added to an incoming document’s target field as a JSON object. Otherwise, the data is added as an array.\\n\\nSee [Enrich](enrich-processor.html \\"Enrich processor\\") for a full list of configuration options.\\n\\nYou also can add other [processors](processors.html \\"Ingest processor reference\\") to your ingest pipeline.\\n\\n### Ingest and enrich documents\\n\\nYou can now use your ingest pipeline to enrich and index documents.\\n\\n![enrich process](images/ingest/enrich/enrich-process.svg)\\n\\nBefore implementing the pipeline in production, we recommend indexing a few test documents first and verifying enrich data was added correctly using the [get API](docs-get.html \\"Get API\\").\\n\\n### Update an enrich index\\n\\nOnce created, you cannot update or index documents to an enrich index. Instead, update your source indices and [execute](execute-enrich-policy-api.html \\"Execute enrich policy API\\") the enrich policy again. This creates a new enrich index from your updated source indices. The previous enrich index will deleted with a delayed maintenance job. By default this is done every 15 minutes.\\n\\nIf wanted, you can [reindex](docs-reindex.html \\"Reindex API\\") or [update](docs-update-by-query.html \\"Update By Query API\\") any already ingested documents using your ingest pipeline.\\n\\n### Update an enrich policy\\n\\nOnce created, you can’t update or change an enrich policy. Instead, you can:\\n\\n1. Create and [execute](execute-enrich-policy-api.html \\"Execute enrich policy API\\") a new enrich policy.\\n2. Replace the previous enrich policy with the new enrich policy in any in-use enrich processors or ES|QL queries.\\n3. Use the [delete enrich policy](delete-enrich-policy-api.html \\"Delete enrich policy API\\") API or [Index Management in Kibana](index-mgmt.html#manage-enrich-policies \\"Manage enrich policies\\") to delete the previous enrich policy.\\n\\n### Enrich components\\n\\nThe enrich coordinator is a component that manages and performs the searches required to enrich documents on each ingest node. It combines searches from all enrich processors in all pipelines into bulk [multi-searches](search-multi-search.html \\"Multi search API\\").\\n\\nThe enrich policy executor is a component that manages the executions of all enrich policies. When an enrich policy is executed, this component creates a new enrich index and removes the previous enrich index. The enrich policy executions are managed from the elected master node. The execution of these policies occurs on a different node.\\n\\n### Node Settings\\n\\nThe `enrich` processor has node settings for enrich coordinator and enrich policy executor.\\n\\nThe enrich coordinator supports the following node settings:\\n\\n* `enrich.cache_size`\\n\\n  Maximum size of the cache that caches searches for enriching documents. The size can be specified in three units: the raw number of cached searches (e.g. `1000`), an absolute size in bytes (e.g. `100Mb`), or a percentage of the max heap space of the node (e.g. `1%`). Both for the absolute byte size and the percentage of heap space, Elasticsearch does not guarantee that the enrich cache size will adhere exactly to that maximum, as Elasticsearch uses the byte size of the serialized search response which is is a good representation of the used space on the heap, but not an exact match. Defaults to `1%`. There is a single cache for all enrich processors in the cluster.\\n\\n* `enrich.coordinator_proxy.max_concurrent_requests`\\n\\n  Maximum number of concurrent [multi-search requests](search-multi-search.html \\"Multi search API\\") to run when enriching documents. Defaults to `8`.\\n\\n* `enrich.coordinator_proxy.max_lookups_per_request`\\n\\n  Maximum number of searches to include in a [multi-search request](search-multi-search.html \\"Multi search API\\") when enriching documents. Defaults to `128`.\\n\\nThe enrich policy executor supports the following node settings:\\n\\n* `enrich.fetch_size`\\n\\n  Maximum batch size when reindexing a source index into an enrich index. Defaults to `10000`.\\n\\n* `enrich.max_force_merge_attempts`\\n\\n  Maximum number of [force merge](indices-forcemerge.html \\"Force merge API\\") attempts allowed on an enrich index. Defaults to `3`.\\n\\n* `enrich.cleanup_period`\\n\\n  How often Elasticsearch checks whether unused enrich indices can be deleted. Defaults to `15m`.\\n\\n* `enrich.max_concurrent_policy_executions`\\n\\n  Maximum number of enrich policies to execute concurrently. Defaults to `50`.\\n","title":"Set up an enrich processor","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/enrich-setup.html","productName":"elasticsearch","score":144.76215}',
        truncated: {
          truncatedText:
            '{"content":"## Set up an enrich processor\\n\\nTo set up an enrich processor, follow these steps:\\n\\n1. Check the [prerequisites](enrich-setup.html#enrich-prereqs \\"Prerequisites\\").\\n2. [Add enrich data](enrich-setup.html#create-enrich-source-index \\"Add enrich data\\").\\n3. [Create an enrich policy](enrich-setup.html#create-enrich-policy \\"Create an enrich policy\\").\\n4. [Execute the enrich policy](enrich-setup.html#execute-enrich-policy \\"Execute the enrich policy\\").\\n5. [Add an enrich processor to an ingest pipeline](enrich-setup.html#add-enrich-processor \\"Add an enrich processor to an ingest pipeline\\").\\n6. [Ingest and enrich documents](enrich-setup.html#ingest-enrich-docs \\"Ingest and enrich documents\\").\\n\\nOnce you have an enrich processor set up, you can [update your enrich data](enrich-setup.html#update-enrich-data \\"Update an enrich index\\") and [update your enrich policies](enrich-setup.html#update-enrich-policies \\"Update an enrich policy\\").\\n\\nThe enrich processor performs several operations and may impact the speed of your ingest pipeline.\\n\\nWe strongly recommend testing and benchmarking your enrich processors before deploying them in production.\\n\\nWe do not recommend using the enrich processor to append real-time data. The enrich processor works best with reference data that doesn’t change frequently.\\n\\n#### Prerequisites\\n\\nTo use enrich policies, you must have:\\n\\n* `read` index privileges for any indices used\\n* The `enrich_user` [built-in role](built-in-roles.html \\"Built-in roles\\")\\n\\n### Add enrich data\\n\\nTo begin, add documents to one or more source indices. These documents should contain the enrich data you eventually want to add to incoming data... <truncated>',
          originalTokenCount: 2095,
          truncatedTokenCount: 400,
        },
        llmScore: 4,
      },
      {
        selected: false,
        document: {
          content:
            '#### Parse data using ingest pipelines\n\nIngest pipelines preprocess and enrich APM documents before indexing them. For example, a pipeline might define one processor that removes a field, one that transforms a field, and another that renames a field.\n\nThe default APM pipelines are defined in Elasticsearch apm-data plugin index templates. Elasticsearch then uses the index pattern in these index templates to match pipelines to APM data streams.\n\n###### Custom ingest pipelines\n\nElastic APM supports custom ingest pipelines. A custom pipeline allows you to transform data to better match your specific use case. This can be useful, for example, to ensure data security by removing or obfuscating sensitive information.\n\nEach data stream ships with a default pipeline. This default pipeline calls an initially non-existent and non-versioned "`@custom`" ingest pipeline. If left uncreated, this pipeline has no effect on your data. However, if utilized, this pipeline can be used for custom data processing, adding fields, sanitizing data, and more.\n\nIn addition, ingest pipelines can also be used to direct application metrics (`metrics-apm.app.*`) to a data stream with a different dataset, e.g. to combine metrics for two applications. Sending other APM data to alternate data streams, like traces (`traces-apm.*`), logs (`logs-apm.*`), and internal metrics (`metrics-apm.internal*`) is not currently supported.\n\n###### `@custom` ingest pipeline naming convention\n\n`@custom` pipelines are specific to each data stream and follow a similar naming convention: `<type>-<dataset>@custom`. As a reminder, the default APM data streams are:\n\n* Application traces: `traces-apm-<namespace>`\n* RUM and iOS agent application traces: `traces-apm.rum-<namespace>`\n* APM internal metrics: `metrics-apm.internal-<namespace>`\n* APM transaction metrics: `metrics-apm.transaction.<metricset.interval>-<namespace>`\n* APM service destination metrics: `metrics-apm.service_destination.<metricset.interval>-<namespace>`\n* APM service transaction metrics: `metrics-apm.service_transaction.<metricset.interval>-<namespace>`\n* APM service summary metrics: `metrics-apm.service_summary.<metricset.interval>-<namespace>`\n* Application metrics: `metrics-apm.app.<service.name>-<namespace>`\n* APM error/exception logging: `logs-apm.error-<namespace>`\n* Applications UI logging: `logs-apm.app.<service.name>-<namespace>`\n\nTo match a custom ingest pipeline with a data stream, follow the `<type>-<dataset>@custom` template, or replace `-namespace` with `@custom` in the table above. For example, to target application traces, you’d create a pipeline named `traces-apm@custom`.\n\nThe `@custom` pipeline can directly contain processors or you can use the pipeline processor to call other pipelines that can be shared across multiple data streams or integrations. The `@custom` pipeline will persist across all version upgrades.\n\n###### Create a `@custom` ingest pipeline\n\nThe process for creating a custom ingest pipeline is as follows:\n\n* Create a pipeline with processors specific to your use case\n* Add the newly created pipeline to an `@custom` pipeline that matches an APM data stream\n\nIf you prefer more guidance, see one of these tutorials:\n\n* [Ingest pipeline filters](apm-custom-filter.html#apm-filters-ingest-pipeline "Ingest pipeline filters") — Learn how to obfuscate passwords stored in the `http.request.body.original` field.\n* [APM data stream rerouting](apm-data-streams.html#apm-data-stream-rerouting "APM data stream rerouting") — Learn how to reroute APM data to user-defined APM data streams.\n',
          title: 'Parse data using ingest pipelines',
          url: 'https://www.elastic.co/guide/en/observability/8.17/apm-ingest-pipelines.html',
          productName: 'observability',
          score: 142.52174,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/observability/8.17/apm-ingest-pipelines.html',
        title: 'Parse data using ingest pipelines',
        score: 142.52174,
        source: {
          product_documentation: {},
        },
        text: '{"content":"#### Parse data using ingest pipelines\\n\\nIngest pipelines preprocess and enrich APM documents before indexing them. For example, a pipeline might define one processor that removes a field, one that transforms a field, and another that renames a field.\\n\\nThe default APM pipelines are defined in Elasticsearch apm-data plugin index templates. Elasticsearch then uses the index pattern in these index templates to match pipelines to APM data streams.\\n\\n###### Custom ingest pipelines\\n\\nElastic APM supports custom ingest pipelines. A custom pipeline allows you to transform data to better match your specific use case. This can be useful, for example, to ensure data security by removing or obfuscating sensitive information.\\n\\nEach data stream ships with a default pipeline. This default pipeline calls an initially non-existent and non-versioned \\"`@custom`\\" ingest pipeline. If left uncreated, this pipeline has no effect on your data. However, if utilized, this pipeline can be used for custom data processing, adding fields, sanitizing data, and more.\\n\\nIn addition, ingest pipelines can also be used to direct application metrics (`metrics-apm.app.*`) to a data stream with a different dataset, e.g. to combine metrics for two applications. Sending other APM data to alternate data streams, like traces (`traces-apm.*`), logs (`logs-apm.*`), and internal metrics (`metrics-apm.internal*`) is not currently supported.\\n\\n###### `@custom` ingest pipeline naming convention\\n\\n`@custom` pipelines are specific to each data stream and follow a similar naming convention: `<type>-<dataset>@custom`. As a reminder, the default APM data streams are:\\n\\n* Application traces: `traces-apm-<namespace>`\\n* RUM and iOS agent application traces: `traces-apm.rum-<namespace>`\\n* APM internal metrics: `metrics-apm.internal-<namespace>`\\n* APM transaction metrics: `metrics-apm.transaction.<metricset.interval>-<namespace>`\\n* APM service destination metrics: `metrics-apm.service_destination.<metricset.interval>-<namespace>`\\n* APM service transaction metrics: `metrics-apm.service_transaction.<metricset.interval>-<namespace>`\\n* APM service summary metrics: `metrics-apm.service_summary.<metricset.interval>-<namespace>`\\n* Application metrics: `metrics-apm.app.<service.name>-<namespace>`\\n* APM error/exception logging: `logs-apm.error-<namespace>`\\n* Applications UI logging: `logs-apm.app.<service.name>-<namespace>`\\n\\nTo match a custom ingest pipeline with a data stream, follow the `<type>-<dataset>@custom` template, or replace `-namespace` with `@custom` in the table above. For example, to target application traces, you’d create a pipeline named `traces-apm@custom`.\\n\\nThe `@custom` pipeline can directly contain processors or you can use the pipeline processor to call other pipelines that can be shared across multiple data streams or integrations. The `@custom` pipeline will persist across all version upgrades.\\n\\n###### Create a `@custom` ingest pipeline\\n\\nThe process for creating a custom ingest pipeline is as follows:\\n\\n* Create a pipeline with processors specific to your use case\\n* Add the newly created pipeline to an `@custom` pipeline that matches an APM data stream\\n\\nIf you prefer more guidance, see one of these tutorials:\\n\\n* [Ingest pipeline filters](apm-custom-filter.html#apm-filters-ingest-pipeline \\"Ingest pipeline filters\\") — Learn how to obfuscate passwords stored in the `http.request.body.original` field.\\n* [APM data stream rerouting](apm-data-streams.html#apm-data-stream-rerouting \\"APM data stream rerouting\\") — Learn how to reroute APM data to user-defined APM data streams.\\n","title":"Parse data using ingest pipelines","url":"https://www.elastic.co/guide/en/observability/8.17/apm-ingest-pipelines.html","productName":"observability","score":142.52174}',
        truncated: {
          truncatedText:
            '{"content":"#### Parse data using ingest pipelines\\n\\nIngest pipelines preprocess and enrich APM documents before indexing them. For example, a pipeline might define one processor that removes a field, one that transforms a field, and another that renames a field.\\n\\nThe default APM pipelines are defined in Elasticsearch apm-data plugin index templates. Elasticsearch then uses the index pattern in these index templates to match pipelines to APM data streams.\\n\\n###### Custom ingest pipelines\\n\\nElastic APM supports custom ingest pipelines. A custom pipeline allows you to transform data to better match your specific use case. This can be useful, for example, to ensure data security by removing or obfuscating sensitive information.\\n\\nEach data stream ships with a default pipeline. This default pipeline calls an initially non-existent and non-versioned \\"`@custom`\\" ingest pipeline. If left uncreated, this pipeline has no effect on your data. However, if utilized, this pipeline can be used for custom data processing, adding fields, sanitizing data, and more.\\n\\nIn addition, ingest pipelines can also be used to direct application metrics (`metrics-apm.app.*`) to a data stream with a different dataset, e.g. to combine metrics for two applications. Sending other APM data to alternate data streams, like traces (`traces-apm.*`), logs (`logs-apm.*`), and internal metrics (`metrics-apm.internal*`) is not currently supported.\\n\\n###### `@custom` ingest pipeline naming convention\\n\\n`@custom` pipelines are specific to each data stream and follow a similar naming convention: `<type>-<dataset>@custom`. As a reminder, the default APM data streams are:\\n\\n* Application traces: `traces-apm-<namespace>`\\n* RUM and iOS agent application traces: `traces-apm.rum-<namespace>`\\n* APM internal metrics: `metrics-apm.internal-... <truncated>',
          originalTokenCount: 879,
          truncatedTokenCount: 400,
        },
        llmScore: 3,
      },
      {
        selected: false,
        document: {
          author: {
            login: 'ishleenk17',
          },
          issue_comments: [
            {
              author: {
                login: 'ruflin',
              },
              body: 'Looking forward to the outcome of this. Ideally ES would provide us some tooling here but until we get there, lets build it for the dev workflow. @marc-gr you might also be interested in this.',
            },
            {
              author: {
                login: 'ishleenk17',
              },
              body: '### Solution\r\nThe solution is to have a binary flag along with the if checks. This can be added by the developer to the ingest pipelines when he wants to enter the debug mode.\r\n\r\nAs in the below example: \r\n\r\nA plain if check \r\nif: “ctx.http?.response?.status_code != null \r\n\r\nshould become \r\nif: “ctx.http?.response?.status_code != null && !debug\r\n\r\nBut, there is no way a processor can read an env/parameter which is not coming through the documents.\r\nAs an alternative, we can make this a bound variable in the painless script that can be used anywhere painless is used.  It would then become independent of the ingest pipelines entirely. \r\n\r\n### Next Steps: \r\nExplore the injection of bound variables in a painless context. (Might require working with the es-core-infra/data management team) and demonstrate its usage for one of the integrations\r\n',
            },
            {
              author: {
                login: 'ruflin',
              },
              body: 'It might be worth also chatting about this with someone from the Elasticsearch team working on ingest pipelines (@dakrone maybe)  and share the experience. There might be more ideas.',
            },
            {
              author: {
                login: 'ishleenk17',
              },
              body: 'I am already in touch with @dakrone @rjernst  @stu-elastic  from ES data management and core team to discuss further on this.\r\n@rjernst @stu-elastic as discussed we can use script params in the IngestConditionalScript. Does pipeline has a facility to alter these params on the fly or are these expected to come through the document ?',
            },
            {
              author: {
                login: 'stu-elastic',
              },
              body: '>  Does pipeline has a facility to alter these params on the fly or are these expected to come through the document ?\r\n\r\nNo, that would require a code change, some code would have to tell IngestDocument (or via a similar code path) that for this invocation of the pipeline, we want to include script parameters.  Then, when the IngestConditionalScript is instantiated, it is passed the fresh set of parameters.\r\n\r\nAlternatively, you can set a flag on the document itself and the conditional becomes `!ctx.debug && ...`',
            },
            {
              author: {
                login: 'ruflin',
              },
              body: 'Some additional ideas on my end. Ideally we would not have to modify the pipelines itself to enable debugging.\r\n\r\nOne approach could be to enable debugging for pipelines in general through a flag:\r\n\r\n```\r\nPOST logs-foo-bar/_doc?debug_ingest_pipeline=true\r\n{\r\n  "@timestamp": "2099-11-15T13:12:00",\r\n  "message": "Hello world",\r\n}\r\n```\r\n\r\nWhat that would exactly means would need more discussions. Also my assumption would be that a user that only has append permissions, could not just change the behaviour of ingestion by adding a flag. \r\n\r\nAnother option is to offer more fine grained options:\r\n\r\n```\r\nPOST logs-foo-bar/_doc?debug_params=["ignore_missing_false", "on_failure_false"]\r\n{\r\n  "@timestamp": "2099-11-15T13:12:00",\r\n  "message": "Hello world",\r\n}\r\n```\r\n\r\nWhat should be skipped, can be specified in some debug params.\r\n\r\nOne of the goals on my end is that for debugging a pipeline, the pipeline and the data do not have to be modified.',
            },
            {
              author: {
                login: 'ruflin',
              },
              body: 'One more idea that might be less intrusive, allow these flags for the simulate API: https://www.elastic.co/guide/en/elasticsearch/reference/current/simulate-pipeline-api.html',
            },
          ],
          title: 'Spacetime: Better debugging of Ingest Pipelines ',
          body: '## Description Better debugging of the ingest pipelines for developers ## Goals Ingest Pipelines make use of processors. Whenever we use processors, the failures can be handled in multiple ways. Different categories of Failure handling: Ignore_missing: If true and field doesn’t exist or is null, the processor exits without modifying the document. Drawbacks: None Eg: GCP: <img width="467" alt="Screenshot 2022-12-02 at 2 08 14 PM" src="https://user-images.githubusercontent.com/102962586/205251631-111cb656-0712-4106-b13d-400f8cc6ecba.png"> Ignore_failure: No modification is done to the document in case failure occurs and this is set to true. Drawbacks: We might actually not catch a probable error. This is used in multiple places across integrations. Eg: AWS Cloudtrail: <img width="467" alt="Screenshot 2022-12-02 at 2 08 14 PM" src="https://user-images.githubusercontent.com/102962586/205251669-b46eb286-4a24-4635-bfe1-720a43ca174b.png"> On_failure: In this case the failure is caught and required action is taken. Eg: Eg: Zscaler_Zia : This is a good way to handle failures, but very few fields have this. <img width="585" alt="Screenshot 2022-12-02 at 2 13 05 PM" src="https://user-images.githubusercontent.com/102962586/205252167-e20ad90c-8025-4a54-8e83-b880b815ba5a.png"> Null failure handling with if conditions: Drawbacks: If the field exists and is incorrectly updated by someone else before the elastic agent. Then those errors will not be caught. Eg: Azure: <img width="1131" alt="Screenshot 2022-12-02 at 2 15 45 PM" src="https://user-images.githubusercontent.com/102962586/205252678-324dcc9f-daf2-41de-a6f5-d1876e15754e.png"> No handling: Failures are not handled at all Drawback: Pipeline processing will stop in case failures are not handled Eg: Apache: <img width="620" alt="Screenshot 2022-12-02 at 2 17 05 PM" src="https://user-images.githubusercontent.com/102962586/205252904-ea9803dc-d749-48cf-b77d-7355a3ad961f.png"> Having no failure handling, resulted in a couple of [SDH’s](https://github.com/elastic/integrations/issues/3451) where target_field already existed but this situation was not handled. This could have been handled by following null failure check as in (4). But if we would have used (4) we wouldn’t have known that LS was incorrectly adding its own value to event.original. Hence, we need to find a better way out.',
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/2478',
          number: 2478,
          createdAt: '2022-12-02T08:49:39Z',
          assignees_list: [
            {
              login: 'ishleenk17',
            },
          ],
          labels_field: [
            {
              name: 'spacetime',
              description: '',
            },
            {
              name: 'Team:Service-Integrations',
              description: 'Label for the Service Integrations team',
            },
          ],
          state: 'OPEN',
          id: 'I_kwDOCYaMzs5XxN7a',
          closedAt: null,
          _timestamp: '2022-12-21T14:52:12Z',
        },
        id: 'search-observability-dev/I_kwDOCYaMzs5XxN7a',
        title: 'Spacetime: Better debugging of Ingest Pipelines ',
        score: 140.9871,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"ishleenk17"},"issue_comments":[{"author":{"login":"ruflin"},"body":"Looking forward to the outcome of this. Ideally ES would provide us some tooling here but until we get there, lets build it for the dev workflow. @marc-gr you might also be interested in this."},{"author":{"login":"ishleenk17"},"body":"### Solution\\r\\nThe solution is to have a binary flag along with the if checks. This can be added by the developer to the ingest pipelines when he wants to enter the debug mode.\\r\\n\\r\\nAs in the below example: \\r\\n\\r\\nA plain if check \\r\\nif: “ctx.http?.response?.status_code != null \\r\\n\\r\\nshould become \\r\\nif: “ctx.http?.response?.status_code != null && !debug\\r\\n\\r\\nBut, there is no way a processor can read an env/parameter which is not coming through the documents.\\r\\nAs an alternative, we can make this a bound variable in the painless script that can be used anywhere painless is used.  It would then become independent of the ingest pipelines entirely. \\r\\n\\r\\n### Next Steps: \\r\\nExplore the injection of bound variables in a painless context. (Might require working with the es-core-infra/data management team) and demonstrate its usage for one of the integrations\\r\\n"},{"author":{"login":"ruflin"},"body":"It might be worth also chatting about this with someone from the Elasticsearch team working on ingest pipelines (@dakrone maybe)  and share the experience. There might be more ideas."},{"author":{"login":"ishleenk17"},"body":"I am already in touch with @dakrone @rjernst  @stu-elastic  from ES data management and core team to discuss further on this.\\r\\n@rjernst @stu-elastic as discussed we can use script params in the IngestConditionalScript. Does pipeline has a facility to alter these params on the fly or are these expected to come through the document ?"},{"author":{"login":"stu-elastic"},"body":">  Does pipeline has a facility to alter these params on the fly or are these expected to come through the document ?\\r\\n\\r\\nNo, that would require a code change, some code would have to tell IngestDocument (or via a similar code path) that for this invocation of the pipeline, we want to include script parameters.  Then, when the IngestConditionalScript is instantiated, it is passed the fresh set of parameters.\\r\\n\\r\\nAlternatively, you can set a flag on the document itself and the conditional becomes `!ctx.debug && ...`"},{"author":{"login":"ruflin"},"body":"Some additional ideas on my end. Ideally we would not have to modify the pipelines itself to enable debugging.\\r\\n\\r\\nOne approach could be to enable debugging for pipelines in general through a flag:\\r\\n\\r\\n```\\r\\nPOST logs-foo-bar/_doc?debug_ingest_pipeline=true\\r\\n{\\r\\n  \\"@timestamp\\": \\"2099-11-15T13:12:00\\",\\r\\n  \\"message\\": \\"Hello world\\",\\r\\n}\\r\\n```\\r\\n\\r\\nWhat that would exactly means would need more discussions. Also my assumption would be that a user that only has append permissions, could not just change the behaviour of ingestion by adding a flag. \\r\\n\\r\\nAnother option is to offer more fine grained options:\\r\\n\\r\\n```\\r\\nPOST logs-foo-bar/_doc?debug_params=[\\"ignore_missing_false\\", \\"on_failure_false\\"]\\r\\n{\\r\\n  \\"@timestamp\\": \\"2099-11-15T13:12:00\\",\\r\\n  \\"message\\": \\"Hello world\\",\\r\\n}\\r\\n```\\r\\n\\r\\nWhat should be skipped, can be specified in some debug params.\\r\\n\\r\\nOne of the goals on my end is that for debugging a pipeline, the pipeline and the data do not have to be modified."},{"author":{"login":"ruflin"},"body":"One more idea that might be less intrusive, allow these flags for the simulate API: https://www.elastic.co/guide/en/elasticsearch/reference/current/simulate-pipeline-api.html"}],"title":"Spacetime: Better debugging of Ingest Pipelines ","body":"## Description Better debugging of the ingest pipelines for developers ## Goals Ingest Pipelines make use of processors. Whenever we use processors, the failures can be handled in multiple ways. Different categories of Failure handling: Ignore_missing: If true and field doesn’t exist or is null, the processor exits without modifying the document. Drawbacks: None Eg: GCP: <img width=\\"467\\" alt=\\"Screenshot 2022-12-02 at 2 08 14 PM\\" src=\\"https://user-images.githubusercontent.com/102962586/205251631-111cb656-0712-4106-b13d-400f8cc6ecba.png\\"> Ignore_failure: No modification is done to the document in case failure occurs and this is set to true. Drawbacks: We might actually not catch a probable error. This is used in multiple places across integrations. Eg: AWS Cloudtrail: <img width=\\"467\\" alt=\\"Screenshot 2022-12-02 at 2 08 14 PM\\" src=\\"https://user-images.githubusercontent.com/102962586/205251669-b46eb286-4a24-4635-bfe1-720a43ca174b.png\\"> On_failure: In this case the failure is caught and required action is taken. Eg: Eg: Zscaler_Zia : This is a good way to handle failures, but very few fields have this. <img width=\\"585\\" alt=\\"Screenshot 2022-12-02 at 2 13 05 PM\\" src=\\"https://user-images.githubusercontent.com/102962586/205252167-e20ad90c-8025-4a54-8e83-b880b815ba5a.png\\"> Null failure handling with if conditions: Drawbacks: If the field exists and is incorrectly updated by someone else before the elastic agent. Then those errors will not be caught. Eg: Azure: <img width=\\"1131\\" alt=\\"Screenshot 2022-12-02 at 2 15 45 PM\\" src=\\"https://user-images.githubusercontent.com/102962586/205252678-324dcc9f-daf2-41de-a6f5-d1876e15754e.png\\"> No handling: Failures are not handled at all Drawback: Pipeline processing will stop in case failures are not handled Eg: Apache: <img width=\\"620\\" alt=\\"Screenshot 2022-12-02 at 2 17 05 PM\\" src=\\"https://user-images.githubusercontent.com/102962586/205252904-ea9803dc-d749-48cf-b77d-7355a3ad961f.png\\"> Having no failure handling, resulted in a couple of [SDH’s](https://github.com/elastic/integrations/issues/3451) where target_field already existed but this situation was not handled. This could have been handled by following null failure check as in (4). But if we would have used (4) we wouldn’t have known that LS was incorrectly adding its own value to event.original. Hence, we need to find a better way out.","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/2478","number":2478,"createdAt":"2022-12-02T08:49:39Z","assignees_list":[{"login":"ishleenk17"}],"labels_field":[{"name":"spacetime","description":""},{"name":"Team:Service-Integrations","description":"Label for the Service Integrations team"}],"state":"OPEN","id":"I_kwDOCYaMzs5XxN7a","closedAt":null,"_timestamp":"2022-12-21T14:52:12Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"ishleenk17"},"issue_comments":[{"author":{"login":"ruflin"},"body":"Looking forward to the outcome of this. Ideally ES would provide us some tooling here but until we get there, lets build it for the dev workflow. @marc-gr you might also be interested in this."},{"author":{"login":"ishleenk17"},"body":"### Solution\\r\\nThe solution is to have a binary flag along with the if checks. This can be added by the developer to the ingest pipelines when he wants to enter the debug mode.\\r\\n\\r\\nAs in the below example: \\r\\n\\r\\nA plain if check \\r\\nif: “ctx.http?.response?.status_code != null \\r\\n\\r\\nshould become \\r\\nif: “ctx.http?.response?.status_code != null && !debug\\r\\n\\r\\nBut, there is no way a processor can read an env/parameter which is not coming through the documents.\\r\\nAs an alternative, we can make this a bound variable in the painless script that can be used anywhere painless is used.  It would then become independent of the ingest pipelines entirely. \\r\\n\\r\\n### Next Steps: \\r\\nExplore the injection of bound variables in a painless context. (Might require working with the es-core-infra/data management team) and demonstrate its usage for one of the integrations\\r\\n"},{"author":{"login":"ruflin"},"body":"It might be worth also chatting about this with someone from the Elasticsearch team working on ingest pipelines (@dakrone maybe)  and share the experience. There might be more ideas."},{"author":{"login":"ishleenk17"},"body":"I am already in touch with @dakrone @rjernst  @stu-elastic  from ES data management and core team to discuss further on this.\\r\\n@rjernst @stu-elastic as discussed we can... <truncated>',
          originalTokenCount: 1687,
          truncatedTokenCount: 400,
        },
        llmScore: 1,
      },
      {
        selected: false,
        document: {
          author: {
            login: 'clintongormley',
          },
          issue_comments: [
            {
              author: {
                login: 'ruflin',
              },
              body: 'Quite a few changes have been discussed to the above in https://docs.google.com/document/d/1UonHaMWgLvmLgiUfNnPg38CyNUPs2hfGDrsI-DCfkgw/edit# My goal is to also document the above in a [public place](https://github.com/elastic/kibana/pull/55361) and have implementation issue for each of the above. Some things like Alias Templates are not implemented yet so keep this issue open for now.',
            },
            {
              author: {
                login: 'ruflin',
              },
              body: 'Closing this issue as the documentation of the above was moved here: https://github.com/elastic/kibana/blob/feature-ingest/docs/epm/index.asciidoc It misses some details on the final pipeline and the ML jobs. As soon as we will start to implement it, we will also add it to the docs.',
            },
          ],
          title:
            'RFC: Assets - dashboards, alias templates, ingest pipelines, ILM policies, ML Jobs',
          body: "Parent issue https://github.com/elastic/observability-dev/issues/440 # Preface This issue supersedes https://github.com/elastic/observability-dev/issues/484 for three reasons: 1. This issue starts with a different assumption: that the user should not be able to edit the mappings and pipelines for existing packages. If they want to customise a package then they should clone it and give it a new name. 2. We would use a newly proposed feature called alias templates. These are like index templates except that they specify an alias pattern to match. When the user indexes into an alias/index which doesn't exist and which matches an alias pattern, ES create an index called `{alias}-0000` and sets the write alias to the `{alias}` name. This removes the need for us to create the index and alias manually before being able to index into it. 3. We can rely on `constant_keyword` fields for fast filtering and aggs instead of needing to parse the index names to extract package, dataset, etc values. # Intro Each package (eg MySQL, Nginx, etc) contains a number of assets, eg visualisations, dashboards, alias templates, ingest pipelines, etc. These assets can be installed: * when Kibana is first started, eg ILM policies * when the package is installed (eg dashboards) * when the policy is set up (eg an alias template which includes the `label` name) # ILM Policies We will install two ILM policies at startup if they don't already exist - one for `logs` and one for `metrics`. Data streams will use the appropriate policy by default - the user can go in and edit the policy, or they can add and use their own policies. # Dashboard and visualisations Dashboards and their visualisations should be installed into Kibana when the package is installed. Visualisations are not shared - they are used only by the dashboard(s) in the package. The same dashboards are used for all instances of the package. For instance, if mysql is being monitored once in `prod`, once in `dev`, and once in `staging`, then there would be only one dashboard which allows the user to filter data by zero or more of the labels `prod`, `dev`, and `staging`. The label values for this filter are automatically calculated. Dashboards and their visualisations are removed from Kibana when a package is uninstalled. Dashboards should be upgraded to the most recent version. The full Dashboard name consists of the package name plus the dashboard sub-name, eg `mysql-overview`, `mysql-detail`. # Ingest Pipelines Docs may need to be passed through multiple ingest pipelines before ingestion. There will be: * the package-level ingest pipeline for the dataset being processed, which should also deal with upgrading older versions of data. * some globally required metadata such as ingest timestamp and username/uid * other solutions may want to enrich documents, eg SIEM or machine learning. There are three ways of specifying that a pipeline should be applied: * in the request, * as a `default_pipeline` (which is run unless one is specified in the request), or: * as a `final_pipeline` (which is always run). ## Per-dataset ingest pipelines Each package contains an ingest pipeline per dataset template, named as `{type}-{dataset}-{version}`, eg the MySQL package will contain three ingest pipelines: `logs-mysql.access-7.5`, `logs-mysql.error-7.5`, and `metrics-mysql.status-7.5`. These pipelines are also responsible for upgrading data from older versions to fit the new data model, which they can do using version checks. The package level pipeline would be configured as the `default_pipeline` in the alias template. That way, when the index is rolled over and the new mapping is in place, ingest will start using the new pipeline at the same time, ensuring there is no race condition between mapping and pipeline changes. These pipelines are installed when the package is installed into Kibana. ## Global ingest pipeline There should be a single global ingest pipeline which solutions can hook into to ensure that their processing is performed, eg the security processor which writes the username or API key UID into the document, SIEM enrichment, ML enrichment, etc. All alias templates will specify that the `final_pipeline` is this global, required, pipeline. The Ingest Manager should be in charge of assembling and publishing the global pipeline. The global pipeline also needs to be versioned (a simple counter, rather than package level versions), and the version should be incremented every time a change happens that requires syncing the pipeline with mapping changes. The Ingest Manager should provide an API which updates the global pipeline version in all alias templates, then triggers a rollover of all indices to ensure the mapping is in place at the same moment the new global pipeline starts working. # Alias Templates There are five sources of index mappings and settings: * The global pipeline requires certain fields, eg index timestamp and user. * Each dataset in a package contains mappings. * Each stream might require different settings, eg number of primary/replica shards, ILM policy, shard allocation, etc. * Different environments may require extra fields, eg docker, kubernetes, aws, etc. * Solutions like SIEM or ML might require any number of fields. ## Global Alias Template The global alias template should be in sync with the global ingest pipeline, which should only change at most once per stack upgrade (excluding call outs to SIEM and other solutions). Rather than having a real global alias template, these couple of fields can be rolled into each stream alias template. **Perhaps SIEM fields can be included here?** ## Preconfigured Stream Alias Templates Each dataset in a package comes with a standard list of fields, which might vary by package version. These fields can be merged with the global fields like index timestamp and ingest user. When a stream is added to a Policy and assigned a label, the user can also configure things like the number of primary/replica shards, the ILM policy, and the shard allocation. This information is added to an alias template called `{type}-{label}-{policy}-{dataset}` (eg `logs-prod-db_servers-mysql.access`) which matches aliases of the same name, ie `{type}-{label}-{policy}-{dataset}` (eg `logs-prod-db_servers-mysql.access`). The alias template specifies that the `default_pipeline` should be `{type}-{dataset}-{version}` and the `final_pipeline` should be `global-{version}`. This way, as soon as the index rolls over, the new mappings will be in sync with the versioned ingest pipeline. ## Autodetected labels per Stream Alias Templates In Kubernetes, we may know that we have various technologies installed, but namespaces may be added dynamically by the user and we want to autodetect these namespaces and create the alias automatically. To support this, we can support wildcard alias templates much like the stream templates above. The user would be able to configure: * The wildcard for matching, eg `*` or `*@my_k8s_cluster` etc * The template for the label, eg `{k8s.env}/{k8s.namespace}@my_k8s_cluster` * Default settings for number of shards, ILM policy, shard allocation, etc We would create a stream alias template which contains the global fields plus the input fields plus the user specified index settings. It would match on the wildcard specified by the user, and the `default_pipeline` would be `{type}-{dataset}-{version}` and the `final_pipeline` would be `global-{version}`. Later, if the user wanted to customise the index settings for one particular label, then they would create a stream alias template instead, and give it a higher priority. # Machine Learning Jobs It is not useful to start ML jobs on installation as there is no data to process and early data may not be representative of a stable state. Also, we should ask before enabling ML jobs as they can be resource intensive. The ML package (today hardcoded in the ML app), could be installed as part of the package. Apart from the ML jobs that should be run, the package would also contain a query which can determine whether any new data sources have been added to the system. If new sources are detected, the ML app could popup an alert to the user, asking them if they want to enable these jobs.",
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/499',
          number: 499,
          createdAt: '2019-12-09T17:34:12Z',
          assignees_list: [
            {
              login: 'ruflin',
            },
          ],
          labels_field: [
            {
              name: 'meta',
              description: '',
            },
            {
              name: 'Team:Ingest Management',
              description: 'Label for the Ingest Management team',
            },
          ],
          state: 'CLOSED',
          id: 'MDU6SXNzdWU1MzUwNzkyNTg=',
          closedAt: '2020-02-12T13:17:17Z',
          _timestamp: '2025-02-17T07:27:33Z',
        },
        id: 'search-observability-dev/MDU6SXNzdWU1MzUwNzkyNTg=',
        title: 'RFC: Assets - dashboards, alias templates, ingest pipelines, ILM policies, ML Jobs',
        score: 139.48297,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"clintongormley"},"issue_comments":[{"author":{"login":"ruflin"},"body":"Quite a few changes have been discussed to the above in https://docs.google.com/document/d/1UonHaMWgLvmLgiUfNnPg38CyNUPs2hfGDrsI-DCfkgw/edit# My goal is to also document the above in a [public place](https://github.com/elastic/kibana/pull/55361) and have implementation issue for each of the above. Some things like Alias Templates are not implemented yet so keep this issue open for now."},{"author":{"login":"ruflin"},"body":"Closing this issue as the documentation of the above was moved here: https://github.com/elastic/kibana/blob/feature-ingest/docs/epm/index.asciidoc It misses some details on the final pipeline and the ML jobs. As soon as we will start to implement it, we will also add it to the docs."}],"title":"RFC: Assets - dashboards, alias templates, ingest pipelines, ILM policies, ML Jobs","body":"Parent issue https://github.com/elastic/observability-dev/issues/440 # Preface This issue supersedes https://github.com/elastic/observability-dev/issues/484 for three reasons: 1. This issue starts with a different assumption: that the user should not be able to edit the mappings and pipelines for existing packages. If they want to customise a package then they should clone it and give it a new name. 2. We would use a newly proposed feature called alias templates. These are like index templates except that they specify an alias pattern to match. When the user indexes into an alias/index which doesn\'t exist and which matches an alias pattern, ES create an index called `{alias}-0000` and sets the write alias to the `{alias}` name. This removes the need for us to create the index and alias manually before being able to index into it. 3. We can rely on `constant_keyword` fields for fast filtering and aggs instead of needing to parse the index names to extract package, dataset, etc values. # Intro Each package (eg MySQL, Nginx, etc) contains a number of assets, eg visualisations, dashboards, alias templates, ingest pipelines, etc. These assets can be installed: * when Kibana is first started, eg ILM policies * when the package is installed (eg dashboards) * when the policy is set up (eg an alias template which includes the `label` name) # ILM Policies We will install two ILM policies at startup if they don\'t already exist - one for `logs` and one for `metrics`. Data streams will use the appropriate policy by default - the user can go in and edit the policy, or they can add and use their own policies. # Dashboard and visualisations Dashboards and their visualisations should be installed into Kibana when the package is installed. Visualisations are not shared - they are used only by the dashboard(s) in the package. The same dashboards are used for all instances of the package. For instance, if mysql is being monitored once in `prod`, once in `dev`, and once in `staging`, then there would be only one dashboard which allows the user to filter data by zero or more of the labels `prod`, `dev`, and `staging`. The label values for this filter are automatically calculated. Dashboards and their visualisations are removed from Kibana when a package is uninstalled. Dashboards should be upgraded to the most recent version. The full Dashboard name consists of the package name plus the dashboard sub-name, eg `mysql-overview`, `mysql-detail`. # Ingest Pipelines Docs may need to be passed through multiple ingest pipelines before ingestion. There will be: * the package-level ingest pipeline for the dataset being processed, which should also deal with upgrading older versions of data. * some globally required metadata such as ingest timestamp and username/uid * other solutions may want to enrich documents, eg SIEM or machine learning. There are three ways of specifying that a pipeline should be applied: * in the request, * as a `default_pipeline` (which is run unless one is specified in the request), or: * as a `final_pipeline` (which is always run). ## Per-dataset ingest pipelines Each package contains an ingest pipeline per dataset template, named as `{type}-{dataset}-{version}`, eg the MySQL package will contain three ingest pipelines: `logs-mysql.access-7.5`, `logs-mysql.error-7.5`, and `metrics-mysql.status-7.5`. These pipelines are also responsible for upgrading data from older versions to fit the new data model, which they can do using version checks. The package level pipeline would be configured as the `default_pipeline` in the alias template. That way, when the index is rolled over and the new mapping is in place, ingest will start using the new pipeline at the same time, ensuring there is no race condition between mapping and pipeline changes. These pipelines are installed when the package is installed into Kibana. ## Global ingest pipeline There should be a single global ingest pipeline which solutions can hook into to ensure that their processing is performed, eg the security processor which writes the username or API key UID into the document, SIEM enrichment, ML enrichment, etc. All alias templates will specify that the `final_pipeline` is this global, required, pipeline. The Ingest Manager should be in charge of assembling and publishing the global pipeline. The global pipeline also needs to be versioned (a simple counter, rather than package level versions), and the version should be incremented every time a change happens that requires syncing the pipeline with mapping changes. The Ingest Manager should provide an API which updates the global pipeline version in all alias templates, then triggers a rollover of all indices to ensure the mapping is in place at the same moment the new global pipeline starts working. # Alias Templates There are five sources of index mappings and settings: * The global pipeline requires certain fields, eg index timestamp and user. * Each dataset in a package contains mappings. * Each stream might require different settings, eg number of primary/replica shards, ILM policy, shard allocation, etc. * Different environments may require extra fields, eg docker, kubernetes, aws, etc. * Solutions like SIEM or ML might require any number of fields. ## Global Alias Template The global alias template should be in sync with the global ingest pipeline, which should only change at most once per stack upgrade (excluding call outs to SIEM and other solutions). Rather than having a real global alias template, these couple of fields can be rolled into each stream alias template. **Perhaps SIEM fields can be included here?** ## Preconfigured Stream Alias Templates Each dataset in a package comes with a standard list of fields, which might vary by package version. These fields can be merged with the global fields like index timestamp and ingest user. When a stream is added to a Policy and assigned a label, the user can also configure things like the number of primary/replica shards, the ILM policy, and the shard allocation. This information is added to an alias template called `{type}-{label}-{policy}-{dataset}` (eg `logs-prod-db_servers-mysql.access`) which matches aliases of the same name, ie `{type}-{label}-{policy}-{dataset}` (eg `logs-prod-db_servers-mysql.access`). The alias template specifies that the `default_pipeline` should be `{type}-{dataset}-{version}` and the `final_pipeline` should be `global-{version}`. This way, as soon as the index rolls over, the new mappings will be in sync with the versioned ingest pipeline. ## Autodetected labels per Stream Alias Templates In Kubernetes, we may know that we have various technologies installed, but namespaces may be added dynamically by the user and we want to autodetect these namespaces and create the alias automatically. To support this, we can support wildcard alias templates much like the stream templates above. The user would be able to configure: * The wildcard for matching, eg `*` or `*@my_k8s_cluster` etc * The template for the label, eg `{k8s.env}/{k8s.namespace}@my_k8s_cluster` * Default settings for number of shards, ILM policy, shard allocation, etc We would create a stream alias template which contains the global fields plus the input fields plus the user specified index settings. It would match on the wildcard specified by the user, and the `default_pipeline` would be `{type}-{dataset}-{version}` and the `final_pipeline` would be `global-{version}`. Later, if the user wanted to customise the index settings for one particular label, then they would create a stream alias template instead, and give it a higher priority. # Machine Learning Jobs It is not useful to start ML jobs on installation as there is no data to process and early data may not be representative of a stable state. Also, we should ask before enabling ML jobs as they can be resource intensive. The ML package (today hardcoded in the ML app), could be installed as part of the package. Apart from the ML jobs that should be run, the package would also contain a query which can determine whether any new data sources have been added to the system. If new sources are detected, the ML app could popup an alert to the user, asking them if they want to enable these jobs.","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/499","number":499,"createdAt":"2019-12-09T17:34:12Z","assignees_list":[{"login":"ruflin"}],"labels_field":[{"name":"meta","description":""},{"name":"Team:Ingest Management","description":"Label for the Ingest Management team"}],"state":"CLOSED","id":"MDU6SXNzdWU1MzUwNzkyNTg=","closedAt":"2020-02-12T13:17:17Z","_timestamp":"2025-02-17T07:27:33Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"clintongormley"},"issue_comments":[{"author":{"login":"ruflin"},"body":"Quite a few changes have been discussed to the above in https://docs.google.com/document/d/1UonHaMWgLvmLgiUfNnPg38CyNUPs2hfGDrsI-DCfkgw/edit# My goal is to also document the above in a [public place](https://github.com/elastic/kibana/pull/55361) and have implementation issue for each of the above. Some things like Alias Templates are not implemented yet so keep this issue open for now."},{"author":{"login":"ruflin"},"body":"Closing this issue as the documentation of the above was moved here: https://github.com/elastic/kibana/blob/feature-ingest/docs/epm/index.asciidoc It misses some details on the final pipeline and the ML jobs. As soon as we will start to implement it, we will also add it to the docs."}],"title":"RFC: Assets - dashboards, alias templates, ingest pipelines, ILM policies, ML Jobs","body":"Parent issue https://github.com/elastic/observability-dev/issues/440 # Preface This issue supersedes https://github.com/elastic/observability-dev/issues/484 for three reasons: 1. This issue starts with a different assumption: that the user should not be able to edit the mappings and pipelines for existing packages. If they want to customise a package then they should clone it and give it a new name. 2. We would use a newly proposed feature called alias templates. These are like index templates except that they specify an alias pattern to match. When the user indexes into an alias/index which doesn\'t exist and which matches an alias pattern, ES create an index called `{alias}-0000` and sets the write alias to the `{alias}` name. This removes the need for us to... <truncated>',
          originalTokenCount: 2098,
          truncatedTokenCount: 400,
        },
        llmScore: 1,
      },
      {
        selected: false,
        document: {
          content:
            '## Get pipeline API\n\nReturns information about one or more ingest pipelines. This API returns a local reference of the pipeline.\n\n```\nresp = client.ingest.get_pipeline(\n    id="my-pipeline-id",\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.get_pipeline(\n  id: \'my-pipeline-id\'\n)\nputs response\n```\n\n```\nconst response = await client.ingest.getPipeline({\n  id: "my-pipeline-id",\n});\nconsole.log(response);\n```\n\n```\nGET /_ingest/pipeline/my-pipeline-id\n```\n\n### Request\n\n`GET /_ingest/pipeline/<pipeline>`\n\n`GET /_ingest/pipeline`\n\n### Prerequisites\n\n* If the Elasticsearch security features are enabled, you must have the `read_pipeline`, `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to use this API.\n\n### Path parameters\n\n* `<pipeline>`\n\n  (Optional, string) Comma-separated list of pipeline IDs to retrieve. Wildcard (`*`) expressions are supported.\n\n  To get all ingest pipelines, omit this parameter or use `*`.\n\n### Query parameters\n\n* `master_timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n### Examples\n\n#### Get information for a specific ingest pipeline\n\n```\nresp = client.ingest.get_pipeline(\n    id="my-pipeline-id",\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.get_pipeline(\n  id: \'my-pipeline-id\'\n)\nputs response\n```\n\n```\nconst response = await client.ingest.getPipeline({\n  id: "my-pipeline-id",\n});\nconsole.log(response);\n```\n\n```\nGET /_ingest/pipeline/my-pipeline-id\n```\n\nThe API returns the following response:\n\n```\n{\n  "my-pipeline-id" : {\n    "description" : "describe pipeline",\n    "version" : 123,\n    "processors" : [\n      {\n        "set" : {\n          "field" : "foo",\n          "value" : "bar"\n        }\n      }\n    ]\n  }\n}\n```\n',
          title: 'Get pipeline API',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/get-pipeline-api.html',
          productName: 'elasticsearch',
          score: 137.86902,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/get-pipeline-api.html',
        title: 'Get pipeline API',
        score: 137.86902,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Get pipeline API\\n\\nReturns information about one or more ingest pipelines. This API returns a local reference of the pipeline.\\n\\n```\\nresp = client.ingest.get_pipeline(\\n    id=\\"my-pipeline-id\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.get_pipeline(\\n  id: \'my-pipeline-id\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.getPipeline({\\n  id: \\"my-pipeline-id\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nGET /_ingest/pipeline/my-pipeline-id\\n```\\n\\n### Request\\n\\n`GET /_ingest/pipeline/<pipeline>`\\n\\n`GET /_ingest/pipeline`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `read_pipeline`, `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to use this API.\\n\\n### Path parameters\\n\\n* `<pipeline>`\\n\\n  (Optional, string) Comma-separated list of pipeline IDs to retrieve. Wildcard (`*`) expressions are supported.\\n\\n  To get all ingest pipelines, omit this parameter or use `*`.\\n\\n### Query parameters\\n\\n* `master_timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n### Examples\\n\\n#### Get information for a specific ingest pipeline\\n\\n```\\nresp = client.ingest.get_pipeline(\\n    id=\\"my-pipeline-id\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.get_pipeline(\\n  id: \'my-pipeline-id\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.getPipeline({\\n  id: \\"my-pipeline-id\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nGET /_ingest/pipeline/my-pipeline-id\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n  \\"my-pipeline-id\\" : {\\n    \\"description\\" : \\"describe pipeline\\",\\n    \\"version\\" : 123,\\n    \\"processors\\" : [\\n      {\\n        \\"set\\" : {\\n          \\"field\\" : \\"foo\\",\\n          \\"value\\" : \\"bar\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n","title":"Get pipeline API","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/get-pipeline-api.html","productName":"elasticsearch","score":137.86902}',
        truncated: {
          truncatedText:
            '{"content":"## Get pipeline API\\n\\nReturns information about one or more ingest pipelines. This API returns a local reference of the pipeline.\\n\\n```\\nresp = client.ingest.get_pipeline(\\n    id=\\"my-pipeline-id\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.get_pipeline(\\n  id: \'my-pipeline-id\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.getPipeline({\\n  id: \\"my-pipeline-id\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nGET /_ingest/pipeline/my-pipeline-id\\n```\\n\\n### Request\\n\\n`GET /_ingest/pipeline/<pipeline>`\\n\\n`GET /_ingest/pipeline`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `read_pipeline`, `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to use this API.\\n\\n### Path parameters\\n\\n* `<pipeline>`\\n\\n  (Optional, string) Comma-separated list of pipeline IDs to retrieve. Wildcard (`*`) expressions are supported.\\n\\n  To get all ingest pipelines, omit this parameter or use `*`.\\n\\n### Query parameters\\n\\n* `master_timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n### Examples\\n\\n#### Get information for a specific ingest pipeline\\n\\n``... <truncated>',
          originalTokenCount: 659,
          truncatedTokenCount: 400,
        },
        llmScore: 4,
      },
      {
        selected: false,
        document: {
          author: {
            login: 'clintongormley',
          },
          issue_comments: [
            {
              author: {
                login: 'clintongormley',
              },
              body: "I've changed my mind about index templates (and updated the issue description).  We should use the same pattern as with ingest pipelines and have a package level template which is installed with the package and which can be upgraded by us, then create a label'ed version which the user can customise.",
            },
            {
              author: {
                login: 'ruflin',
              },
              body: '* Templates: Just to make sure we are on the same page: If the user wants to add his own mapping, he will do it in the `{type}-{label}-{package}-{input}` template and we rely on template inheritance to get the full mapping?\r\n* Ingest Pipeline: The order of execution is: `logs-prod-mysql-access -> logs-mysql-access -> global`. The user could modify his own pipeline that he ends up with `logs-prod-mysql-access -> global`. I think this is good for the nginx use case. Lets assume the user uses a custom log format. If he processes it in his ingest pipeline, "package" pipeline will not see the fields it wants to see as it is already processed. So we should allow the user to opt-out of it. The part I worry is that the user then might create a different data structure but that is then basically a user configuration issue.\r\n* Breaking changes for pipelines: Are you referring here to breaking changes in Elasticsearch related to pipelines or breaking changes in packages?',
            },
            {
              author: {
                login: 'ruflin',
              },
              body: 'Lets assume a user has `mysql` and `nginx` in production. He wants to monitor the services + the system of these servers. For `mysql` he would like to collect the `disk` info every 10 seconds as it is more critical and for `nginx` every 5 minutes. For this he sets up two system data sources and calls it `system-nginx` and `system-mysql`. Both data sources are assignedto  the `prod` label which then means the disk info goes into the index `metrics-prod-mysql-disk`. All good so far as the user wants to look at all the data together in one dashboard. What the user can\'t do with the above is say I want to keep the disk info from `mysql` around longer than from `nginx` and do rollups separately. To do this, the user has to use two different labels, `prod-mysql` and `prod-nginx` and assign the data sources separately. \r\n\r\nThe reason I wrote down the above is that so far my thinking was that an ILM policy is attached to an input in a data source "template". But that concept does not work anymore if 2 data source templates with different ILM policies configured are assigned to the same policy. On assignment, one of the two must win as there can be only one. At least two options here:\r\n\r\n* We allow the user to still configure ILM policy on the data source template and notify the user if there is a conflict (first one wins\r\n* We apply the default ILM policy and only after a label is assigned and the data source is created, we allow the user to change it.',
            },
            {
              author: {
                login: 'clintongormley',
              },
              body: '> Templates: Just to make sure we are on the same page: If the user wants to add his own mapping, he will do it in the {type}-{label}-{package}-{input} template and we rely on template inheritance to get the full mapping?\r\n\r\nCorrect\r\n\r\n> Ingest Pipeline: The order of execution is: logs-prod-mysql-access -> logs-mysql-access -> global. The user could modify his own pipeline that he ends up with logs-prod-mysql-access -> global. I think this is good for the nginx use case. Lets assume the user uses a custom log format. If he processes it in his ingest pipeline, "package" pipeline will not see the fields it wants to see as it is already processed. So we should allow the user to opt-out of it. The part I worry is that the user then might create a different data structure but that is then basically a user configuration issue.\r\n\r\nAgreed. \r\n\r\n> Breaking changes for pipelines: Are you referring here to breaking changes in Elasticsearch related to pipelines or breaking changes in packages?\r\n\r\nI was thinking about breaking changes in packages, but I suppose it would apply to Elasticsearch as well.',
            },
            {
              author: {
                login: 'clintongormley',
              },
              body: '> But that concept does not work anymore if 2 data source templates with different ILM policies configured are assigned to the same policy. On assignment, one of the two must win as there can be only one.\r\n\r\nGah! :(\r\n\r\nPerhaps we should use `{type}-{label}-{datasource}-{input}` instead (ie use `{datasource}` name instead of `{package}` name.  This was a problem before as we wanted the following to work:\r\n- `mysql-west` on port 2000 \r\n- `mysql-east` on port 2001\r\n- both log into `*-mysql-*` as the fields are all the same\r\n\r\nHowever, with the proposed `constant_keyword` field https://github.com/elastic/elasticsearch/pull/49713, both `mysql-west` and `mysql-east` would have the "technology" field (whatever the name is) set to `mysql`, so we could query both of those indices at the same time.  I think that would work.  \r\n\r\nDoes that break anything you can think of?\r\n\r\n/cc @ruflin @roncohen ',
            },
            {
              author: {
                login: 'ruflin',
              },
              body: "From a purely indexing perspective we need two things:\r\n\r\n* Unique name for the data structure we index the data for\r\n* Unique identifier to separate the indices of the same data structure per label per data source.\r\n\r\nWe have a unique identifier for the data structure already today and it is called `dataset` or `event.dataset` in ECS terms. This is for example `nginx.access` for the access logs.\r\n\r\nWith this the above becomes `{type}-{label}-{datasource}-{dataset}`. \r\n\r\n## Dataset \r\n\r\nIt bothered me for a while that packages was part of the index name and I think is beneficial to replace it with the above. Let me give some examples:\r\n\r\nThe dataset uniquely describes the data and no the combination of package + input. This is important because for `auditd` we have a special case already today. There aren't multiple inputs and the data should flow into `logs-{label}-{datasource}-auditd` and not `logs-{label}-.{datasource}-auditd.auditd`. The `event.dataset` value here is `auditd`. \r\n\r\nIf the dataset defines the index name, it also means we can repackage packages in different ways. Lets take stack monitoring: There are at least 4 packages: ES, KB, LS, Beats, ... We could repackage these for as a  Stack-Monitoring package and still have the 4 packages. If someone now sets up the ES package or the Stack-Monitoring package, the data would still flow into the same indices but separated by the unique identifiers. If now someone looks at ES dashboards, he could see all data that matches also from other packages.\r\n\r\nBreaking changes happen on the metrics side much more often then on the logging side, at least that is so far my experience. In the case of nginx, if the metrics change, at the moment we have to ship a new package. An alternative here is to just add a new data set. Lets assume `nginx.status` metrics had a breaking change. We now also ship `nginx.status2` wit the same package. The user upgrades and can choose which one he wants to use for his version but still can use the same package for the `nginx.access` and `nginx.error` logs and don't have to go to `nginx2.access` for all the things.\r\n\r\n## Deletion \r\n\r\nOne thing that bothered me in the past was how a user could delete all the data for a package manually. The `package` would be a field in each event. The user then can use a `deleteByQuery` which normally is pretty inefficient. But now with `constant_keyword` this could be optimized. If package is a `constant_keyword` the deleteByQuery would only have to look at the index mapping  for the field and if it matches, it knows all other documents will match too, so the index can be dropped directly.",
            },
            {
              author: {
                login: 'clintongormley',
              },
              body: 'Superseded by https://github.com/elastic/observability-dev/issues/499',
            },
          ],
          title:
            'RFC: Assets - dashboards, index templates, ingest pipelines, ILM policies, ML Jobs',
          body: "**Superseded by https://github.com/elastic/observability-dev/issues/499** Parent issue https://github.com/elastic/observability-dev/issues/440 Each package (eg MySQL, Nginx, etc) contains a number of assets, eg visualisations, dashboards, a template for an index templates, a template for an ingest pipeline, etc. Some of these assets are installed globally (eg ILM policies), some once when the package is installed (eg dashboard), while others are installed once per label (eg index templates and ingest pipeline). Assets which require a label name (eg `prod`) can only be installed when a policy is defined, and one should be installed per label. Note: labels may be shared by multiple policies. For instance, there may be a policy for `prod_east` and `prod_west`, both of which use the label `prod`. MySQL data from each policy would go into the same `*-prod-mysql-*` indices. # ILM Policies We will install two ILM policies at startup if they don't already exist - one for `logs` and one for `metrics`. Data streams will use the appropriate policy by default - the user can go in and edit the policy, or they can add and use their own policies. # Dashboard and visualisations Dashboards and their visualisations should be installed into Kibana when the package is installed. Visualisations are not shared - they are used only by the dashboard(s) in the package. The same dashboards are used for all instances of the package. For instance, if mysql is being monitored once in `prod`, once in `dev`, and once in `staging`, then there would be only one dashboard which features three labels `prod`, `dev`, and `staging`. The user can choose to see data for all mysql instances, or from one or more labels. Dashboards and their visualisations are removed from Kibana when a package is uninstalled. Dashboards can be named after the package name, eg `MySQL-Main`, `MySQL-Foo` # Index templates ~~While index templates in Elasticsearch support inheritance, it will be easier from the user's point of view to maintain one index template per label. That way, a user can view a single index template and see all fields that are defined, without having to guess about which templates might be matching. They can also change a single file without fear of impacting other data sources.~~ We should have an index template per package/data-stream which is installed when the package is installed, and would be named `{type}-{package}-{input}` eg `logs-mysql-access`. This should be considered read only by the user, and we can overwrite this template with a new version whenever we want. All required fields would be included in the template. It is possible that a particular technology will be used in kubernetes or docker or cloud, or perhaps all of the above, in which case we'd want the metadata from each of these sources to be included. All of these metadata fields are of type `keyword`, so we can add a dynamic rule which says to map them as `keyword` only. Only exception to this is `host.containerized` which is `boolean` and can be added as a standard field. Then, when a data source is applied to a policy (and has a label), we create an index template called `{type}-{label}-{package}-{input}` eg `logs-prod-mysql-access`, which just contains eg the ILM policy, the number of primary shards, etc. This template is owned by the user and can be customised by them. We will never overwrite it. # Ingest pipelines Docs may need to be passed through multiple ingest pipelines before ingestion. There will be the standard ingest pipeline for the technology being processed. It is quite likely that a user might add their own custom parsing. Other solutions may want to enrich documents, eg SIEM or machine learning. Additionally, we may need pipelines which deal with upgrading old versions of data to current. There are three ways of specifying that a pipeline should be applied - in the request, as a `default_pipeline` (which is run unless one is specified in the request), or as a `final_pipeline` (which is always run). ## Global ingest pipeline There should be a single global ingest pipeline which solutions can hook into to ensure that their processing is performed, eg the security processor which writes the username or API key UID into the document, SIEM enrichment, ML enrichment, etc. Of course, SIEM or ML may not be installed, so some application (probably EPM/Ingest Manager) needs to be in charge of assembling this global pipeline and writing it whenever the app configuration changes. All index templates will specify that the `final_pipeline` is this global, required, pipeline. ## Package-level data source pipelines Each data source in a package contains an ingest pipeline for parsing that data source, eg the MySQL package will contain three ingest pipelines: `logs-mysql-access`, `logs-mysql-error`, and `metrics-mysql-status`. These pipelines should be considered by the user to be read only and can be upgraded by the system whenever needed. These are installed when the package is installed into Kibana. ## Per-label pipelines When a data source is applied to a policy, we create a pipeline which includes the label name, eg `logs-prod-mysql-access`. This is the custom pipeline that the user is allowed to edit, to add their own parsing. The only thing it does by default is call the package-level data source pipeline (ie it calls `logs-mysql-access`). **Question**: Is there a problem in always assuming that we will never need to upgrade the per-label pipeline? My main concern here is how we handle breaking changes between versions. # Machine Learning Jobs It is not useful to start ML jobs on installation as there is no data to process and early data may not be representative of a stable state. Also, we should ask before enabling ML jobs as they can be resource intensive. The ML module (today hardcoded in the ML app), could be installed as part of the package. Apart from the ML jobs that should be run, the module would also contain a query which can determine whether any new data sources have been added to the system. If new sources are detected, the ML app could popup an alert to the user, asking them if they want to enable these jobs.",
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/484',
          number: 484,
          createdAt: '2019-11-28T17:34:06Z',
          assignees_list: [],
          labels_field: [],
          state: 'CLOSED',
          id: 'MDU6SXNzdWU1MzAwMjY2MjE=',
          closedAt: '2019-12-09T17:36:20Z',
          _timestamp: '2019-12-09T17:36:28Z',
        },
        id: 'search-observability-dev/MDU6SXNzdWU1MzAwMjY2MjE=',
        title: 'RFC: Assets - dashboards, index templates, ingest pipelines, ILM policies, ML Jobs',
        score: 136.58463,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"clintongormley"},"issue_comments":[{"author":{"login":"clintongormley"},"body":"I\'ve changed my mind about index templates (and updated the issue description).  We should use the same pattern as with ingest pipelines and have a package level template which is installed with the package and which can be upgraded by us, then create a label\'ed version which the user can customise."},{"author":{"login":"ruflin"},"body":"* Templates: Just to make sure we are on the same page: If the user wants to add his own mapping, he will do it in the `{type}-{label}-{package}-{input}` template and we rely on template inheritance to get the full mapping?\\r\\n* Ingest Pipeline: The order of execution is: `logs-prod-mysql-access -> logs-mysql-access -> global`. The user could modify his own pipeline that he ends up with `logs-prod-mysql-access -> global`. I think this is good for the nginx use case. Lets assume the user uses a custom log format. If he processes it in his ingest pipeline, \\"package\\" pipeline will not see the fields it wants to see as it is already processed. So we should allow the user to opt-out of it. The part I worry is that the user then might create a different data structure but that is then basically a user configuration issue.\\r\\n* Breaking changes for pipelines: Are you referring here to breaking changes in Elasticsearch related to pipelines or breaking changes in packages?"},{"author":{"login":"ruflin"},"body":"Lets assume a user has `mysql` and `nginx` in production. He wants to monitor the services + the system of these servers. For `mysql` he would like to collect the `disk` info every 10 seconds as it is more critical and for `nginx` every 5 minutes. For this he sets up two system data sources and calls it `system-nginx` and `system-mysql`. Both data sources are assignedto  the `prod` label which then means the disk info goes into the index `metrics-prod-mysql-disk`. All good so far as the user wants to look at all the data together in one dashboard. What the user can\'t do with the above is say I want to keep the disk info from `mysql` around longer than from `nginx` and do rollups separately. To do this, the user has to use two different labels, `prod-mysql` and `prod-nginx` and assign the data sources separately. \\r\\n\\r\\nThe reason I wrote down the above is that so far my thinking was that an ILM policy is attached to an input in a data source \\"template\\". But that concept does not work anymore if 2 data source templates with different ILM policies configured are assigned to the same policy. On assignment, one of the two must win as there can be only one. At least two options here:\\r\\n\\r\\n* We allow the user to still configure ILM policy on the data source template and notify the user if there is a conflict (first one wins\\r\\n* We apply the default ILM policy and only after a label is assigned and the data source is created, we allow the user to change it."},{"author":{"login":"clintongormley"},"body":"> Templates: Just to make sure we are on the same page: If the user wants to add his own mapping, he will do it in the {type}-{label}-{package}-{input} template and we rely on template inheritance to get the full mapping?\\r\\n\\r\\nCorrect\\r\\n\\r\\n> Ingest Pipeline: The order of execution is: logs-prod-mysql-access -> logs-mysql-access -> global. The user could modify his own pipeline that he ends up with logs-prod-mysql-access -> global. I think this is good for the nginx use case. Lets assume the user uses a custom log format. If he processes it in his ingest pipeline, \\"package\\" pipeline will not see the fields it wants to see as it is already processed. So we should allow the user to opt-out of it. The part I worry is that the user then might create a different data structure but that is then basically a user configuration issue.\\r\\n\\r\\nAgreed. \\r\\n\\r\\n> Breaking changes for pipelines: Are you referring here to breaking changes in Elasticsearch related to pipelines or breaking changes in packages?\\r\\n\\r\\nI was thinking about breaking changes in packages, but I suppose it would apply to Elasticsearch as well."},{"author":{"login":"clintongormley"},"body":"> But that concept does not work anymore if 2 data source templates with different ILM policies configured are assigned to the same policy. On assignment, one of the two must win as there can be only one.\\r\\n\\r\\nGah! :(\\r\\n\\r\\nPerhaps we should use `{type}-{label}-{datasource}-{input}` instead (ie use `{datasource}` name instead of `{package}` name.  This was a problem before as we wanted the following to work:\\r\\n- `mysql-west` on port 2000 \\r\\n- `mysql-east` on port 2001\\r\\n- both log into `*-mysql-*` as the fields are all the same\\r\\n\\r\\nHowever, with the proposed `constant_keyword` field https://github.com/elastic/elasticsearch/pull/49713, both `mysql-west` and `mysql-east` would have the \\"technology\\" field (whatever the name is) set to `mysql`, so we could query both of those indices at the same time.  I think that would work.  \\r\\n\\r\\nDoes that break anything you can think of?\\r\\n\\r\\n/cc @ruflin @roncohen "},{"author":{"login":"ruflin"},"body":"From a purely indexing perspective we need two things:\\r\\n\\r\\n* Unique name for the data structure we index the data for\\r\\n* Unique identifier to separate the indices of the same data structure per label per data source.\\r\\n\\r\\nWe have a unique identifier for the data structure already today and it is called `dataset` or `event.dataset` in ECS terms. This is for example `nginx.access` for the access logs.\\r\\n\\r\\nWith this the above becomes `{type}-{label}-{datasource}-{dataset}`. \\r\\n\\r\\n## Dataset \\r\\n\\r\\nIt bothered me for a while that packages was part of the index name and I think is beneficial to replace it with the above. Let me give some examples:\\r\\n\\r\\nThe dataset uniquely describes the data and no the combination of package + input. This is important because for `auditd` we have a special case already today. There aren\'t multiple inputs and the data should flow into `logs-{label}-{datasource}-auditd` and not `logs-{label}-.{datasource}-auditd.auditd`. The `event.dataset` value here is `auditd`. \\r\\n\\r\\nIf the dataset defines the index name, it also means we can repackage packages in different ways. Lets take stack monitoring: There are at least 4 packages: ES, KB, LS, Beats, ... We could repackage these for as a  Stack-Monitoring package and still have the 4 packages. If someone now sets up the ES package or the Stack-Monitoring package, the data would still flow into the same indices but separated by the unique identifiers. If now someone looks at ES dashboards, he could see all data that matches also from other packages.\\r\\n\\r\\nBreaking changes happen on the metrics side much more often then on the logging side, at least that is so far my experience. In the case of nginx, if the metrics change, at the moment we have to ship a new package. An alternative here is to just add a new data set. Lets assume `nginx.status` metrics had a breaking change. We now also ship `nginx.status2` wit the same package. The user upgrades and can choose which one he wants to use for his version but still can use the same package for the `nginx.access` and `nginx.error` logs and don\'t have to go to `nginx2.access` for all the things.\\r\\n\\r\\n## Deletion \\r\\n\\r\\nOne thing that bothered me in the past was how a user could delete all the data for a package manually. The `package` would be a field in each event. The user then can use a `deleteByQuery` which normally is pretty inefficient. But now with `constant_keyword` this could be optimized. If package is a `constant_keyword` the deleteByQuery would only have to look at the index mapping  for the field and if it matches, it knows all other documents will match too, so the index can be dropped directly."},{"author":{"login":"clintongormley"},"body":"Superseded by https://github.com/elastic/observability-dev/issues/499"}],"title":"RFC: Assets - dashboards, index templates, ingest pipelines, ILM policies, ML Jobs","body":"**Superseded by https://github.com/elastic/observability-dev/issues/499** Parent issue https://github.com/elastic/observability-dev/issues/440 Each package (eg MySQL, Nginx, etc) contains a number of assets, eg visualisations, dashboards, a template for an index templates, a template for an ingest pipeline, etc. Some of these assets are installed globally (eg ILM policies), some once when the package is installed (eg dashboard), while others are installed once per label (eg index templates and ingest pipeline). Assets which require a label name (eg `prod`) can only be installed when a policy is defined, and one should be installed per label. Note: labels may be shared by multiple policies. For instance, there may be a policy for `prod_east` and `prod_west`, both of which use the label `prod`. MySQL data from each policy would go into the same `*-prod-mysql-*` indices. # ILM Policies We will install two ILM policies at startup if they don\'t already exist - one for `logs` and one for `metrics`. Data streams will use the appropriate policy by default - the user can go in and edit the policy, or they can add and use their own policies. # Dashboard and visualisations Dashboards and their visualisations should be installed into Kibana when the package is installed. Visualisations are not shared - they are used only by the dashboard(s) in the package. The same dashboards are used for all instances of the package. For instance, if mysql is being monitored once in `prod`, once in `dev`, and once in `staging`, then there would be only one dashboard which features three labels `prod`, `dev`, and `staging`. The user can choose to see data for all mysql instances, or from one or more labels. Dashboards and their visualisations are removed from Kibana when a package is uninstalled. Dashboards can be named after the package name, eg `MySQL-Main`, `MySQL-Foo` # Index templates ~~While index templates in Elasticsearch support inheritance, it will be easier from the user\'s point of view to maintain one index template per label. That way, a user can view a single index template and see all fields that are defined, without having to guess about which templates might be matching. They can also change a single file without fear of impacting other data sources.~~ We should have an index template per package/data-stream which is installed when the package is installed, and would be named `{type}-{package}-{input}` eg `logs-mysql-access`. This should be considered read only by the user, and we can overwrite this template with a new version whenever we want. All required fields would be included in the template. It is possible that a particular technology will be used in kubernetes or docker or cloud, or perhaps all of the above, in which case we\'d want the metadata from each of these sources to be included. All of these metadata fields are of type `keyword`, so we can add a dynamic rule which says to map them as `keyword` only. Only exception to this is `host.containerized` which is `boolean` and can be added as a standard field. Then, when a data source is applied to a policy (and has a label), we create an index template called `{type}-{label}-{package}-{input}` eg `logs-prod-mysql-access`, which just contains eg the ILM policy, the number of primary shards, etc. This template is owned by the user and can be customised by them. We will never overwrite it. # Ingest pipelines Docs may need to be passed through multiple ingest pipelines before ingestion. There will be the standard ingest pipeline for the technology being processed. It is quite likely that a user might add their own custom parsing. Other solutions may want to enrich documents, eg SIEM or machine learning. Additionally, we may need pipelines which deal with upgrading old versions of data to current. There are three ways of specifying that a pipeline should be applied - in the request, as a `default_pipeline` (which is run unless one is specified in the request), or as a `final_pipeline` (which is always run). ## Global ingest pipeline There should be a single global ingest pipeline which solutions can hook into to ensure that their processing is performed, eg the security processor which writes the username or API key UID into the document, SIEM enrichment, ML enrichment, etc. Of course, SIEM or ML may not be installed, so some application (probably EPM/Ingest Manager) needs to be in charge of assembling this global pipeline and writing it whenever the app configuration changes. All index templates will specify that the `final_pipeline` is this global, required, pipeline. ## Package-level data source pipelines Each data source in a package contains an ingest pipeline for parsing that data source, eg the MySQL package will contain three ingest pipelines: `logs-mysql-access`, `logs-mysql-error`, and `metrics-mysql-status`. These pipelines should be considered by the user to be read only and can be upgraded by the system whenever needed. These are installed when the package is installed into Kibana. ## Per-label pipelines When a data source is applied to a policy, we create a pipeline which includes the label name, eg `logs-prod-mysql-access`. This is the custom pipeline that the user is allowed to edit, to add their own parsing. The only thing it does by default is call the package-level data source pipeline (ie it calls `logs-mysql-access`). **Question**: Is there a problem in always assuming that we will never need to upgrade the per-label pipeline? My main concern here is how we handle breaking changes between versions. # Machine Learning Jobs It is not useful to start ML jobs on installation as there is no data to process and early data may not be representative of a stable state. Also, we should ask before enabling ML jobs as they can be resource intensive. The ML module (today hardcoded in the ML app), could be installed as part of the package. Apart from the ML jobs that should be run, the module would also contain a query which can determine whether any new data sources have been added to the system. If new sources are detected, the ML app could popup an alert to the user, asking them if they want to enable these jobs.","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/484","number":484,"createdAt":"2019-11-28T17:34:06Z","assignees_list":[],"labels_field":[],"state":"CLOSED","id":"MDU6SXNzdWU1MzAwMjY2MjE=","closedAt":"2019-12-09T17:36:20Z","_timestamp":"2019-12-09T17:36:28Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"clintongormley"},"issue_comments":[{"author":{"login":"clintongormley"},"body":"I\'ve changed my mind about index templates (and updated the issue description).  We should use the same pattern as with ingest pipelines and have a package level template which is installed with the package and which can be upgraded by us, then create a label\'ed version which the user can customise."},{"author":{"login":"ruflin"},"body":"* Templates: Just to make sure we are on the same page: If the user wants to add his own mapping, he will do it in the `{type}-{label}-{package}-{input}` template and we rely on template inheritance to get the full mapping?\\r\\n* Ingest Pipeline: The order of execution is: `logs-prod-mysql-access -> logs-mysql-access -> global`. The user could modify his own pipeline that he ends up with `logs-prod-mysql-access -> global`. I think this is good for the nginx use case. Lets assume the user uses a custom log format. If he processes it in his ingest pipeline, \\"package\\" pipeline will not see the fields it wants to see as it is already processed. So we should allow the user to opt-out of it. The part I worry is that the user then might create a different data structure but that is then basically a user configuration issue.\\r\\n* Breaking changes for pipelines: Are you referring here to breaking changes in Elasticsearch related to pipelines or breaking changes in packages?"},{"author":{"login":"ruflin"},"body":"Lets assume a user has `mysql` and `nginx` in production. He wants to monitor the services + the system of these servers. For `mysql` he would like to collect the `disk` info every 10 seconds as it is more critical and for `nginx` every 5 minutes. For this he sets up two system data sources and calls it `system-nginx`... <truncated>',
          originalTokenCount: 3312,
          truncatedTokenCount: 400,
        },
        llmScore: 2,
      },
      {
        selected: false,
        document: {
          content:
            '## Using ingest processors in Painless\n\nSome [ingest processors](/guide/en/elasticsearch/reference/8.17/processors.html) expose behavior through Painless methods that can be called in Painless scripts that execute in ingest pipelines.\n\n### Method usage\n\nAll ingest methods available in Painless are scoped to the `Processors` namespace. For example:\n\n```\nPOST /_ingest/pipeline/_simulate?verbose\n{\n  "pipeline": {\n    "processors": [\n      {\n        "script": {\n          "lang": "painless",\n          "source": """\n            long bytes = Processors.bytes(ctx.size);\n            ctx.size_in_bytes = bytes;\n          """\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_source": {\n        "size": "1kb"\n      }\n    }\n  ]\n}\n```\n\n### Ingest methods reference\n\n#### Byte conversion\n\nUse the [bytes processor](/guide/en/elasticsearch/reference/8.17/bytes-processor.html) to return the number of bytes in the human-readable byte value supplied in the `value` parameter.\n\n```\nlong bytes(String value);\n```\n\n#### Lowercase conversion\n\nUse the [lowercase processor](/guide/en/elasticsearch/reference/8.17/lowercase-processor.html) to convert the supplied string in the `value` parameter to its lowercase equivalent.\n\n```\nString lowercase(String value);\n```\n\n#### Uppercase conversion\n\nUse the [uppercase processor](/guide/en/elasticsearch/reference/8.17/uppercase-processor.html) to convert the supplied string in the `value` parameter to its uppercase equivalent.\n\n```\nString uppercase(String value);\n```\n\n#### JSON parsing\n\nUse the [JSON processor](/guide/en/elasticsearch/reference/8.17/json-processor.html) to convert JSON strings to structured JSON objects. The first `json` method accepts a map and a key. The processor converts the JSON string in the map as specified by the `key` parameter to structured JSON content. That content is added directly to the `map` object.\n\nThe second `json` method accepts a JSON string in the `value` parameter and returns a structured JSON object.\n\n```\nvoid json(Map<String, Object> map, String key);\nObject json(Object value);\n```\n\nYou can then add this object to the document through the context object:\n\n```\nObject json = Processors.json(ctx.inputJsonString);\nctx.structuredJson = json;\n```\n\n#### URL decoding\n\nUse the [URL decode processor](/guide/en/elasticsearch/reference/8.17/urldecode-processor.html) to URL-decode the string supplied in the `value` parameter.\n\n```\nString urlDecode(String value);\n```\n\n#### URI decomposition\n\nUse the [URI parts processor](/guide/en/elasticsearch/reference/8.17/uri-parts-processor.html) to decompose the URI string supplied in the `value` parameter. Returns a map of key-value pairs in which the key is the name of the URI component such as `domain` or `path` and the value is the corresponding value for that component.\n\n```\nString uriParts(String value);\n```\n\n#### Network community ID\n\nUse the [community ID processor](/guide/en/elasticsearch/reference/8.17/community-id-processor.html) to compute the network community ID for network flow data.\n\n```\nString communityId(String sourceIpAddrString, String destIpAddrString, Object ianaNumber, Object transport, Object sourcePort, Object destinationPort, Object icmpType, Object icmpCode, int seed)\nString communityId(String sourceIpAddrString, String destIpAddrString, Object ianaNumber, Object transport, Object sourcePort, Object destinationPort, Object icmpType, Object icmpCode)\n```\n',
          title: 'Using ingest processors in Painless',
          url: 'https://www.elastic.co/guide/en/elasticsearch/painless/8.17/painless-ingest.html',
          productName: 'elasticsearch',
          score: 136.43681,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/painless/8.17/painless-ingest.html',
        title: 'Using ingest processors in Painless',
        score: 136.43681,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Using ingest processors in Painless\\n\\nSome [ingest processors](/guide/en/elasticsearch/reference/8.17/processors.html) expose behavior through Painless methods that can be called in Painless scripts that execute in ingest pipelines.\\n\\n### Method usage\\n\\nAll ingest methods available in Painless are scoped to the `Processors` namespace. For example:\\n\\n```\\nPOST /_ingest/pipeline/_simulate?verbose\\n{\\n  \\"pipeline\\": {\\n    \\"processors\\": [\\n      {\\n        \\"script\\": {\\n          \\"lang\\": \\"painless\\",\\n          \\"source\\": \\"\\"\\"\\n            long bytes = Processors.bytes(ctx.size);\\n            ctx.size_in_bytes = bytes;\\n          \\"\\"\\"\\n        }\\n      }\\n    ]\\n  },\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"size\\": \\"1kb\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Ingest methods reference\\n\\n#### Byte conversion\\n\\nUse the [bytes processor](/guide/en/elasticsearch/reference/8.17/bytes-processor.html) to return the number of bytes in the human-readable byte value supplied in the `value` parameter.\\n\\n```\\nlong bytes(String value);\\n```\\n\\n#### Lowercase conversion\\n\\nUse the [lowercase processor](/guide/en/elasticsearch/reference/8.17/lowercase-processor.html) to convert the supplied string in the `value` parameter to its lowercase equivalent.\\n\\n```\\nString lowercase(String value);\\n```\\n\\n#### Uppercase conversion\\n\\nUse the [uppercase processor](/guide/en/elasticsearch/reference/8.17/uppercase-processor.html) to convert the supplied string in the `value` parameter to its uppercase equivalent.\\n\\n```\\nString uppercase(String value);\\n```\\n\\n#### JSON parsing\\n\\nUse the [JSON processor](/guide/en/elasticsearch/reference/8.17/json-processor.html) to convert JSON strings to structured JSON objects. The first `json` method accepts a map and a key. The processor converts the JSON string in the map as specified by the `key` parameter to structured JSON content. That content is added directly to the `map` object.\\n\\nThe second `json` method accepts a JSON string in the `value` parameter and returns a structured JSON object.\\n\\n```\\nvoid json(Map<String, Object> map, String key);\\nObject json(Object value);\\n```\\n\\nYou can then add this object to the document through the context object:\\n\\n```\\nObject json = Processors.json(ctx.inputJsonString);\\nctx.structuredJson = json;\\n```\\n\\n#### URL decoding\\n\\nUse the [URL decode processor](/guide/en/elasticsearch/reference/8.17/urldecode-processor.html) to URL-decode the string supplied in the `value` parameter.\\n\\n```\\nString urlDecode(String value);\\n```\\n\\n#### URI decomposition\\n\\nUse the [URI parts processor](/guide/en/elasticsearch/reference/8.17/uri-parts-processor.html) to decompose the URI string supplied in the `value` parameter. Returns a map of key-value pairs in which the key is the name of the URI component such as `domain` or `path` and the value is the corresponding value for that component.\\n\\n```\\nString uriParts(String value);\\n```\\n\\n#### Network community ID\\n\\nUse the [community ID processor](/guide/en/elasticsearch/reference/8.17/community-id-processor.html) to compute the network community ID for network flow data.\\n\\n```\\nString communityId(String sourceIpAddrString, String destIpAddrString, Object ianaNumber, Object transport, Object sourcePort, Object destinationPort, Object icmpType, Object icmpCode, int seed)\\nString communityId(String sourceIpAddrString, String destIpAddrString, Object ianaNumber, Object transport, Object sourcePort, Object destinationPort, Object icmpType, Object icmpCode)\\n```\\n","title":"Using ingest processors in Painless","url":"https://www.elastic.co/guide/en/elasticsearch/painless/8.17/painless-ingest.html","productName":"elasticsearch","score":136.43681}',
        truncated: {
          truncatedText:
            '{"content":"## Using ingest processors in Painless\\n\\nSome [ingest processors](/guide/en/elasticsearch/reference/8.17/processors.html) expose behavior through Painless methods that can be called in Painless scripts that execute in ingest pipelines.\\n\\n### Method usage\\n\\nAll ingest methods available in Painless are scoped to the `Processors` namespace. For example:\\n\\n```\\nPOST /_ingest/pipeline/_simulate?verbose\\n{\\n  \\"pipeline\\": {\\n    \\"processors\\": [\\n      {\\n        \\"script\\": {\\n          \\"lang\\": \\"painless\\",\\n          \\"source\\": \\"\\"\\"\\n            long bytes = Processors.bytes(ctx.size);\\n            ctx.size_in_bytes = bytes;\\n          \\"\\"\\"\\n        }\\n      }\\n    ]\\n  },\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"size\\": \\"1kb\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Ingest methods reference\\n\\n#### Byte conversion\\n\\nUse the [bytes processor](/guide/en/elasticsearch/reference/8.17/bytes-processor.html) to return the number of bytes in the human-readable byte value supplied in the `value` parameter.\\n\\n```\\nlong bytes(String value);\\n```\\n\\n#### Lowercase conversion\\n\\nUse the [lowercase processor](/guide/en/elasticsearch/reference/8.17/lowercase-processor.html) to convert the supplied string in the `value` parameter to its lowercase equivalent.\\n\\n```\\nString lowercase(String value);\\n```\\n\\n#### Uppercase conversion\\n\\nUse the [uppercase processor](/guide/en/elasticsearch/reference/8.17/uppercase-processor.html) to convert the supplied string in the `value` parameter to its uppercase equivalent.\\n\\n```\\nString uppercase(String... <truncated>',
          originalTokenCount: 918,
          truncatedTokenCount: 400,
        },
        llmScore: 3,
      },
      {
        selected: false,
        document: {
          content:
            '## Simulate pipeline API\n\nExecutes an ingest pipeline against a set of provided documents.\n\n```\nresp = client.ingest.simulate(\n    id="my-pipeline-id",\n    docs=[\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "bar"\n            }\n        },\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "rab"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nconst response = await client.ingest.simulate({\n  id: "my-pipeline-id",\n  docs: [\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "bar",\n      },\n    },\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "rab",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/pipeline/my-pipeline-id/_simulate\n{\n  "docs": [\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ]\n}\n```\n\n### Request\n\n`POST /_ingest/pipeline/<pipeline>/_simulate`\n\n`GET /_ingest/pipeline/<pipeline>/_simulate`\n\n`POST /_ingest/pipeline/_simulate`\n\n`GET /_ingest/pipeline/_simulate`\n\n### Prerequisites\n\n* If the Elasticsearch security features are enabled, you must have the `read_pipeline`, `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to use this API.\n\n### Description\n\nThe simulate pipeline API executes a specific pipeline against a set of documents provided in the body of the request.\n\nYou can either specify an existing pipeline to execute against the provided documents or supply a pipeline definition in the body of the request.\n\n### Path parameters\n\n* `<pipeline>`\n\n  (Required\\*, string) Pipeline to test. If you don’t specify a `pipeline` in the request body, this parameter is required.\n\n### Query parameters\n\n* `verbose`\n\n  (Optional, Boolean) If `true`, the response includes output data for each processor in the executed pipeline.\n\n### Request body\n\n* `pipeline`\n\n  (Required\\*, object) Pipeline to test. If you don’t specify the `<pipeline>` request path parameter, this parameter is required. If you specify both this and the request path parameter, the API only uses the request path parameter.\n\n  Properties of `pipeline`\n\n  * `description`\n\n    (Optional, string) Description of the ingest pipeline.\n\n  * `on_failure`\n\n    (Optional, array of [processor](processors.html "Ingest processor reference") objects) Processors to run immediately after a processor failure.\n\n    Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\n\n  * `processors`\n\n    (Required, array of [processor](processors.html "Ingest processor reference") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\n\n  * `version`\n\n    (Optional, integer) Version number used by external systems to track ingest pipelines.\n\n    See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params "Query parameters") parameter above for how the version attribute is used.\n\n  * `_meta`\n\n    (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\n\n  * `deprecated`\n\n    (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\n\n* `docs`\n\n  (Required, array of objects) Sample documents to test in the pipeline.\n\n  Properties of `docs` objects\n\n  * `_id`\n\n    (Optional, string) Unique identifier for the document. This ID must be unique within the `_index`.\n\n  * `_index`\n\n    (Optional, string) Name of the index containing the document.\n\n  * `_routing`\n\n    (Optional, string) Value used to send the document to a specific primary shard. See the [`_routing`](mapping-routing-field.html "_routing field") field.\n\n  * `_source`\n\n    (Required, object) JSON body for the document.\n\n### Examples\n\n#### Specify a pipeline as a path parameter\n\n```\nresp = client.ingest.simulate(\n    id="my-pipeline-id",\n    docs=[\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "bar"\n            }\n        },\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "rab"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nconst response = await client.ingest.simulate({\n  id: "my-pipeline-id",\n  docs: [\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "bar",\n      },\n    },\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "rab",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/pipeline/my-pipeline-id/_simulate\n{\n  "docs": [\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ]\n}\n```\n\nThe API returns the following response:\n\n```\n{\n   "docs": [\n      {\n         "doc": {\n            "_id": "id",\n            "_index": "index",\n            "_version": "-3",\n            "_source": {\n               "field2": "_value",\n               "foo": "bar"\n            },\n            "_ingest": {\n               "timestamp": "2017-05-04T22:30:03.187Z"\n            }\n         }\n      },\n      {\n         "doc": {\n            "_id": "id",\n            "_index": "index",\n            "_version": "-3",\n            "_source": {\n               "field2": "_value",\n               "foo": "rab"\n            },\n            "_ingest": {\n               "timestamp": "2017-05-04T22:30:03.188Z"\n            }\n         }\n      }\n   ]\n}\n```\n\n#### Specify a pipeline in the request body\n\n```\nresp = client.ingest.simulate(\n    pipeline={\n        "description": "_description",\n        "processors": [\n            {\n                "set": {\n                    "field": "field2",\n                    "value": "_value"\n                }\n            }\n        ]\n    },\n    docs=[\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "bar"\n            }\n        },\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "rab"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.simulate(\n  body: {\n    pipeline: {\n      description: \'_description\',\n      processors: [\n        {\n          set: {\n            field: \'field2\',\n            value: \'_value\'\n          }\n        }\n      ]\n    },\n    docs: [\n      {\n        _index: \'index\',\n        _id: \'id\',\n        _source: {\n          foo: \'bar\'\n        }\n      },\n      {\n        _index: \'index\',\n        _id: \'id\',\n        _source: {\n          foo: \'rab\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.simulate({\n  pipeline: {\n    description: "_description",\n    processors: [\n      {\n        set: {\n          field: "field2",\n          value: "_value",\n        },\n      },\n    ],\n  },\n  docs: [\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "bar",\n      },\n    },\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "rab",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/pipeline/_simulate\n{\n  "pipeline" :\n  {\n    "description": "_description",\n    "processors": [\n      {\n        "set" : {\n          "field" : "field2",\n          "value" : "_value"\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ]\n}\n```\n\nThe API returns the following response:\n\n```\n{\n   "docs": [\n      {\n         "doc": {\n            "_id": "id",\n            "_index": "index",\n            "_version": "-3",\n            "_source": {\n               "field2": "_value",\n               "foo": "bar"\n            },\n            "_ingest": {\n               "timestamp": "2017-05-04T22:30:03.187Z"\n            }\n         }\n      },\n      {\n         "doc": {\n            "_id": "id",\n            "_index": "index",\n            "_version": "-3",\n            "_source": {\n               "field2": "_value",\n               "foo": "rab"\n            },\n            "_ingest": {\n               "timestamp": "2017-05-04T22:30:03.188Z"\n            }\n         }\n      }\n   ]\n}\n```\n\n#### View verbose results\n\nYou can use the simulate pipeline API to see how each processor affects the ingest document as it passes through the pipeline. To see the intermediate results of each processor in the simulate request, you can add the `verbose` parameter to the request.\n\n```\nresp = client.ingest.simulate(\n    verbose=True,\n    pipeline={\n        "description": "_description",\n        "processors": [\n            {\n                "set": {\n                    "field": "field2",\n                    "value": "_value2"\n                }\n            },\n            {\n                "set": {\n                    "field": "field3",\n                    "value": "_value3"\n                }\n            }\n        ]\n    },\n    docs=[\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "bar"\n            }\n        },\n        {\n            "_index": "index",\n            "_id": "id",\n            "_source": {\n                "foo": "rab"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.simulate(\n  verbose: true,\n  body: {\n    pipeline: {\n      description: \'_description\',\n      processors: [\n        {\n          set: {\n            field: \'field2\',\n            value: \'_value2\'\n          }\n        },\n        {\n          set: {\n            field: \'field3\',\n            value: \'_value3\'\n          }\n        }\n      ]\n    },\n    docs: [\n      {\n        _index: \'index\',\n        _id: \'id\',\n        _source: {\n          foo: \'bar\'\n        }\n      },\n      {\n        _index: \'index\',\n        _id: \'id\',\n        _source: {\n          foo: \'rab\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.simulate({\n  verbose: "true",\n  pipeline: {\n    description: "_description",\n    processors: [\n      {\n        set: {\n          field: "field2",\n          value: "_value2",\n        },\n      },\n      {\n        set: {\n          field: "field3",\n          value: "_value3",\n        },\n      },\n    ],\n  },\n  docs: [\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "bar",\n      },\n    },\n    {\n      _index: "index",\n      _id: "id",\n      _source: {\n        foo: "rab",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/pipeline/_simulate?verbose=true\n{\n  "pipeline" :\n  {\n    "description": "_description",\n    "processors": [\n      {\n        "set" : {\n          "field" : "field2",\n          "value" : "_value2"\n        }\n      },\n      {\n        "set" : {\n          "field" : "field3",\n          "value" : "_value3"\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "index",\n      "_id": "id",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ]\n}\n```\n\nThe API returns the following response:\n\n```\n{\n  "docs" : [\n    {\n      "processor_results" : [\n        {\n          "processor_type" : "set",\n          "status" : "success",\n          "doc" : {\n            "_index" : "index",\n            "_id" : "id",\n            "_version": "-3",\n            "_source" : {\n              "field2" : "_value2",\n              "foo" : "bar"\n            },\n            "_ingest" : {\n              "pipeline" : "_simulate_pipeline",\n              "timestamp" : "2020-07-30T01:21:24.251836Z"\n            }\n          }\n        },\n        {\n          "processor_type" : "set",\n          "status" : "success",\n          "doc" : {\n            "_index" : "index",\n            "_id" : "id",\n            "_version": "-3",\n            "_source" : {\n              "field3" : "_value3",\n              "field2" : "_value2",\n              "foo" : "bar"\n            },\n            "_ingest" : {\n              "pipeline" : "_simulate_pipeline",\n              "timestamp" : "2020-07-30T01:21:24.251836Z"\n            }\n          }\n        }\n      ]\n    },\n    {\n      "processor_results" : [\n        {\n          "processor_type" : "set",\n          "status" : "success",\n          "doc" : {\n            "_index" : "index",\n            "_id" : "id",\n            "_version": "-3",\n            "_source" : {\n              "field2" : "_value2",\n              "foo" : "rab"\n            },\n            "_ingest" : {\n              "pipeline" : "_simulate_pipeline",\n              "timestamp" : "2020-07-30T01:21:24.251863Z"\n            }\n          }\n        },\n        {\n          "processor_type" : "set",\n          "status" : "success",\n          "doc" : {\n            "_index" : "index",\n            "_id" : "id",\n            "_version": "-3",\n            "_source" : {\n              "field3" : "_value3",\n              "field2" : "_value2",\n              "foo" : "rab"\n            },\n            "_ingest" : {\n              "pipeline" : "_simulate_pipeline",\n              "timestamp" : "2020-07-30T01:21:24.251863Z"\n            }\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n',
          title: 'Simulate pipeline API',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html',
          productName: 'elasticsearch',
          score: 135.32358,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html',
        title: 'Simulate pipeline API',
        score: 135.32358,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Simulate pipeline API\\n\\nExecutes an ingest pipeline against a set of provided documents.\\n\\n```\\nresp = client.ingest.simulate(\\n    id=\\"my-pipeline-id\\",\\n    docs=[\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"bar\\"\\n            }\\n        },\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"rab\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  id: \\"my-pipeline-id\\",\\n  docs: [\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"bar\\",\\n      },\\n    },\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"rab\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/pipeline/my-pipeline-id/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Request\\n\\n`POST /_ingest/pipeline/<pipeline>/_simulate`\\n\\n`GET /_ingest/pipeline/<pipeline>/_simulate`\\n\\n`POST /_ingest/pipeline/_simulate`\\n\\n`GET /_ingest/pipeline/_simulate`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `read_pipeline`, `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to use this API.\\n\\n### Description\\n\\nThe simulate pipeline API executes a specific pipeline against a set of documents provided in the body of the request.\\n\\nYou can either specify an existing pipeline to execute against the provided documents or supply a pipeline definition in the body of the request.\\n\\n### Path parameters\\n\\n* `<pipeline>`\\n\\n  (Required\\\\*, string) Pipeline to test. If you don’t specify a `pipeline` in the request body, this parameter is required.\\n\\n### Query parameters\\n\\n* `verbose`\\n\\n  (Optional, Boolean) If `true`, the response includes output data for each processor in the executed pipeline.\\n\\n### Request body\\n\\n* `pipeline`\\n\\n  (Required\\\\*, object) Pipeline to test. If you don’t specify the `<pipeline>` request path parameter, this parameter is required. If you specify both this and the request path parameter, the API only uses the request path parameter.\\n\\n  Properties of `pipeline`\\n\\n  * `description`\\n\\n    (Optional, string) Description of the ingest pipeline.\\n\\n  * `on_failure`\\n\\n    (Optional, array of [processor](processors.html \\"Ingest processor reference\\") objects) Processors to run immediately after a processor failure.\\n\\n    Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\\n\\n  * `processors`\\n\\n    (Required, array of [processor](processors.html \\"Ingest processor reference\\") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\\n\\n  * `version`\\n\\n    (Optional, integer) Version number used by external systems to track ingest pipelines.\\n\\n    See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params \\"Query parameters\\") parameter above for how the version attribute is used.\\n\\n  * `_meta`\\n\\n    (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\\n\\n  * `deprecated`\\n\\n    (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\\n\\n* `docs`\\n\\n  (Required, array of objects) Sample documents to test in the pipeline.\\n\\n  Properties of `docs` objects\\n\\n  * `_id`\\n\\n    (Optional, string) Unique identifier for the document. This ID must be unique within the `_index`.\\n\\n  * `_index`\\n\\n    (Optional, string) Name of the index containing the document.\\n\\n  * `_routing`\\n\\n    (Optional, string) Value used to send the document to a specific primary shard. See the [`_routing`](mapping-routing-field.html \\"_routing field\\") field.\\n\\n  * `_source`\\n\\n    (Required, object) JSON body for the document.\\n\\n### Examples\\n\\n#### Specify a pipeline as a path parameter\\n\\n```\\nresp = client.ingest.simulate(\\n    id=\\"my-pipeline-id\\",\\n    docs=[\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"bar\\"\\n            }\\n        },\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"rab\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  id: \\"my-pipeline-id\\",\\n  docs: [\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"bar\\",\\n      },\\n    },\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"rab\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/pipeline/my-pipeline-id/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n   \\"docs\\": [\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"id\\",\\n            \\"_index\\": \\"index\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\": {\\n               \\"field2\\": \\"_value\\",\\n               \\"foo\\": \\"bar\\"\\n            },\\n            \\"_ingest\\": {\\n               \\"timestamp\\": \\"2017-05-04T22:30:03.187Z\\"\\n            }\\n         }\\n      },\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"id\\",\\n            \\"_index\\": \\"index\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\": {\\n               \\"field2\\": \\"_value\\",\\n               \\"foo\\": \\"rab\\"\\n            },\\n            \\"_ingest\\": {\\n               \\"timestamp\\": \\"2017-05-04T22:30:03.188Z\\"\\n            }\\n         }\\n      }\\n   ]\\n}\\n```\\n\\n#### Specify a pipeline in the request body\\n\\n```\\nresp = client.ingest.simulate(\\n    pipeline={\\n        \\"description\\": \\"_description\\",\\n        \\"processors\\": [\\n            {\\n                \\"set\\": {\\n                    \\"field\\": \\"field2\\",\\n                    \\"value\\": \\"_value\\"\\n                }\\n            }\\n        ]\\n    },\\n    docs=[\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"bar\\"\\n            }\\n        },\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"rab\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.simulate(\\n  body: {\\n    pipeline: {\\n      description: \'_description\',\\n      processors: [\\n        {\\n          set: {\\n            field: \'field2\',\\n            value: \'_value\'\\n          }\\n        }\\n      ]\\n    },\\n    docs: [\\n      {\\n        _index: \'index\',\\n        _id: \'id\',\\n        _source: {\\n          foo: \'bar\'\\n        }\\n      },\\n      {\\n        _index: \'index\',\\n        _id: \'id\',\\n        _source: {\\n          foo: \'rab\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  pipeline: {\\n    description: \\"_description\\",\\n    processors: [\\n      {\\n        set: {\\n          field: \\"field2\\",\\n          value: \\"_value\\",\\n        },\\n      },\\n    ],\\n  },\\n  docs: [\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"bar\\",\\n      },\\n    },\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"rab\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/pipeline/_simulate\\n{\\n  \\"pipeline\\" :\\n  {\\n    \\"description\\": \\"_description\\",\\n    \\"processors\\": [\\n      {\\n        \\"set\\" : {\\n          \\"field\\" : \\"field2\\",\\n          \\"value\\" : \\"_value\\"\\n        }\\n      }\\n    ]\\n  },\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n   \\"docs\\": [\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"id\\",\\n            \\"_index\\": \\"index\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\": {\\n               \\"field2\\": \\"_value\\",\\n               \\"foo\\": \\"bar\\"\\n            },\\n            \\"_ingest\\": {\\n               \\"timestamp\\": \\"2017-05-04T22:30:03.187Z\\"\\n            }\\n         }\\n      },\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"id\\",\\n            \\"_index\\": \\"index\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\": {\\n               \\"field2\\": \\"_value\\",\\n               \\"foo\\": \\"rab\\"\\n            },\\n            \\"_ingest\\": {\\n               \\"timestamp\\": \\"2017-05-04T22:30:03.188Z\\"\\n            }\\n         }\\n      }\\n   ]\\n}\\n```\\n\\n#### View verbose results\\n\\nYou can use the simulate pipeline API to see how each processor affects the ingest document as it passes through the pipeline. To see the intermediate results of each processor in the simulate request, you can add the `verbose` parameter to the request.\\n\\n```\\nresp = client.ingest.simulate(\\n    verbose=True,\\n    pipeline={\\n        \\"description\\": \\"_description\\",\\n        \\"processors\\": [\\n            {\\n                \\"set\\": {\\n                    \\"field\\": \\"field2\\",\\n                    \\"value\\": \\"_value2\\"\\n                }\\n            },\\n            {\\n                \\"set\\": {\\n                    \\"field\\": \\"field3\\",\\n                    \\"value\\": \\"_value3\\"\\n                }\\n            }\\n        ]\\n    },\\n    docs=[\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"bar\\"\\n            }\\n        },\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"rab\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.simulate(\\n  verbose: true,\\n  body: {\\n    pipeline: {\\n      description: \'_description\',\\n      processors: [\\n        {\\n          set: {\\n            field: \'field2\',\\n            value: \'_value2\'\\n          }\\n        },\\n        {\\n          set: {\\n            field: \'field3\',\\n            value: \'_value3\'\\n          }\\n        }\\n      ]\\n    },\\n    docs: [\\n      {\\n        _index: \'index\',\\n        _id: \'id\',\\n        _source: {\\n          foo: \'bar\'\\n        }\\n      },\\n      {\\n        _index: \'index\',\\n        _id: \'id\',\\n        _source: {\\n          foo: \'rab\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  verbose: \\"true\\",\\n  pipeline: {\\n    description: \\"_description\\",\\n    processors: [\\n      {\\n        set: {\\n          field: \\"field2\\",\\n          value: \\"_value2\\",\\n        },\\n      },\\n      {\\n        set: {\\n          field: \\"field3\\",\\n          value: \\"_value3\\",\\n        },\\n      },\\n    ],\\n  },\\n  docs: [\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"bar\\",\\n      },\\n    },\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"rab\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/pipeline/_simulate?verbose=true\\n{\\n  \\"pipeline\\" :\\n  {\\n    \\"description\\": \\"_description\\",\\n    \\"processors\\": [\\n      {\\n        \\"set\\" : {\\n          \\"field\\" : \\"field2\\",\\n          \\"value\\" : \\"_value2\\"\\n        }\\n      },\\n      {\\n        \\"set\\" : {\\n          \\"field\\" : \\"field3\\",\\n          \\"value\\" : \\"_value3\\"\\n        }\\n      }\\n    ]\\n  },\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n  \\"docs\\" : [\\n    {\\n      \\"processor_results\\" : [\\n        {\\n          \\"processor_type\\" : \\"set\\",\\n          \\"status\\" : \\"success\\",\\n          \\"doc\\" : {\\n            \\"_index\\" : \\"index\\",\\n            \\"_id\\" : \\"id\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\" : {\\n              \\"field2\\" : \\"_value2\\",\\n              \\"foo\\" : \\"bar\\"\\n            },\\n            \\"_ingest\\" : {\\n              \\"pipeline\\" : \\"_simulate_pipeline\\",\\n              \\"timestamp\\" : \\"2020-07-30T01:21:24.251836Z\\"\\n            }\\n          }\\n        },\\n        {\\n          \\"processor_type\\" : \\"set\\",\\n          \\"status\\" : \\"success\\",\\n          \\"doc\\" : {\\n            \\"_index\\" : \\"index\\",\\n            \\"_id\\" : \\"id\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\" : {\\n              \\"field3\\" : \\"_value3\\",\\n              \\"field2\\" : \\"_value2\\",\\n              \\"foo\\" : \\"bar\\"\\n            },\\n            \\"_ingest\\" : {\\n              \\"pipeline\\" : \\"_simulate_pipeline\\",\\n              \\"timestamp\\" : \\"2020-07-30T01:21:24.251836Z\\"\\n            }\\n          }\\n        }\\n      ]\\n    },\\n    {\\n      \\"processor_results\\" : [\\n        {\\n          \\"processor_type\\" : \\"set\\",\\n          \\"status\\" : \\"success\\",\\n          \\"doc\\" : {\\n            \\"_index\\" : \\"index\\",\\n            \\"_id\\" : \\"id\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\" : {\\n              \\"field2\\" : \\"_value2\\",\\n              \\"foo\\" : \\"rab\\"\\n            },\\n            \\"_ingest\\" : {\\n              \\"pipeline\\" : \\"_simulate_pipeline\\",\\n              \\"timestamp\\" : \\"2020-07-30T01:21:24.251863Z\\"\\n            }\\n          }\\n        },\\n        {\\n          \\"processor_type\\" : \\"set\\",\\n          \\"status\\" : \\"success\\",\\n          \\"doc\\" : {\\n            \\"_index\\" : \\"index\\",\\n            \\"_id\\" : \\"id\\",\\n            \\"_version\\": \\"-3\\",\\n            \\"_source\\" : {\\n              \\"field3\\" : \\"_value3\\",\\n              \\"field2\\" : \\"_value2\\",\\n              \\"foo\\" : \\"rab\\"\\n            },\\n            \\"_ingest\\" : {\\n              \\"pipeline\\" : \\"_simulate_pipeline\\",\\n              \\"timestamp\\" : \\"2020-07-30T01:21:24.251863Z\\"\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ]\\n}\\n```\\n","title":"Simulate pipeline API","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html","productName":"elasticsearch","score":135.32358}',
        truncated: {
          truncatedText:
            '{"content":"## Simulate pipeline API\\n\\nExecutes an ingest pipeline against a set of provided documents.\\n\\n```\\nresp = client.ingest.simulate(\\n    id=\\"my-pipeline-id\\",\\n    docs=[\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"bar\\"\\n            }\\n        },\\n        {\\n            \\"_index\\": \\"index\\",\\n            \\"_id\\": \\"id\\",\\n            \\"_source\\": {\\n                \\"foo\\": \\"rab\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.ingest.simulate({\\n  id: \\"my-pipeline-id\\",\\n  docs: [\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"bar\\",\\n      },\\n    },\\n    {\\n      _index: \\"index\\",\\n      _id: \\"id\\",\\n      _source: {\\n        foo: \\"rab\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/pipeline/my-pipeline-id/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Request\\n... <truncated>',
          originalTokenCount: 4442,
          truncatedTokenCount: 400,
        },
        llmScore: 4,
      },
      {
        selected: true,
        document: {
          content:
            '## Create or update pipeline API\n\nCreates or updates an [ingest pipeline](ingest.html "Ingest pipelines"). Changes made using this API take effect immediately.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline-id",\n    description="My optional pipeline description",\n    processors=[\n        {\n            "set": {\n                "description": "My optional processor description",\n                "field": "my-keyword-field",\n                "value": "foo"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline-id\',\n  body: {\n    description: \'My optional pipeline description\',\n    processors: [\n      {\n        set: {\n          description: \'My optional processor description\',\n          field: \'my-keyword-field\',\n          value: \'foo\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline-id",\n  description: "My optional pipeline description",\n  processors: [\n    {\n      set: {\n        description: "My optional processor description",\n        field: "my-keyword-field",\n        value: "foo",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline-id\n{\n  "description" : "My optional pipeline description",\n  "processors" : [\n    {\n      "set" : {\n        "description" : "My optional processor description",\n        "field": "my-keyword-field",\n        "value": "foo"\n      }\n    }\n  ]\n}\n```\n\n### Request\n\n`PUT /_ingest/pipeline/<pipeline>`\n\n### Prerequisites\n\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to use this API.\n\n### Path parameters\n\n* `<pipeline>`\n\n  (Required, string) ID of the ingest pipeline to create or update.\n\n  To avoid naming collisions with built-in and Fleet-managed ingest pipelines, avoid using `@` as part of your own ingest pipelines names. The exception of that rule are the `*@custom` ingest pipelines that let you safely add a custom pipeline to managed pipelines. See also [Pipelines for Fleet and Elastic Agent](ingest.html#pipelines-for-fleet-elastic-agent "Pipelines for Fleet and Elastic Agent").\n\n### Query parameters\n\n* `if_version`\n\n  (Optional, integer) Perform the operation only if the pipeline has this version. If specified and the update is successful, the pipeline’s version is incremented.\n\n* `master_timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n* `timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for a response from all relevant nodes in the cluster after updating the cluster metadata. If no response is received before the timeout expires, the cluster metadata update still applies but the response will indicate that it was not completely acknowledged. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n### Request body\n\n* `description`\n\n  (Optional, string) Description of the ingest pipeline.\n\n* `on_failure`\n\n  (Optional, array of [processor](processors.html "Ingest processor reference") objects) Processors to run immediately after a processor failure.\n\n  Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\n\n* `processors`\n\n  (Required, array of [processor](processors.html "Ingest processor reference") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\n\n* `version`\n\n  (Optional, integer) Version number used by external systems to track ingest pipelines.\n\n  See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params "Query parameters") parameter above for how the version attribute is used.\n\n* `_meta`\n\n  (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\n\n* `deprecated`\n\n  (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\n\n### Examples\n\n#### Pipeline metadata\n\nYou can use the `_meta` parameter to add arbitrary metadata to a pipeline. This user-defined object is stored in the cluster state, so keeping it short is preferable.\n\nThe `_meta` parameter is optional and not automatically generated or used by Elasticsearch.\n\nTo unset `_meta`, replace the pipeline without specifying one.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline-id",\n    description="My optional pipeline description",\n    processors=[\n        {\n            "set": {\n                "description": "My optional processor description",\n                "field": "my-keyword-field",\n                "value": "foo"\n            }\n        }\n    ],\n    meta={\n        "reason": "set my-keyword-field to foo",\n        "serialization": {\n            "class": "MyPipeline",\n            "id": 10\n        }\n    },\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline-id\',\n  body: {\n    description: \'My optional pipeline description\',\n    processors: [\n      {\n        set: {\n          description: \'My optional processor description\',\n          field: \'my-keyword-field\',\n          value: \'foo\'\n        }\n      }\n    ],\n    _meta: {\n      reason: \'set my-keyword-field to foo\',\n      serialization: {\n        class: \'MyPipeline\',\n        id: 10\n      }\n    }\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline-id",\n  description: "My optional pipeline description",\n  processors: [\n    {\n      set: {\n        description: "My optional processor description",\n        field: "my-keyword-field",\n        value: "foo",\n      },\n    },\n  ],\n  _meta: {\n    reason: "set my-keyword-field to foo",\n    serialization: {\n      class: "MyPipeline",\n      id: 10,\n    },\n  },\n});\nconsole.log(response);\n```\n\n```\nPUT /_ingest/pipeline/my-pipeline-id\n{\n  "description" : "My optional pipeline description",\n  "processors" : [\n    {\n      "set" : {\n        "description" : "My optional processor description",\n        "field": "my-keyword-field",\n        "value": "foo"\n      }\n    }\n  ],\n  "_meta": {\n    "reason": "set my-keyword-field to foo",\n    "serialization": {\n      "class": "MyPipeline",\n      "id": 10\n    }\n  }\n}\n```\n\nTo check the `_meta`, use the [get pipeline](get-pipeline-api.html "Get pipeline API") API.\n',
          title: 'Create or update pipeline API',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/put-pipeline-api.html',
          productName: 'elasticsearch',
          score: 135.16127,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/put-pipeline-api.html',
        title: 'Create or update pipeline API',
        score: 135.16127,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Create or update pipeline API\\n\\nCreates or updates an [ingest pipeline](ingest.html \\"Ingest pipelines\\"). Changes made using this API take effect immediately.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline-id\\",\\n    description=\\"My optional pipeline description\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"My optional processor description\\",\\n                \\"field\\": \\"my-keyword-field\\",\\n                \\"value\\": \\"foo\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline-id\',\\n  body: {\\n    description: \'My optional pipeline description\',\\n    processors: [\\n      {\\n        set: {\\n          description: \'My optional processor description\',\\n          field: \'my-keyword-field\',\\n          value: \'foo\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline-id\\",\\n  description: \\"My optional pipeline description\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"My optional processor description\\",\\n        field: \\"my-keyword-field\\",\\n        value: \\"foo\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline-id\\n{\\n  \\"description\\" : \\"My optional pipeline description\\",\\n  \\"processors\\" : [\\n    {\\n      \\"set\\" : {\\n        \\"description\\" : \\"My optional processor description\\",\\n        \\"field\\": \\"my-keyword-field\\",\\n        \\"value\\": \\"foo\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n### Request\\n\\n`PUT /_ingest/pipeline/<pipeline>`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to use this API.\\n\\n### Path parameters\\n\\n* `<pipeline>`\\n\\n  (Required, string) ID of the ingest pipeline to create or update.\\n\\n  To avoid naming collisions with built-in and Fleet-managed ingest pipelines, avoid using `@` as part of your own ingest pipelines names. The exception of that rule are the `*@custom` ingest pipelines that let you safely add a custom pipeline to managed pipelines. See also [Pipelines for Fleet and Elastic Agent](ingest.html#pipelines-for-fleet-elastic-agent \\"Pipelines for Fleet and Elastic Agent\\").\\n\\n### Query parameters\\n\\n* `if_version`\\n\\n  (Optional, integer) Perform the operation only if the pipeline has this version. If specified and the update is successful, the pipeline’s version is incremented.\\n\\n* `master_timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n* `timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for a response from all relevant nodes in the cluster after updating the cluster metadata. If no response is received before the timeout expires, the cluster metadata update still applies but the response will indicate that it was not completely acknowledged. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n### Request body\\n\\n* `description`\\n\\n  (Optional, string) Description of the ingest pipeline.\\n\\n* `on_failure`\\n\\n  (Optional, array of [processor](processors.html \\"Ingest processor reference\\") objects) Processors to run immediately after a processor failure.\\n\\n  Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\\n\\n* `processors`\\n\\n  (Required, array of [processor](processors.html \\"Ingest processor reference\\") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\\n\\n* `version`\\n\\n  (Optional, integer) Version number used by external systems to track ingest pipelines.\\n\\n  See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params \\"Query parameters\\") parameter above for how the version attribute is used.\\n\\n* `_meta`\\n\\n  (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\\n\\n* `deprecated`\\n\\n  (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\\n\\n### Examples\\n\\n#### Pipeline metadata\\n\\nYou can use the `_meta` parameter to add arbitrary metadata to a pipeline. This user-defined object is stored in the cluster state, so keeping it short is preferable.\\n\\nThe `_meta` parameter is optional and not automatically generated or used by Elasticsearch.\\n\\nTo unset `_meta`, replace the pipeline without specifying one.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline-id\\",\\n    description=\\"My optional pipeline description\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"My optional processor description\\",\\n                \\"field\\": \\"my-keyword-field\\",\\n                \\"value\\": \\"foo\\"\\n            }\\n        }\\n    ],\\n    meta={\\n        \\"reason\\": \\"set my-keyword-field to foo\\",\\n        \\"serialization\\": {\\n            \\"class\\": \\"MyPipeline\\",\\n            \\"id\\": 10\\n        }\\n    },\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline-id\',\\n  body: {\\n    description: \'My optional pipeline description\',\\n    processors: [\\n      {\\n        set: {\\n          description: \'My optional processor description\',\\n          field: \'my-keyword-field\',\\n          value: \'foo\'\\n        }\\n      }\\n    ],\\n    _meta: {\\n      reason: \'set my-keyword-field to foo\',\\n      serialization: {\\n        class: \'MyPipeline\',\\n        id: 10\\n      }\\n    }\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline-id\\",\\n  description: \\"My optional pipeline description\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"My optional processor description\\",\\n        field: \\"my-keyword-field\\",\\n        value: \\"foo\\",\\n      },\\n    },\\n  ],\\n  _meta: {\\n    reason: \\"set my-keyword-field to foo\\",\\n    serialization: {\\n      class: \\"MyPipeline\\",\\n      id: 10,\\n    },\\n  },\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT /_ingest/pipeline/my-pipeline-id\\n{\\n  \\"description\\" : \\"My optional pipeline description\\",\\n  \\"processors\\" : [\\n    {\\n      \\"set\\" : {\\n        \\"description\\" : \\"My optional processor description\\",\\n        \\"field\\": \\"my-keyword-field\\",\\n        \\"value\\": \\"foo\\"\\n      }\\n    }\\n  ],\\n  \\"_meta\\": {\\n    \\"reason\\": \\"set my-keyword-field to foo\\",\\n    \\"serialization\\": {\\n      \\"class\\": \\"MyPipeline\\",\\n      \\"id\\": 10\\n    }\\n  }\\n}\\n```\\n\\nTo check the `_meta`, use the [get pipeline](get-pipeline-api.html \\"Get pipeline API\\") API.\\n","title":"Create or update pipeline API","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/put-pipeline-api.html","productName":"elasticsearch","score":135.16127}',
        truncated: {
          truncatedText:
            '{"content":"## Create or update pipeline API\\n\\nCreates or updates an [ingest pipeline](ingest.html \\"Ingest pipelines\\"). Changes made using this API take effect immediately.\\n\\n```\\nresp = client.ingest.put_pipeline(\\n    id=\\"my-pipeline-id\\",\\n    description=\\"My optional pipeline description\\",\\n    processors=[\\n        {\\n            \\"set\\": {\\n                \\"description\\": \\"My optional processor description\\",\\n                \\"field\\": \\"my-keyword-field\\",\\n                \\"value\\": \\"foo\\"\\n            }\\n        }\\n    ],\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.put_pipeline(\\n  id: \'my-pipeline-id\',\\n  body: {\\n    description: \'My optional pipeline description\',\\n    processors: [\\n      {\\n        set: {\\n          description: \'My optional processor description\',\\n          field: \'my-keyword-field\',\\n          value: \'foo\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.putPipeline({\\n  id: \\"my-pipeline-id\\",\\n  description: \\"My optional pipeline description\\",\\n  processors: [\\n    {\\n      set: {\\n        description: \\"My optional processor description\\",\\n        field: \\"my-keyword-field\\",\\n        value: \\"foo\\",\\n      },\\n    },\\n  ],\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPUT _ingest/pipeline/my-pipeline-id\\n{\\n  \\"description\\" : \\"My optional pipeline description\\",\\n  \\"processors\\" : [\\n    {\\n      \\"set\\" : {\\n        \\"description\\" : \\"My optional processor description\\",\\n        \\"field\\":... <truncated>',
          originalTokenCount: 1903,
          truncatedTokenCount: 400,
        },
        llmScore: 5,
      },
      {
        selected: false,
        document: {
          author: {
            login: 'miltonhultgren',
          },
          issue_comments: [
            {
              author: {
                login: 'miltonhultgren',
              },
              body: 'This came out of the investigation done [here](https://github.com/elastic/kibana/issues/137277#issuecomment-1253665335).',
            },
            {
              author: {
                login: 'pmuellr',
              },
              body: 'Have you looked at [Canvas Expressions](https://www.elastic.co/guide/en/kibana/current/canvas-expression-lifecycle.html) yet?  They can be used server-side, but not sure if anyone is really using them there yet.  From when I last looked, there\'s not a ton of functions available server-side, but they do seem to be easy to add.\r\n\r\nI\'ve had thoughts about doing something with them in a rule - user provides an "expression", and perhaps a "parameter", which should end up returning a table of alerts-as-rows, with some fixed "column" name as the alert id, the rest of the columns as context variables, similar to how my [space-time es sql rule works](https://github.com/pmuellr/kibana/blob/onweek/2022/x-pack/examples/ow22_pmuellr/docs/sql_alerting_rule.md).\r\n\r\nOf course, we also have ESQL coming, which seems very similar ...\r\n',
            },
            {
              author: {
                login: 'miltonhultgren',
              },
              body: 'I think the scope for this space time was a bit too ambitious to manage in a week with a shut it down day 😂 \r\nMy de-scoped plan is to look at the rules we do have today, try to break them into pipeline "steps" and try to reduce this set of steps down as much as possible, then try to "build" some new rule using those steps to try to verify if that works "in theory".\r\n\r\nSo far I\'ve broken the type of steps into 4 phases:\r\n1. Collect and select (run queries, decide which of the returned documents/buckets to use)\r\n2. Process (derive new values, extract values from documents, parse other formats)\r\n3. Evaluate (decide if an alert should be fired, based on thresholds of gauges/counters, rate of change, missing data)\r\n4. Alert and act (decide how many alerts should be created, based on phase 1 we might need to ungroup items, shape this into a domain specific message and fire of actions based on severity)\r\n\r\nAs examples of phase 1, you might just do a plain query to grab a single document, or you want to grab multiple documents, or the last and first document in a "series" or the last and next-to-last, or you want to run a single aggregation or multiple aggregations to get metrics rather than documents out. \r\nAt the same time it shouldn\'t just be a leaky abstraction over the ES DSL. I imagine that phase 1 would be defined as any number of data fetching blocks and the resulting set of "things" (documents, metrics) would be passed into phase 2 where phase 2 is more like an ingest pipeline that can be chained "infinitely" to shape those "things" and at the end of that pipeline the "things" get passed into phase 3 for the evaluation. \r\nI\'m still wondering if the evaluation can be generic based on the flexibility of the previous steps or if it needs to be more like a script that is some kind of predicate function (think Painless scripts that are used in ES in many places).\r\n\r\nThat, together with the idea of an ingest pipeline, makes me wonder again why the Kibana Rules run outside of Elasticsearch versus things like ingest pipelines, ML jobs and transforms. I wonder, **if** we had this kind of building block system that lived in ES, would that be okay? Would that allow us to avoid some of the performance challenges we face today but still be flexible enough that we don\'t have to regularly work inside the ES code base to add new Rules? Would love to hear from @pmuellr and team on that thought, I\'m likely missing something obvious.\r\n\r\nIn the meantime, I\'m going to keep trying to see if I can figure out what a minimal set of steps would be to produce enough flexibility and how that might look in a DSL for rule pipelines, if there is time or next ON Week I would like to dig more into the actual code of such a "pipeline executor" and see what the actual interfaces between steps would look like.',
            },
            {
              author: {
                login: 'pmuellr',
              },
              body: '> That, together with the idea of an ingest pipeline, makes me wonder again why the Kibana Rules run outside of Elasticsearch versus things like ingest pipelines, ML jobs and transforms. I wonder, if we had this kind of building block system that lived in ES, would that be okay? Would that allow us to avoid some of the performance challenges we face today but still be flexible enough that we don\'t have to regularly work inside the ES code base to add new Rules? Would love to hear from @pmuellr and team on that thought, I\'m likely missing something obvious.\r\n\r\nNo argument from me!  Other than I don\'t want to port all our code to Java (issue with "port" in general, not "Java" per se 😀).\r\n\r\nI\'ve also wondered about things like "why aren\'t alerting rules just simple queries on transforms" - so a rule is some kind of simple search over an index produced by a presumably more complex transform.  Guess what, there\'s yur "alerts as data"!  :-)\r\n\r\nWith ESQL in the wings, how does that change things?  What about serverless? Lot\'s o questions!',
            },
            {
              author: {
                login: 'miltonhultgren',
              },
              body: '@dgieselaar has also done some prior art in this space! \r\nhttps://github.com/dgieselaar/kibana/pull/4',
            },
            {
              author: {
                login: 'miltonhultgren',
              },
              body: "I didn't much progress during last week, mostly due to a lack of focus~. \r\nAfter this week I'm mostly left with a lot of questions of how similar or different the initial query steps are, but also a feeling that this would be a very powerful feature, specially to offer to Integration developers.\r\n\r\n<details>\r\n  <summary>Some rough ideas for the kinds of steps to support</summary>\r\n  \r\n Collect&Select:\r\n    Single document\r\n    Set of documents (2..N)\r\n      Top N\r\n      Filter?\r\n    Grouped set of documents\r\n    Value from aggregation\r\n    Grouped values from aggregations\r\n  \r\n  Process:\r\n    Map/Reduce?\r\n    Extract field/value/parse JSON\r\n    Math\r\n    Combine\r\n    Custom script?\r\n  \r\n  Evaluate:\r\n    Custom script?\r\n    Threshold\r\n    Rate\r\n    Counter change\r\n  \r\n  Report&Act:\r\n    Split from groups\r\n    Severity selection\r\n</details>",
            },
            {
              author: {
                login: 'pmuellr',
              },
              body: 'One thing we\'ve been pushing for a year or so is moving more of the rule processing into ES.  I\'m pretty sure that happened with some of the o11y metric rules after Affirm, and we\'ve made some changes to Index Threshold as well to do the threshold comparison in ES, instead of Kibana.\r\n\r\nSo, the advice you\'d get from us today would be Collect&Select, Process and Evaluate should be all be done in ES, in hopefully one request.  Report&Act items "Split from groups" and "Severity selection" also sound like things that could be done in ES.\r\n\r\nWhen you think about ESQL, I wonder if your rough grouping above would somehow become a grouping of different kinds of "stages" (or whatever they call them) in an ESQL pipeline?',
            },
            {
              author: {
                login: 'miltonhultgren',
              },
              body: '> When you think about ESQL, I wonder if your rough grouping above would somehow become a grouping of different kinds of "stages" (or whatever they call them) in an ESQL pipeline?\r\n\r\nYes, this was at the back of my mind while working on this since you mentioned ESQL in your previous comment! \r\nI suppose I should add a todo for this to check in with some ESQL folks what they think about this and what\'s possible today/tomorrow.',
            },
          ],
          title: 'Spacetime: Rules as Pipelines',
          body: 'In a few places within the Elastic stack we have ways to compose steps into a pipeline, such as Ingest pipelines and Beast/Logstash processors. This allows our users to customize their usage of the stack in a very flexible way which means we can meet more needs with a similar amount of code/developer effort. For alerting rules, we currently maintain a lot of different rules but I imagine there are many steps done in those rules that are similar and that are simply composed in different ways. I would like to explore what it would look like to have a way to compose a few steps into a rule executor and register this executor so that rules can be created from it and run. ### What might be needed: - Rule (executor) as config (likely JSON in a Saved Object, excellent for putting into integrations) - Steps that can be composed (smaller JavaScript functions that are called in sequence) - Resulting alerts are given some domain context - A way to stream/segment/partition/loop the pipeline for when the dataset becomes to large (high cardinality group by) Doing this might end up simplifying the executors by moving some coordinating logic out into a higher layer, and allow previews of each step without having to run the whole pipeline.',
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/2395',
          number: 2395,
          createdAt: '2022-09-27T14:10:33Z',
          assignees_list: [],
          labels_field: [
            {
              name: 'spacetime',
              description: '',
            },
          ],
          state: 'OPEN',
          id: 'I_kwDOCYaMzs5SuLW_',
          closedAt: null,
          _timestamp: '2023-11-13T23:44:59Z',
        },
        id: 'search-observability-dev/I_kwDOCYaMzs5SuLW_',
        title: 'Spacetime: Rules as Pipelines',
        score: 134.25397,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"miltonhultgren"},"issue_comments":[{"author":{"login":"miltonhultgren"},"body":"This came out of the investigation done [here](https://github.com/elastic/kibana/issues/137277#issuecomment-1253665335)."},{"author":{"login":"pmuellr"},"body":"Have you looked at [Canvas Expressions](https://www.elastic.co/guide/en/kibana/current/canvas-expression-lifecycle.html) yet?  They can be used server-side, but not sure if anyone is really using them there yet.  From when I last looked, there\'s not a ton of functions available server-side, but they do seem to be easy to add.\\r\\n\\r\\nI\'ve had thoughts about doing something with them in a rule - user provides an \\"expression\\", and perhaps a \\"parameter\\", which should end up returning a table of alerts-as-rows, with some fixed \\"column\\" name as the alert id, the rest of the columns as context variables, similar to how my [space-time es sql rule works](https://github.com/pmuellr/kibana/blob/onweek/2022/x-pack/examples/ow22_pmuellr/docs/sql_alerting_rule.md).\\r\\n\\r\\nOf course, we also have ESQL coming, which seems very similar ...\\r\\n"},{"author":{"login":"miltonhultgren"},"body":"I think the scope for this space time was a bit too ambitious to manage in a week with a shut it down day 😂 \\r\\nMy de-scoped plan is to look at the rules we do have today, try to break them into pipeline \\"steps\\" and try to reduce this set of steps down as much as possible, then try to \\"build\\" some new rule using those steps to try to verify if that works \\"in theory\\".\\r\\n\\r\\nSo far I\'ve broken the type of steps into 4 phases:\\r\\n1. Collect and select (run queries, decide which of the returned documents/buckets to use)\\r\\n2. Process (derive new values, extract values from documents, parse other formats)\\r\\n3. Evaluate (decide if an alert should be fired, based on thresholds of gauges/counters, rate of change, missing data)\\r\\n4. Alert and act (decide how many alerts should be created, based on phase 1 we might need to ungroup items, shape this into a domain specific message and fire of actions based on severity)\\r\\n\\r\\nAs examples of phase 1, you might just do a plain query to grab a single document, or you want to grab multiple documents, or the last and first document in a \\"series\\" or the last and next-to-last, or you want to run a single aggregation or multiple aggregations to get metrics rather than documents out. \\r\\nAt the same time it shouldn\'t just be a leaky abstraction over the ES DSL. I imagine that phase 1 would be defined as any number of data fetching blocks and the resulting set of \\"things\\" (documents, metrics) would be passed into phase 2 where phase 2 is more like an ingest pipeline that can be chained \\"infinitely\\" to shape those \\"things\\" and at the end of that pipeline the \\"things\\" get passed into phase 3 for the evaluation. \\r\\nI\'m still wondering if the evaluation can be generic based on the flexibility of the previous steps or if it needs to be more like a script that is some kind of predicate function (think Painless scripts that are used in ES in many places).\\r\\n\\r\\nThat, together with the idea of an ingest pipeline, makes me wonder again why the Kibana Rules run outside of Elasticsearch versus things like ingest pipelines, ML jobs and transforms. I wonder, **if** we had this kind of building block system that lived in ES, would that be okay? Would that allow us to avoid some of the performance challenges we face today but still be flexible enough that we don\'t have to regularly work inside the ES code base to add new Rules? Would love to hear from @pmuellr and team on that thought, I\'m likely missing something obvious.\\r\\n\\r\\nIn the meantime, I\'m going to keep trying to see if I can figure out what a minimal set of steps would be to produce enough flexibility and how that might look in a DSL for rule pipelines, if there is time or next ON Week I would like to dig more into the actual code of such a \\"pipeline executor\\" and see what the actual interfaces between steps would look like."},{"author":{"login":"pmuellr"},"body":"> That, together with the idea of an ingest pipeline, makes me wonder again why the Kibana Rules run outside of Elasticsearch versus things like ingest pipelines, ML jobs and transforms. I wonder, if we had this kind of building block system that lived in ES, would that be okay? Would that allow us to avoid some of the performance challenges we face today but still be flexible enough that we don\'t have to regularly work inside the ES code base to add new Rules? Would love to hear from @pmuellr and team on that thought, I\'m likely missing something obvious.\\r\\n\\r\\nNo argument from me!  Other than I don\'t want to port all our code to Java (issue with \\"port\\" in general, not \\"Java\\" per se 😀).\\r\\n\\r\\nI\'ve also wondered about things like \\"why aren\'t alerting rules just simple queries on transforms\\" - so a rule is some kind of simple search over an index produced by a presumably more complex transform.  Guess what, there\'s yur \\"alerts as data\\"!  :-)\\r\\n\\r\\nWith ESQL in the wings, how does that change things?  What about serverless? Lot\'s o questions!"},{"author":{"login":"miltonhultgren"},"body":"@dgieselaar has also done some prior art in this space! \\r\\nhttps://github.com/dgieselaar/kibana/pull/4"},{"author":{"login":"miltonhultgren"},"body":"I didn\'t much progress during last week, mostly due to a lack of focus~. \\r\\nAfter this week I\'m mostly left with a lot of questions of how similar or different the initial query steps are, but also a feeling that this would be a very powerful feature, specially to offer to Integration developers.\\r\\n\\r\\n<details>\\r\\n  <summary>Some rough ideas for the kinds of steps to support</summary>\\r\\n  \\r\\n Collect&Select:\\r\\n    Single document\\r\\n    Set of documents (2..N)\\r\\n      Top N\\r\\n      Filter?\\r\\n    Grouped set of documents\\r\\n    Value from aggregation\\r\\n    Grouped values from aggregations\\r\\n  \\r\\n  Process:\\r\\n    Map/Reduce?\\r\\n    Extract field/value/parse JSON\\r\\n    Math\\r\\n    Combine\\r\\n    Custom script?\\r\\n  \\r\\n  Evaluate:\\r\\n    Custom script?\\r\\n    Threshold\\r\\n    Rate\\r\\n    Counter change\\r\\n  \\r\\n  Report&Act:\\r\\n    Split from groups\\r\\n    Severity selection\\r\\n</details>"},{"author":{"login":"pmuellr"},"body":"One thing we\'ve been pushing for a year or so is moving more of the rule processing into ES.  I\'m pretty sure that happened with some of the o11y metric rules after Affirm, and we\'ve made some changes to Index Threshold as well to do the threshold comparison in ES, instead of Kibana.\\r\\n\\r\\nSo, the advice you\'d get from us today would be Collect&Select, Process and Evaluate should be all be done in ES, in hopefully one request.  Report&Act items \\"Split from groups\\" and \\"Severity selection\\" also sound like things that could be done in ES.\\r\\n\\r\\nWhen you think about ESQL, I wonder if your rough grouping above would somehow become a grouping of different kinds of \\"stages\\" (or whatever they call them) in an ESQL pipeline?"},{"author":{"login":"miltonhultgren"},"body":"> When you think about ESQL, I wonder if your rough grouping above would somehow become a grouping of different kinds of \\"stages\\" (or whatever they call them) in an ESQL pipeline?\\r\\n\\r\\nYes, this was at the back of my mind while working on this since you mentioned ESQL in your previous comment! \\r\\nI suppose I should add a todo for this to check in with some ESQL folks what they think about this and what\'s possible today/tomorrow."}],"title":"Spacetime: Rules as Pipelines","body":"In a few places within the Elastic stack we have ways to compose steps into a pipeline, such as Ingest pipelines and Beast/Logstash processors. This allows our users to customize their usage of the stack in a very flexible way which means we can meet more needs with a similar amount of code/developer effort. For alerting rules, we currently maintain a lot of different rules but I imagine there are many steps done in those rules that are similar and that are simply composed in different ways. I would like to explore what it would look like to have a way to compose a few steps into a rule executor and register this executor so that rules can be created from it and run. ### What might be needed: - Rule (executor) as config (likely JSON in a Saved Object, excellent for putting into integrations) - Steps that can be composed (smaller JavaScript functions that are called in sequence) - Resulting alerts are given some domain context - A way to stream/segment/partition/loop the pipeline for when the dataset becomes to large (high cardinality group by) Doing this might end up simplifying the executors by moving some coordinating logic out into a higher layer, and allow previews of each step without having to run the whole pipeline.","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/2395","number":2395,"createdAt":"2022-09-27T14:10:33Z","assignees_list":[],"labels_field":[{"name":"spacetime","description":""}],"state":"OPEN","id":"I_kwDOCYaMzs5SuLW_","closedAt":null,"_timestamp":"2023-11-13T23:44:59Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"miltonhultgren"},"issue_comments":[{"author":{"login":"miltonhultgren"},"body":"This came out of the investigation done [here](https://github.com/elastic/kibana/issues/137277#issuecomment-1253665335)."},{"author":{"login":"pmuellr"},"body":"Have you looked at [Canvas Expressions](https://www.elastic.co/guide/en/kibana/current/canvas-expression-lifecycle.html) yet?  They can be used server-side, but not sure if anyone is really using them there yet.  From when I last looked, there\'s not a ton of functions available server-side, but they do seem to be easy to add.\\r\\n\\r\\nI\'ve had thoughts about doing something with them in a rule - user provides an \\"expression\\", and perhaps a \\"parameter\\", which should end up returning a table of alerts-as-rows, with some fixed \\"column\\" name as the alert id, the rest of the columns as context variables, similar to how my [space-time es sql rule works](https://github.com/pmuellr/kibana/blob/onweek/2022/x-pack/examples/ow22_pmuellr/docs/sql_alerting_rule.md).\\r\\n\\r\\nOf course, we also have ESQL coming, which seems very similar ...\\r\\n"},{"author":{"login":"miltonhultgren"},"body":"I think the scope for this space time was a bit too ambitious to manage in a week with a shut it down day 😂 \\r\\nMy de-scoped plan is to look at the rules we do have today, try to break them into pipeline \\"steps\\" and try to reduce this set of steps down as much as possible, then try to \\"build\\" some new rule using those steps to try to verify if that works \\"in theory\\".\\r\\n\\r\\nSo far I\'ve broken the type of steps into 4 phases... <truncated>',
          originalTokenCount: 2176,
          truncatedTokenCount: 400,
        },
        llmScore: 1,
      },
      {
        selected: true,
        document: {
          content:
            '## Ingest processor reference\n\nAn [ingest pipeline](ingest.html "Ingest pipelines") is made up of a sequence of processors that are applied to documents as they are ingested into an index. Each processor performs a specific task, such as filtering, transforming, or enriching data.\n\nEach successive processor depends on the output of the previous processor, so the order of processors is important. The modified documents are indexed into Elasticsearch after all processors are applied.\n\nElasticsearch includes over 40 configurable processors. The subpages in this section contain reference documentation for each processor. To get a list of available processors, use the [nodes info](cluster-nodes-info.html "Nodes info API") API.\n\n```\nresp = client.nodes.info(\n    node_id="ingest",\n    filter_path="nodes.*.ingest.processors",\n)\nprint(resp)\n```\n\n```\nresponse = client.nodes.info(\n  node_id: \'ingest\',\n  filter_path: \'nodes.*.ingest.processors\'\n)\nputs response\n```\n\n```\nconst response = await client.nodes.info({\n  node_id: "ingest",\n  filter_path: "nodes.*.ingest.processors",\n});\nconsole.log(response);\n```\n\n```\nGET _nodes/ingest?filter_path=nodes.*.ingest.processors\n```\n\n### Ingest processors by category\n\nWe’ve categorized the available processors on this page and summarized their functions. This will help you find the right processor for your use case.\n\n* [Data enrichment processors](processors.html#ingest-process-category-data-enrichment "Data enrichment processors")\n* [Data transformation processors](processors.html#ingest-process-category-data-transformation "Data transformation processors")\n* [Data filtering processors](processors.html#ingest-process-category-data-filtering "Data filtering processors")\n* [Pipeline handling processors](processors.html#ingest-process-category-pipeline-handling "Pipeline handling processors")\n* [Array/JSON handling processors](processors.html#ingest-process-category-array-json-handling "Array/JSON handling processors")\n\n### Data enrichment processors\n\n#### General outcomes\n\n* [`append` processor](append-processor.html "Append processor")\n\n  Appends a value to a field.\n\n* [`date_index_name` processor](date-index-name-processor.html "Date index name processor")\n\n  Points documents to the right time-based index based on a date or timestamp field.\n\n* [`enrich` processor](enrich-processor.html "Enrich processor")\n\n  Enriches documents with data from another index.\n\nRefer to [Enrich your data](ingest-enriching-data.html "Enrich your data") for detailed examples of how to use the `enrich` processor to add data from your existing indices to incoming documents during ingest.\n\n* [`inference` processor](inference-processor.html "Inference processor")\n\n  Uses machine learning to classify and tag text fields.\n\n#### Specific outcomes\n\n* [`attachment` processor](attachment.html "Attachment processor")\n\n  Parses and indexes binary data, such as PDFs and Word documents.\n\n* [`circle` processor](ingest-circle-processor.html "Circle processor")\n\n  Converts a location field to a Geo-Point field.\n\n* [`community_id` processor](community-id-processor.html "Community ID processor")\n\n  Computes the Community ID for network flow data.\n\n* [`fingerprint` processor](fingerprint-processor.html "Fingerprint processor")\n\n  Computes a hash of the document’s content.\n\n* [`geo_grid` processor](ingest-geo-grid-processor.html "Geo-grid processor")\n\n  Converts geo-grid definitions of grid tiles or cells to regular bounding boxes or polygons which describe their shape.\n\n* [`geoip` processor](geoip-processor.html "GeoIP processor")\n\n  Adds information about the geographical location of an IPv4 or IPv6 address from a Maxmind database.\n\n* [`ip_location` processor](ip-location-processor.html "IP location processor")\n\n  Adds information about the geographical location of an IPv4 or IPv6 address from an ip geolocation database.\n\n* [`network_direction` processor](network-direction-processor.html "Network direction processor")\n\n  Calculates the network direction given a source IP address, destination IP address, and a list of internal networks.\n\n* [`registered_domain` processor](registered-domain-processor.html "Registered domain processor")\n\n  Extracts the registered domain (also known as the effective top-level domain or eTLD), sub-domain, and top-level domain from a fully qualified domain name (FQDN).\n\n* [`set_security_user` processor](ingest-node-set-security-user-processor.html "Set security user processor")\n\n  Sets user-related details (such as `username`, `roles`, `email`, `full_name`,`metadata`, `api_key`, `realm` and `authentication_type`) from the current authenticated user to the current document by pre-processing the ingest.\n\n* [`uri_parts` processor](uri-parts-processor.html "URI parts processor")\n\n  Parses a Uniform Resource Identifier (URI) string and extracts its components as an object.\n\n* [`urldecode` processor](urldecode-processor.html "URL decode processor")\n\n  URL-decodes a string.\n\n* [`user_agent` processor](user-agent-processor.html "User agent processor")\n\n  Parses user-agent strings to extract information about web clients.\n\n### Data transformation processors\n\n#### General outcomes\n\n* [`convert` processor](convert-processor.html "Convert processor")\n\n  Converts a field in the currently ingested document to a different type, such as converting a string to an integer.\n\n* [`dissect` processor](dissect-processor.html "Dissect processor")\n\n  Extracts structured fields out of a single text field within a document. Unlike the [grok processor](grok-processor.html "Grok processor"), dissect does not use regular expressions. This makes the dissect’s a simpler and often faster alternative.\n\n* [`grok` processor](grok-processor.html "Grok processor")\n\n  Extracts structured fields out of a single text field within a document, using the [Grok](grok.html "Grokking grok") regular expression dialect that supports reusable aliased expressions.\n\n* [`gsub` processor](gsub-processor.html "Gsub processor")\n\n  Converts a string field by applying a regular expression and a replacement.\n\n* [`redact` processor](redact-processor.html "Redact processor")\n\n  Uses the [Grok](grok.html "Grokking grok") rules engine to obscure text in the input document matching the given Grok patterns.\n\n* [`rename` processor](rename-processor.html "Rename processor")\n\n  Renames an existing field.\n\n* [`set` processor](set-processor.html "Set processor")\n\n  Sets a value on a field.\n\n#### Specific outcomes\n\n* [`bytes` processor](bytes-processor.html "Bytes processor")\n\n  Converts a human-readable byte value to its value in bytes (for example `1kb` becomes `1024`).\n\n* [`csv` processor](csv-processor.html "CSV processor")\n\n  Extracts a single line of CSV data from a text field.\n\n* [`date` processor](date-processor.html "Date processor")\n\n  Extracts and converts date fields.\n\n* [`dot_expand`](dot-expand-processor.html "Dot expander processor") processor\n\n  Expands a field with dots into an object field.\n\n* [`html_strip` processor](htmlstrip-processor.html "HTML strip processor")\n\n  Removes HTML tags from a field.\n\n* [`join` processor](join-processor.html "Join processor")\n\n  Joins each element of an array into a single string using a separator character between each element.\n\n* [`kv` processor](kv-processor.html "KV processor")\n\n  Parse messages (or specific event fields) containing key-value pairs.\n\n* [`lowercase` processor](lowercase-processor.html "Lowercase processor") and [`uppercase` processor](uppercase-processor.html "Uppercase processor")\n\n  Converts a string field to lowercase or uppercase.\n\n* [`split` processor](split-processor.html "Split processor")\n\n  Splits a field into an array of values.\n\n* [`trim` processor](trim-processor.html "Trim processor")\n\n  Trims whitespace from field.\n\n### Data filtering processors\n\n* [`drop` processor](drop-processor.html "Drop processor")\n\n  Drops the document without raising any errors.\n\n* [`remove` processor](remove-processor.html "Remove processor")\n\n  Removes fields from documents.\n\n### Pipeline handling processors\n\n* [`fail` processor](fail-processor.html "Fail processor")\n\n  Raises an exception. Useful for when you expect a pipeline to fail and want to relay a specific message to the requester.\n\n* [`pipeline` processor](pipeline-processor.html "Pipeline processor")\n\n  Executes another pipeline.\n\n* [`reroute` processor](reroute-processor.html "Reroute processor")\n\n  Reroutes documents to another target index or data stream.\n\n* [`terminate` processor](terminate-processor.html "Terminate processor")\n\n  Terminates the current ingest pipeline, causing no further processors to be run.\n\n### Array/JSON handling processors\n\n* [`for_each` processor](foreach-processor.html "Foreach processor")\n\n  Runs an ingest processor on each element of an array or object.\n\n* [`json` processor](json-processor.html "JSON processor")\n\n  Converts a JSON string into a structured JSON object.\n\n* [`script` processor](script-processor.html "Script processor")\n\n  Runs an inline or stored [script](modules-scripting.html "Scripting") on incoming documents. The script runs in the [painless `ingest` context](/guide/en/elasticsearch/painless/8.17/painless-ingest-processor-context.html).\n\n* [`sort` processor](sort-processor.html "Sort processor")\n\n  Sorts the elements of an array in ascending or descending order.\n\n### Add additional processors\n\nYou can install additional processors as [plugins](/guide/en/elasticsearch/plugins/8.17/ingest.html).\n\nYou must install any plugin processors on all nodes in your cluster. Otherwise, Elasticsearch will fail to create pipelines containing the processor.\n\nMark a plugin as mandatory by setting `plugin.mandatory` in `elasticsearch.yml`. A node will fail to start if a mandatory plugin is not installed.\n\n```\nplugin.mandatory: my-ingest-plugin\n```\n',
          title: 'Ingest processor reference',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/processors.html',
          productName: 'elasticsearch',
          score: 134.2469,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/processors.html',
        title: 'Ingest processor reference',
        score: 134.2469,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Ingest processor reference\\n\\nAn [ingest pipeline](ingest.html \\"Ingest pipelines\\") is made up of a sequence of processors that are applied to documents as they are ingested into an index. Each processor performs a specific task, such as filtering, transforming, or enriching data.\\n\\nEach successive processor depends on the output of the previous processor, so the order of processors is important. The modified documents are indexed into Elasticsearch after all processors are applied.\\n\\nElasticsearch includes over 40 configurable processors. The subpages in this section contain reference documentation for each processor. To get a list of available processors, use the [nodes info](cluster-nodes-info.html \\"Nodes info API\\") API.\\n\\n```\\nresp = client.nodes.info(\\n    node_id=\\"ingest\\",\\n    filter_path=\\"nodes.*.ingest.processors\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.nodes.info(\\n  node_id: \'ingest\',\\n  filter_path: \'nodes.*.ingest.processors\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.nodes.info({\\n  node_id: \\"ingest\\",\\n  filter_path: \\"nodes.*.ingest.processors\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nGET _nodes/ingest?filter_path=nodes.*.ingest.processors\\n```\\n\\n### Ingest processors by category\\n\\nWe’ve categorized the available processors on this page and summarized their functions. This will help you find the right processor for your use case.\\n\\n* [Data enrichment processors](processors.html#ingest-process-category-data-enrichment \\"Data enrichment processors\\")\\n* [Data transformation processors](processors.html#ingest-process-category-data-transformation \\"Data transformation processors\\")\\n* [Data filtering processors](processors.html#ingest-process-category-data-filtering \\"Data filtering processors\\")\\n* [Pipeline handling processors](processors.html#ingest-process-category-pipeline-handling \\"Pipeline handling processors\\")\\n* [Array/JSON handling processors](processors.html#ingest-process-category-array-json-handling \\"Array/JSON handling processors\\")\\n\\n### Data enrichment processors\\n\\n#### General outcomes\\n\\n* [`append` processor](append-processor.html \\"Append processor\\")\\n\\n  Appends a value to a field.\\n\\n* [`date_index_name` processor](date-index-name-processor.html \\"Date index name processor\\")\\n\\n  Points documents to the right time-based index based on a date or timestamp field.\\n\\n* [`enrich` processor](enrich-processor.html \\"Enrich processor\\")\\n\\n  Enriches documents with data from another index.\\n\\nRefer to [Enrich your data](ingest-enriching-data.html \\"Enrich your data\\") for detailed examples of how to use the `enrich` processor to add data from your existing indices to incoming documents during ingest.\\n\\n* [`inference` processor](inference-processor.html \\"Inference processor\\")\\n\\n  Uses machine learning to classify and tag text fields.\\n\\n#### Specific outcomes\\n\\n* [`attachment` processor](attachment.html \\"Attachment processor\\")\\n\\n  Parses and indexes binary data, such as PDFs and Word documents.\\n\\n* [`circle` processor](ingest-circle-processor.html \\"Circle processor\\")\\n\\n  Converts a location field to a Geo-Point field.\\n\\n* [`community_id` processor](community-id-processor.html \\"Community ID processor\\")\\n\\n  Computes the Community ID for network flow data.\\n\\n* [`fingerprint` processor](fingerprint-processor.html \\"Fingerprint processor\\")\\n\\n  Computes a hash of the document’s content.\\n\\n* [`geo_grid` processor](ingest-geo-grid-processor.html \\"Geo-grid processor\\")\\n\\n  Converts geo-grid definitions of grid tiles or cells to regular bounding boxes or polygons which describe their shape.\\n\\n* [`geoip` processor](geoip-processor.html \\"GeoIP processor\\")\\n\\n  Adds information about the geographical location of an IPv4 or IPv6 address from a Maxmind database.\\n\\n* [`ip_location` processor](ip-location-processor.html \\"IP location processor\\")\\n\\n  Adds information about the geographical location of an IPv4 or IPv6 address from an ip geolocation database.\\n\\n* [`network_direction` processor](network-direction-processor.html \\"Network direction processor\\")\\n\\n  Calculates the network direction given a source IP address, destination IP address, and a list of internal networks.\\n\\n* [`registered_domain` processor](registered-domain-processor.html \\"Registered domain processor\\")\\n\\n  Extracts the registered domain (also known as the effective top-level domain or eTLD), sub-domain, and top-level domain from a fully qualified domain name (FQDN).\\n\\n* [`set_security_user` processor](ingest-node-set-security-user-processor.html \\"Set security user processor\\")\\n\\n  Sets user-related details (such as `username`, `roles`, `email`, `full_name`,`metadata`, `api_key`, `realm` and `authentication_type`) from the current authenticated user to the current document by pre-processing the ingest.\\n\\n* [`uri_parts` processor](uri-parts-processor.html \\"URI parts processor\\")\\n\\n  Parses a Uniform Resource Identifier (URI) string and extracts its components as an object.\\n\\n* [`urldecode` processor](urldecode-processor.html \\"URL decode processor\\")\\n\\n  URL-decodes a string.\\n\\n* [`user_agent` processor](user-agent-processor.html \\"User agent processor\\")\\n\\n  Parses user-agent strings to extract information about web clients.\\n\\n### Data transformation processors\\n\\n#### General outcomes\\n\\n* [`convert` processor](convert-processor.html \\"Convert processor\\")\\n\\n  Converts a field in the currently ingested document to a different type, such as converting a string to an integer.\\n\\n* [`dissect` processor](dissect-processor.html \\"Dissect processor\\")\\n\\n  Extracts structured fields out of a single text field within a document. Unlike the [grok processor](grok-processor.html \\"Grok processor\\"), dissect does not use regular expressions. This makes the dissect’s a simpler and often faster alternative.\\n\\n* [`grok` processor](grok-processor.html \\"Grok processor\\")\\n\\n  Extracts structured fields out of a single text field within a document, using the [Grok](grok.html \\"Grokking grok\\") regular expression dialect that supports reusable aliased expressions.\\n\\n* [`gsub` processor](gsub-processor.html \\"Gsub processor\\")\\n\\n  Converts a string field by applying a regular expression and a replacement.\\n\\n* [`redact` processor](redact-processor.html \\"Redact processor\\")\\n\\n  Uses the [Grok](grok.html \\"Grokking grok\\") rules engine to obscure text in the input document matching the given Grok patterns.\\n\\n* [`rename` processor](rename-processor.html \\"Rename processor\\")\\n\\n  Renames an existing field.\\n\\n* [`set` processor](set-processor.html \\"Set processor\\")\\n\\n  Sets a value on a field.\\n\\n#### Specific outcomes\\n\\n* [`bytes` processor](bytes-processor.html \\"Bytes processor\\")\\n\\n  Converts a human-readable byte value to its value in bytes (for example `1kb` becomes `1024`).\\n\\n* [`csv` processor](csv-processor.html \\"CSV processor\\")\\n\\n  Extracts a single line of CSV data from a text field.\\n\\n* [`date` processor](date-processor.html \\"Date processor\\")\\n\\n  Extracts and converts date fields.\\n\\n* [`dot_expand`](dot-expand-processor.html \\"Dot expander processor\\") processor\\n\\n  Expands a field with dots into an object field.\\n\\n* [`html_strip` processor](htmlstrip-processor.html \\"HTML strip processor\\")\\n\\n  Removes HTML tags from a field.\\n\\n* [`join` processor](join-processor.html \\"Join processor\\")\\n\\n  Joins each element of an array into a single string using a separator character between each element.\\n\\n* [`kv` processor](kv-processor.html \\"KV processor\\")\\n\\n  Parse messages (or specific event fields) containing key-value pairs.\\n\\n* [`lowercase` processor](lowercase-processor.html \\"Lowercase processor\\") and [`uppercase` processor](uppercase-processor.html \\"Uppercase processor\\")\\n\\n  Converts a string field to lowercase or uppercase.\\n\\n* [`split` processor](split-processor.html \\"Split processor\\")\\n\\n  Splits a field into an array of values.\\n\\n* [`trim` processor](trim-processor.html \\"Trim processor\\")\\n\\n  Trims whitespace from field.\\n\\n### Data filtering processors\\n\\n* [`drop` processor](drop-processor.html \\"Drop processor\\")\\n\\n  Drops the document without raising any errors.\\n\\n* [`remove` processor](remove-processor.html \\"Remove processor\\")\\n\\n  Removes fields from documents.\\n\\n### Pipeline handling processors\\n\\n* [`fail` processor](fail-processor.html \\"Fail processor\\")\\n\\n  Raises an exception. Useful for when you expect a pipeline to fail and want to relay a specific message to the requester.\\n\\n* [`pipeline` processor](pipeline-processor.html \\"Pipeline processor\\")\\n\\n  Executes another pipeline.\\n\\n* [`reroute` processor](reroute-processor.html \\"Reroute processor\\")\\n\\n  Reroutes documents to another target index or data stream.\\n\\n* [`terminate` processor](terminate-processor.html \\"Terminate processor\\")\\n\\n  Terminates the current ingest pipeline, causing no further processors to be run.\\n\\n### Array/JSON handling processors\\n\\n* [`for_each` processor](foreach-processor.html \\"Foreach processor\\")\\n\\n  Runs an ingest processor on each element of an array or object.\\n\\n* [`json` processor](json-processor.html \\"JSON processor\\")\\n\\n  Converts a JSON string into a structured JSON object.\\n\\n* [`script` processor](script-processor.html \\"Script processor\\")\\n\\n  Runs an inline or stored [script](modules-scripting.html \\"Scripting\\") on incoming documents. The script runs in the [painless `ingest` context](/guide/en/elasticsearch/painless/8.17/painless-ingest-processor-context.html).\\n\\n* [`sort` processor](sort-processor.html \\"Sort processor\\")\\n\\n  Sorts the elements of an array in ascending or descending order.\\n\\n### Add additional processors\\n\\nYou can install additional processors as [plugins](/guide/en/elasticsearch/plugins/8.17/ingest.html).\\n\\nYou must install any plugin processors on all nodes in your cluster. Otherwise, Elasticsearch will fail to create pipelines containing the processor.\\n\\nMark a plugin as mandatory by setting `plugin.mandatory` in `elasticsearch.yml`. A node will fail to start if a mandatory plugin is not installed.\\n\\n```\\nplugin.mandatory: my-ingest-plugin\\n```\\n","title":"Ingest processor reference","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/processors.html","productName":"elasticsearch","score":134.2469}',
        truncated: {
          truncatedText:
            '{"content":"## Ingest processor reference\\n\\nAn [ingest pipeline](ingest.html \\"Ingest pipelines\\") is made up of a sequence of processors that are applied to documents as they are ingested into an index. Each processor performs a specific task, such as filtering, transforming, or enriching data.\\n\\nEach successive processor depends on the output of the previous processor, so the order of processors is important. The modified documents are indexed into Elasticsearch after all processors are applied.\\n\\nElasticsearch includes over 40 configurable processors. The subpages in this section contain reference documentation for each processor. To get a list of available processors, use the [nodes info](cluster-nodes-info.html \\"Nodes info API\\") API.\\n\\n```\\nresp = client.nodes.info(\\n    node_id=\\"ingest\\",\\n    filter_path=\\"nodes.*.ingest.processors\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.nodes.info(\\n  node_id: \'ingest\',\\n  filter_path: \'nodes.*.ingest.processors\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.nodes.info({\\n  node_id: \\"ingest\\",\\n  filter_path: \\"nodes.*.ingest.processors\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nGET _nodes/ingest?filter_path=nodes.*.ingest.processors\\n```\\n\\n### Ingest processors by category\\n\\nWe’ve categorized the available processors on this page and summarized their functions. This will help you find the right processor for your use case.\\n\\n* [Data enrichment processors](processors.html#ingest-process-category-data-enrichment \\"Data enrichment processors\\")\\n* [Data transformation processors](processors.html#ingest-process-category-data-transformation \\"Data transformation processors\\")\\n* [Data filtering processors](processors... <truncated>',
          originalTokenCount: 2433,
          truncatedTokenCount: 400,
        },
        llmScore: 5,
      },
      {
        selected: false,
        document: {
          author: {
            login: 'EricDavisX',
          },
          issue_comments: [
            {
              author: {
                login: 'EricDavisX',
              },
              body: 'Ivan commented more in chat:\r\n\r\nIvan Fernandez Calvo:palm_tree:  8 minutes ago\r\n\r\nit is not a datastream, alias, or pipeline\r\n\r\nthis is the error in Kibana logs\r\n\tError: Internal Server Error\r\n    at HapiResponseAdapter.toError (/usr/share/kibana/src/core/server/http/router/response_adapter.js:132:19)\r\n    at HapiResponseAdapter.toHapiResponse (/usr/share/kibana/src/core/server/http/router/response_adapter.js:86:19)\r\n    at HapiResponseAdapter.handle (/usr/share/kibana/src/core/server/http/router/response_adapter.js:81:17)\r\n    at Router.handle (/usr/share/kibana/src/core/server/http/router/router.js:162:34)\r\n    at process._tickCallback (internal/process/next_tick.js:68:7)\r\n\r\non Elasticsearch there is no error',
            },
            {
              author: {
                login: 'cachedout',
              },
              body: 'This looks like the issue we already addressed earlier today: https://github.com/elastic/observability-test-environments/issues/791\r\n\r\nCould you please confirm that this is working for you now?',
            },
            {
              author: {
                login: 'EricDavisX',
              },
              body: 'it is working - thanks much.  closing.',
            },
          ],
          title:
            'https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Ingest Manager [process_cluster_event_timeout_exception] failed to process cluster event (put-pipeline-logs-system.syslog-0.5.0) within 30s',
          body: 'https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Ingest Manager [process_cluster_event_timeout_exception] failed to process cluster event (put-pipeline-logs-system.syslog-0.5.0) within 30s I don\'t know what this means or what that pipeline-logs-system is. Ivan chimed in to chat that he was not sure either. screenshot: <img width="1318" alt="ingest-not-available-on-oblt-release" src="https://user-images.githubusercontent.com/12970373/89072754-2d0a2600-d347-11ea-9640-d07446624544.png"> While we\'re researching, can we get any Ingest Manager team input to the location below about how to operate / fix / setup the test clusters, its not mentioned. https://github.com/elastic/observability-dev/blob/master/docs/observability-clusters.md Further on that, I believe that Marcin recently set up a \'staging\' packages branch on the release server, we could document that so everyone knows and we know how to update / reset it as / when needed.',
          type: 'Issue',
          url: 'https://github.com/elastic/observability-dev/issues/1043',
          number: 1043,
          createdAt: '2020-07-31T20:02:34Z',
          assignees_list: [],
          labels_field: [],
          state: 'CLOSED',
          id: 'MDU6SXNzdWU2NzAxNzM4Mjg=',
          closedAt: '2020-08-03T15:20:29Z',
          _timestamp: '2020-08-03T15:20:30Z',
        },
        id: 'search-observability-dev/MDU6SXNzdWU2NzAxNzM4Mjg=',
        title:
          'https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Inge...',
        score: 131.5157,
        source: {
          connector: {
            id: 'Btf5dJUB0NMvlRPXS6A0',
            description: '',
            name: 'search-observability-dev',
            service_type: 'github',
            status: 'connected',
            index_name: 'content-search-observability-dev',
          },
        },
        text: '{"author":{"login":"EricDavisX"},"issue_comments":[{"author":{"login":"EricDavisX"},"body":"Ivan commented more in chat:\\r\\n\\r\\nIvan Fernandez Calvo:palm_tree:  8 minutes ago\\r\\n\\r\\nit is not a datastream, alias, or pipeline\\r\\n\\r\\nthis is the error in Kibana logs\\r\\n\\tError: Internal Server Error\\r\\n    at HapiResponseAdapter.toError (/usr/share/kibana/src/core/server/http/router/response_adapter.js:132:19)\\r\\n    at HapiResponseAdapter.toHapiResponse (/usr/share/kibana/src/core/server/http/router/response_adapter.js:86:19)\\r\\n    at HapiResponseAdapter.handle (/usr/share/kibana/src/core/server/http/router/response_adapter.js:81:17)\\r\\n    at Router.handle (/usr/share/kibana/src/core/server/http/router/router.js:162:34)\\r\\n    at process._tickCallback (internal/process/next_tick.js:68:7)\\r\\n\\r\\non Elasticsearch there is no error"},{"author":{"login":"cachedout"},"body":"This looks like the issue we already addressed earlier today: https://github.com/elastic/observability-test-environments/issues/791\\r\\n\\r\\nCould you please confirm that this is working for you now?"},{"author":{"login":"EricDavisX"},"body":"it is working - thanks much.  closing."}],"title":"https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Ingest Manager [process_cluster_event_timeout_exception] failed to process cluster event (put-pipeline-logs-system.syslog-0.5.0) within 30s","body":"https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Ingest Manager [process_cluster_event_timeout_exception] failed to process cluster event (put-pipeline-logs-system.syslog-0.5.0) within 30s I don\'t know what this means or what that pipeline-logs-system is. Ivan chimed in to chat that he was not sure either. screenshot: <img width=\\"1318\\" alt=\\"ingest-not-available-on-oblt-release\\" src=\\"https://user-images.githubusercontent.com/12970373/89072754-2d0a2600-d347-11ea-9640-d07446624544.png\\"> While we\'re researching, can we get any Ingest Manager team input to the location below about how to operate / fix / setup the test clusters, its not mentioned. https://github.com/elastic/observability-dev/blob/master/docs/observability-clusters.md Further on that, I believe that Marcin recently set up a \'staging\' packages branch on the release server, we could document that so everyone knows and we know how to update / reset it as / when needed.","type":"Issue","url":"https://github.com/elastic/observability-dev/issues/1043","number":1043,"createdAt":"2020-07-31T20:02:34Z","assignees_list":[],"labels_field":[],"state":"CLOSED","id":"MDU6SXNzdWU2NzAxNzM4Mjg=","closedAt":"2020-08-03T15:20:29Z","_timestamp":"2020-08-03T15:20:30Z"}',
        truncated: {
          truncatedText:
            '{"author":{"login":"EricDavisX"},"issue_comments":[{"author":{"login":"EricDavisX"},"body":"Ivan commented more in chat:\\r\\n\\r\\nIvan Fernandez Calvo:palm_tree:  8 minutes ago\\r\\n\\r\\nit is not a datastream, alias, or pipeline\\r\\n\\r\\nthis is the error in Kibana logs\\r\\n\\tError: Internal Server Error\\r\\n    at HapiResponseAdapter.toError (/usr/share/kibana/src/core/server/http/router/response_adapter.js:132:19)\\r\\n    at HapiResponseAdapter.toHapiResponse (/usr/share/kibana/src/core/server/http/router/response_adapter.js:86:19)\\r\\n    at HapiResponseAdapter.handle (/usr/share/kibana/src/core/server/http/router/response_adapter.js:81:17)\\r\\n    at Router.handle (/usr/share/kibana/src/core/server/http/router/router.js:162:34)\\r\\n    at process._tickCallback (internal/process/next_tick.js:68:7)\\r\\n\\r\\non Elasticsearch there is no error"},{"author":{"login":"cachedout"},"body":"This looks like the issue we already addressed earlier today: https://github.com/elastic/observability-test-environments/issues/791\\r\\n\\r\\nCould you please confirm that this is working for you now?"},{"author":{"login":"EricDavisX"},"body":"it is working - thanks much.  closing."}],"title":"https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Ingest Manager [process_cluster_event_timeout_exception] failed to process cluster event (put-pipeline-logs-system.syslog-0.5.0) within 30s","body":"https://release-oblt.elastic.dev Ingest Manager view is broken with error: Unable to initialize Ingest Manager [process_cluster_event_timeout_exception] failed to process cluster event (put-pipeline-... <truncated>',
          originalTokenCount: 712,
          truncatedTokenCount: 400,
        },
        llmScore: 1,
      },
      {
        selected: false,
        document: {
          content:
            '## Parse and organize logs\n\nIf your log data is unstructured or semi-structured, you can parse it and break it into meaningful fields. You can use those fields to explore and analyze your data. For example, you can find logs within a specific timestamp range or filter logs by log level to focus on potential issues.\n\nAfter parsing, you can use the structured fields to further organize your logs by configuring a reroute processor to send specific logs to different target data streams.\n\nRefer to the following sections for more on parsing and organizing your log data:\n\n* [Extract structured fields](logs-parse.html#logs-stream-parse "Extract structured fields"): Extract structured fields like timestamps, log levels, or IP addresses to make querying and filtering your data easier.\n* [Reroute log data to specific data streams](logs-parse.html#logs-stream-reroute "Reroute log data to specific data streams"): Route data from the generic data stream to a target data stream for more granular control over data retention, permissions, and processing.\n\n### Extract structured fields\n\nMake your logs more useful by extracting structured fields from your unstructured log data. Extracting structured fields makes it easier to search, analyze, and filter your log data.\n\nFollow the steps below to see how the following unstructured log data is indexed by default:\n\n```\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\n```\n\nStart by storing the document in the `logs-example-default` data stream:\n\n1. To open **Console**, find `Dev Tools` in the [global search field](/guide/en/kibana/8.17/introduction.html#kibana-navigation-search).\n\n2. In the **Console** tab, add the example log to Elasticsearch using the following command:\n\n   ```\n   POST logs-example-default/_doc\n   {\n     "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."\n   }\n   ```\n\n3. Then, you can retrieve the document with the following search:\n\n   ```\n   GET /logs-example-default/_search\n   ```\n\nThe results should look like this:\n\n```\n{\n  ...\n  "hits": {\n    ...\n    "hits": [\n      {\n        "_index": ".ds-logs-example-default-2023.08.09-000001",\n        ...\n        "_source": {\n          "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.",\n          "@timestamp": "2023-08-09T17:19:27.73312243Z"\n        }\n      }\n    ]\n  }\n}\n```\n\nElasticsearch indexes the `message` field by default and adds a `@timestamp` field. Since there was no timestamp set, it’s set to `now`. At this point, you can search for phrases in the `message` field like `WARN` or `Disk usage exceeds`. For example, use the following command to search for the phrase `WARN` in the log’s `message` field:\n\n```\nGET logs-example-default/_search\n{\n  "query": {\n    "match": {\n      "message": {\n        "query": "WARN"\n      }\n    }\n  }\n}\n```\n\nWhile you can search for phrases in the `message` field, you can’t use this field to filter log data. Your message, however, contains all of the following potential fields you can extract and use to filter and aggregate your log data:\n\n* **@timestamp** (`2023-08-08T13:45:12.123Z`): Extracting this field lets you sort logs by date and time. This is helpful when you want to view your logs in the order that they occurred or identify when issues happened.\n* **log.level** (`WARN`): Extracting this field lets you filter logs by severity. This is helpful if you want to focus on high-severity WARN or ERROR-level logs, and reduce noise by filtering out low-severity INFO-level logs.\n* **host.ip** (`192.168.1.101`): Extracting this field lets you filter logs by the host IP addresses. This is helpful if you want to focus on specific hosts that you’re having issues with or if you want to find disparities between hosts.\n* **message** (`Disk usage exceeds 90%.`): You can search for phrases or words in the message field.\n\nThese fields are part of the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16/ecs-reference.html). The ECS defines a common set of fields that you can use across Elasticsearch when storing data, including log and metric data.\n\n#### Extract the `@timestamp` field\n\nWhen you added the log to Elasticsearch in the previous section, the `@timestamp` field showed when the log was added. The timestamp showing when the log actually occurred was in the unstructured `message` field:\n\n```\n        ...\n        "_source": {\n          "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.",\n          "@timestamp": "2023-08-09T17:19:27.73312243Z"\n        }\n        ...\n```\n\n|                |                                                                                        |\n| :------------- | :------------------------------------------------------------------------------------- |\n| [**](#CO105-1) | The timestamp in the `message` field shows when the log occurred.                      |\n| [**](#CO105-2) | The timestamp in the `@timestamp` field shows when the log was added to Elasticsearch. |\n\nWhen looking into issues, you want to filter for logs by when the issue occurred not when the log was added to your project. To do this, extract the timestamp from the unstructured `message` field to the structured `@timestamp` field by completing the following:\n\n1. [Use an ingest pipeline to extract the `@timestamp` field](logs-parse.html#logs-stream-ingest-pipeline "Use an ingest pipeline to extract the @timestamp field")\n2. [Test the pipeline with the simulate pipeline API](logs-parse.html#logs-stream-simulate-api "Test the pipeline with the simulate pipeline API")\n3. [Configure a data stream with an index template](logs-parse.html#logs-stream-index-template "Configure a data stream with an index template")\n4. [Create a data stream](logs-parse.html#logs-stream-create-data-stream "Create a data stream")\n\n##### Use an ingest pipeline to extract the `@timestamp` field\n\nIngest pipelines consist of a series of processors that perform common transformations on incoming documents before they are indexed. To extract the `@timestamp` field from the example log, use an ingest pipeline with a dissect processor. The [dissect processor](/guide/en/elasticsearch/reference/8.17/dissect-processor.html) extracts structured fields from unstructured log messages based on a pattern you set.\n\nElasticsearch can parse string timestamps that are in `yyyy-MM-dd\'T\'HH:mm:ss.SSSZ` and `yyyy-MM-dd` formats into date fields. Since the log example’s timestamp is in one of these formats, you don’t need additional processors. More complex or nonstandard timestamps require a [date processor](/guide/en/elasticsearch/reference/8.17/date-processor.html) to parse the timestamp into a date field.\n\nUse the following command to extract the timestamp from the `message` field into the `@timestamp` field:\n\n```\nPUT _ingest/pipeline/logs-example-default\n{\n  "description": "Extracts the timestamp",\n  "processors": [\n    {\n      "dissect": {\n        "field": "message",\n        "pattern": "%{@timestamp} %{message}"\n      }\n    }\n  ]\n}\n```\n\n|                |                                                                                                                                                                                                                                                                                                                                                                 |\n| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [**](#CO106-1) | The name of the pipeline,`logs-example-default`, needs to match the name of your data stream. You’ll set up your data stream in the next section. For more information, refer to the [data stream naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme).                                                                            |\n| [**](#CO106-2) | The field you’re extracting data from, `message` in this case.                                                                                                                                                                                                                                                                                                  |\n| [**](#CO106-3) | The pattern of the elements in your log data. The `%{@timestamp} %{message}` pattern extracts the timestamp, `2023-08-08T13:45:12.123Z`, to the `@timestamp` field, while the rest of the message, `WARN 192.168.1.101 Disk usage exceeds 90%.`, stays in the `message` field. The dissect processor looks for the space as a separator defined by the pattern. |\n\n##### Test the pipeline with the simulate pipeline API\n\nThe [simulate pipeline API](/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html#ingest-verbose-param) runs the ingest pipeline without storing any documents. This lets you verify your pipeline works using multiple documents. Run the following command to test your ingest pipeline with the simulate pipeline API.\n\n```\nPOST _ingest/pipeline/logs-example-default/_simulate\n{\n  "docs": [\n    {\n      "_source": {\n        "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."\n      }\n    }\n  ]\n}\n```\n\nThe results should show the `@timestamp` field extracted from the `message` field:\n\n```\n{\n  "docs": [\n    {\n      "doc": {\n        "_index": "_index",\n        "_id": "_id",\n        "_version": "-3",\n        "_source": {\n          "message": "WARN 192.168.1.101 Disk usage exceeds 90%.",\n          "@timestamp": "2023-08-08T13:45:12.123Z"\n        },\n        ...\n      }\n    }\n  ]\n}\n```\n\nMake sure you’ve created the ingest pipeline using the `PUT` command in the previous section before using the simulate pipeline API.\n\n##### Configure a data stream with an index template\n\nAfter creating your ingest pipeline, run the following command to create an index template to configure your data stream’s backing indices:\n\n```\nPUT _index_template/logs-example-default-template\n{\n  "index_patterns": [ "logs-example-*" ],\n  "data_stream": { },\n  "priority": 500,\n  "template": {\n    "settings": {\n      "index.default_pipeline":"logs-example-default"\n    }\n  },\n  "composed_of": [\n    "logs@mappings",\n    "logs@settings",\n    "logs@custom",\n    "ecs@mappings"\n  ],\n  "ignore_missing_component_templates": ["logs@custom"]\n}\n```\n\n|                |                                                                                                                                                                                                                                                                                                                                                    |\n| :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [**](#CO107-1) | `index_pattern`: Needs to match your log data stream. Naming conventions for data streams are `<type>-<dataset>-<namespace>`. In this example, your logs data stream is named `logs-example-*`. Data that matches this pattern will go through your pipeline.                                                                                      |\n| [**](#CO107-2) | `data_stream`: Enables data streams.                                                                                                                                                                                                                                                                                                               |\n| [**](#CO107-3) | `priority`: Sets the priority of you Index Template. Index templates with higher priority take precedence over lower priority. If a data stream matches multiple index templates, Elasticsearch uses the template with the higher priority. Built-in templates have a priority of `200`, so use a priority higher than `200` for custom templates. |\n| [**](#CO107-4) | `index.default_pipeline`: The name of your ingest pipeline. `logs-example-default` in this case.                                                                                                                                                                                                                                                   |\n| [**](#CO107-5) | `composed_of`: Here you can set component templates. Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. Elastic has several built-in templates to help when ingesting your log data.                                                                                     |\n\nThe example index template above sets the following component templates:\n\n* `logs@mappings`: general mappings for log data streams that include disabling automatic date detection from `string` fields and specifying mappings for [`data_stream` ECS fields](/guide/en/ecs/8.16/ecs-data_stream.html).\n\n* `logs@settings`: general settings for log data streams including the following:\n\n  * The default lifecycle policy that rolls over when the primary shard reaches 50 GB or after 30 days.\n  * The default pipeline uses the ingest timestamp if there is no specified `@timestamp` and places a hook for the `logs@custom` pipeline. If a `logs@custom` pipeline is installed, it’s applied to logs ingested into this data stream.\n  * Sets the [`ignore_malformed`](/guide/en/elasticsearch/reference/8.17/ignore-malformed.html) flag to `true`. When ingesting a large batch of log data, a single malformed field like an IP address can cause the entire batch to fail. When set to true, malformed fields with a mapping type that supports this flag are still processed.\n\n* `logs@custom`: a predefined component template that is not installed by default. Use this name to install a custom component template to override or extend any of the default mappings or settings.\n\n* `ecs@mappings`: dynamic templates that automatically ensure your data stream mappings comply with the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16/ecs-reference.html).\n\n##### Create a data stream\n\nCreate your data stream using the [data stream naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme). Name your data stream to match the name of your ingest pipeline, `logs-example-default` in this case. Post the example log to your data stream with this command:\n\n```\nPOST logs-example-default/_doc\n{\n  "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."\n}\n```\n\nView your documents using this command:\n\n```\nGET /logs-example-default/_search\n```\n\nYou should see the pipeline has extracted the `@timestamp` field:\n\n```\n{\n...\n{\n  ...\n  "hits": {\n    ...\n    "hits": [\n      {\n        "_index": ".ds-logs-example-default-2023.08.09-000001",\n        "_id": "RsWy3IkB8yCtA5VGOKLf",\n        "_score": 1,\n        "_source": {\n          "message": "WARN 192.168.1.101 Disk usage exceeds 90%.",\n          "@timestamp": "2023-08-08T13:45:12.123Z"\n        }\n      }\n    ]\n  }\n}\n```\n\n|                |                                   |\n| :------------- | :-------------------------------- |\n| [**](#CO108-1) | The extracted `@timestamp` field. |\n\nYou can now use the `@timestamp` field to sort your logs by the date and time they happened.\n\n##### Troubleshoot the `@timestamp` field\n\nCheck the following common issues and solutions with timestamps:\n\n* **Timestamp failure**: If your data has inconsistent date formats, set `ignore_failure` to `true` for your date processor. This processes logs with correctly formatted dates and ignores those with issues.\n* **Incorrect timezone**: Set your timezone using the `timezone` option on the [date processor](/guide/en/elasticsearch/reference/8.17/date-processor.html).\n* **Incorrect timestamp format**: Your timestamp can be a Java time pattern or one of the following formats: ISO8601, UNIX, UNIX\\_MS, or TAI64N. For more information on timestamp formats, refer to the [mapping date format](/guide/en/elasticsearch/reference/8.17/mapping-date-format.html).\n\n#### Extract the `log.level` field\n\nExtracting the `log.level` field lets you filter by severity and focus on critical issues. This section shows you how to extract the `log.level` field from this example log:\n\n```\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\n```\n\nTo extract and use the `log.level` field:\n\n1. [Add the `log.level` field to the dissect processor pattern in your ingest pipeline.](logs-parse.html#logs-stream-log-level-pipeline "Add log.level to your ingest pipeline")\n2. [Test the pipeline with the simulate API.](logs-parse.html#logs-stream-log-level-simulate "Test the pipeline with the simulate API")\n3. [Query your logs based on the `log.level` field.](logs-parse.html#logs-stream-log-level-query "Query logs based on log.level")\n\n##### Add `log.level` to your ingest pipeline\n\nAdd the `%{log.level}` option to the dissect processor pattern in the ingest pipeline you created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-ingest-pipeline "Use an ingest pipeline to extract the @timestamp field") section with this command:\n\n```\nPUT _ingest/pipeline/logs-example-default\n{\n  "description": "Extracts the timestamp and log level",\n  "processors": [\n    {\n      "dissect": {\n        "field": "message",\n        "pattern": "%{@timestamp} %{log.level} %{message}"\n      }\n    }\n  ]\n}\n```\n\nNow your pipeline will extract these fields:\n\n* The `@timestamp` field: `2023-08-08T13:45:12.123Z`\n* The `log.level` field: `WARN`\n* The `message` field: `192.168.1.101 Disk usage exceeds 90%.`\n\nIn addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-index-template "Configure a data stream with an index template") section.\n\n##### Test the pipeline with the simulate API\n\nTest that your ingest pipeline works as expected with the [simulate pipeline API](/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html#ingest-verbose-param):\n\n```\nPOST _ingest/pipeline/logs-example-default/_simulate\n{\n  "docs": [\n    {\n      "_source": {\n        "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."\n      }\n    }\n  ]\n}\n```\n\nThe results should show the `@timestamp` and the `log.level` fields extracted from the `message` field:\n\n```\n{\n  "docs": [\n    {\n      "doc": {\n        "_index": "_index",\n        "_id": "_id",\n        "_version": "-3",\n        "_source": {\n          "message": "192.168.1.101 Disk usage exceeds 90%.",\n          "log": {\n            "level": "WARN"\n          },\n          "@timestamp": "2023-8-08T13:45:12.123Z",\n        },\n        ...\n      }\n    }\n  ]\n}\n```\n\n##### Query logs based on `log.level`\n\nOnce you’ve extracted the `log.level` field, you can query for high-severity logs like `WARN` and `ERROR`, which may need immediate attention, and filter out less critical `INFO` and `DEBUG` logs.\n\nLet’s say you have the following logs with varying severities:\n\n```\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\n2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\n2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\n2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\n```\n\nAdd them to your data stream using this command:\n\n```\nPOST logs-example-default/_bulk\n{ "create": {} }\n{ "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture." }\n```\n\nThen, query for documents with a log level of `WARN` or `ERROR` with this command:\n\n```\nGET logs-example-default/_search\n{\n  "query": {\n    "terms": {\n      "log.level": ["WARN", "ERROR"]\n    }\n  }\n}\n```\n\nThe results should show only the high-severity logs:\n\n```\n{\n...\n  },\n  "hits": {\n  ...\n    "hits": [\n      {\n        "_index": ".ds-logs-example-default-2023.08.14-000001",\n        "_id": "3TcZ-4kB3FafvEVY4yKx",\n        "_score": 1,\n        "_source": {\n          "message": "192.168.1.101 Disk usage exceeds 90%.",\n          "log": {\n            "level": "WARN"\n          },\n          "@timestamp": "2023-08-08T13:45:12.123Z"\n        }\n      },\n      {\n        "_index": ".ds-logs-example-default-2023.08.14-000001",\n        "_id": "3jcZ-4kB3FafvEVY4yKx",\n        "_score": 1,\n        "_source": {\n          "message": "192.168.1.103 Database connection failed.",\n          "log": {\n            "level": "ERROR"\n          },\n          "@timestamp": "2023-08-08T13:45:14.003Z"\n        }\n      }\n    ]\n  }\n}\n```\n\n#### Extract the `host.ip` field\n\nExtracting the `host.ip` field lets you filter logs by host IP addresses allowing you to focus on specific hosts that you’re having issues with or find disparities between hosts.\n\nThe `host.ip` field is part of the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16/ecs-reference.html). Through the ECS, the `host.ip` field is mapped as an [`ip` field type](/guide/en/elasticsearch/reference/8.17/ip.html). `ip` field types allow range queries so you can find logs with IP addresses in a specific range. You can also query `ip` field types using Classless Inter-Domain Routing (CIDR) notation to find logs from a particular network or subnet.\n\nThis section shows you how to extract the `host.ip` field from the following example logs and query based on the extracted fields:\n\n```\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\n2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\n2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\n2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\n```\n\nTo extract and use the `host.ip` field:\n\n1. [Add the `host.ip` field to your dissect processor in your ingest pipeline.](logs-parse.html#logs-stream-host-ip-pipeline "Add host.ip to your ingest pipeline")\n2. [Test the pipeline with the simulate API.](logs-parse.html#logs-stream-host-ip-simulate "Test the pipeline with the simulate API")\n3. [Query your logs based on the `host.ip` field.](logs-parse.html#logs-stream-host-ip-query "Query logs based on host.ip")\n\n##### Add `host.ip` to your ingest pipeline\n\nAdd the `%{host.ip}` option to the dissect processor pattern in the ingest pipeline you created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-ingest-pipeline "Use an ingest pipeline to extract the @timestamp field") section:\n\n```\nPUT _ingest/pipeline/logs-example-default\n{\n  "description": "Extracts the timestamp log level and host ip",\n  "processors": [\n    {\n      "dissect": {\n        "field": "message",\n        "pattern": "%{@timestamp} %{log.level} %{host.ip} %{message}"\n      }\n    }\n  ]\n}\n```\n\nYour pipeline will extract these fields:\n\n* The `@timestamp` field: `2023-08-08T13:45:12.123Z`\n* The `log.level` field: `WARN`\n* The `host.ip` field: `192.168.1.101`\n* The `message` field: `Disk usage exceeds 90%.`\n\nIn addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-index-template "Configure a data stream with an index template") section.\n\n##### Test the pipeline with the simulate API\n\nTest that your ingest pipeline works as expected with the [simulate pipeline API](/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html#ingest-verbose-param):\n\n```\nPOST _ingest/pipeline/logs-example-default/_simulate\n{\n  "docs": [\n    {\n      "_source": {\n        "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."\n      }\n    }\n  ]\n}\n```\n\nThe results should show the `host.ip`, `@timestamp`, and `log.level` fields extracted from the `message` field:\n\n```\n{\n  "docs": [\n    {\n      "doc": {\n        ...\n        "_source": {\n          "host": {\n            "ip": "192.168.1.101"\n          },\n          "@timestamp": "2023-08-08T13:45:12.123Z",\n          "message": "Disk usage exceeds 90%.",\n          "log": {\n            "level": "WARN"\n          }\n        },\n        ...\n      }\n    }\n  ]\n}\n```\n\n##### Query logs based on `host.ip`\n\nYou can query your logs based on the `host.ip` field in different ways, including using CIDR notation and range queries.\n\nBefore querying your logs, add them to your data stream using this command:\n\n```\nPOST logs-example-default/_bulk\n{ "create": {} }\n{ "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture." }\n```\n\n###### CIDR notation\n\nYou can use [CIDR notation](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation) to query your log data using a block of IP addresses that fall within a certain network segment. CIDR notations uses the format of `[IP address]/[prefix length]`. The following command queries IP addresses in the `192.168.1.0/24` subnet meaning IP addresses from `192.168.1.0` to `192.168.1.255`.\n\n```\nGET logs-example-default/_search\n{\n  "query": {\n    "term": {\n      "host.ip": "192.168.1.0/24"\n    }\n  }\n}\n```\n\nBecause all of the example logs are in this range, you’ll get the following results:\n\n```\n{\n  ...\n  },\n  "hits": {\n    ...\n      {\n        "_index": ".ds-logs-example-default-2023.08.16-000001",\n        "_id": "ak4oAIoBl7fe5ItIixuB",\n        "_score": 1,\n        "_source": {\n          "host": {\n            "ip": "192.168.1.101"\n          },\n          "@timestamp": "2023-08-08T13:45:12.123Z",\n          "message": "Disk usage exceeds 90%.",\n          "log": {\n            "level": "WARN"\n          }\n        }\n      },\n      {\n        "_index": ".ds-logs-example-default-2023.08.16-000001",\n        "_id": "a04oAIoBl7fe5ItIixuC",\n        "_score": 1,\n        "_source": {\n          "host": {\n            "ip": "192.168.1.103"\n          },\n          "@timestamp": "2023-08-08T13:45:14.003Z",\n          "message": "Database connection failed.",\n          "log": {\n            "level": "ERROR"\n          }\n        }\n      },\n      {\n        "_index": ".ds-logs-example-default-2023.08.16-000001",\n        "_id": "bE4oAIoBl7fe5ItIixuC",\n        "_score": 1,\n        "_source": {\n          "host": {\n            "ip": "192.168.1.104"\n          },\n          "@timestamp": "2023-08-08T13:45:15.004Z",\n          "message": "Debugging connection issue.",\n          "log": {\n            "level": "DEBUG"\n          }\n        }\n      },\n      {\n        "_index": ".ds-logs-example-default-2023.08.16-000001",\n        "_id": "bU4oAIoBl7fe5ItIixuC",\n        "_score": 1,\n        "_source": {\n          "host": {\n            "ip": "192.168.1.102"\n          },\n          "@timestamp": "2023-08-08T13:45:16.005Z",\n          "message": "User changed profile picture.",\n          "log": {\n            "level": "INFO"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n###### Range queries\n\nUse [range queries](/guide/en/elasticsearch/reference/8.17/query-dsl-range-query.html) to query logs in a specific range.\n\nThe following command searches for IP addresses greater than or equal to `192.168.1.100` and less than or equal to `192.168.1.102`.\n\n```\nGET logs-example-default/_search\n{\n  "query": {\n    "range": {\n      "host.ip": {\n        "gte": "192.168.1.100",\n        "lte": "192.168.1.102"\n      }\n    }\n  }\n}\n```\n\n|                |                                           |\n| :------------- | :---------------------------------------- |\n| [**](#CO109-1) | Greater than or equal to `192.168.1.100`. |\n| [**](#CO109-2) | Less than or equal to `192.168.1.102`.    |\n\nYou’ll get the following results only showing logs in the range you’ve set:\n\n```\n{\n  ...\n  },\n  "hits": {\n    ...\n      {\n        "_index": ".ds-logs-example-default-2023.08.16-000001",\n        "_id": "ak4oAIoBl7fe5ItIixuB",\n        "_score": 1,\n        "_source": {\n          "host": {\n            "ip": "192.168.1.101"\n          },\n          "@timestamp": "2023-08-08T13:45:12.123Z",\n          "message": "Disk usage exceeds 90%.",\n          "log": {\n            "level": "WARN"\n          }\n        }\n      },\n      {\n        "_index": ".ds-logs-example-default-2023.08.16-000001",\n        "_id": "bU4oAIoBl7fe5ItIixuC",\n        "_score": 1,\n        "_source": {\n          "host": {\n            "ip": "192.168.1.102"\n          },\n          "@timestamp": "2023-08-08T13:45:16.005Z",\n          "message": "User changed profile picture.",\n          "log": {\n            "level": "INFO"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n### Reroute log data to specific data streams\n\nBy default, an ingest pipeline sends your log data to a single data stream. To simplify log data management, use a [reroute processor](/guide/en/elasticsearch/reference/8.17/reroute-processor.html) to route data from the generic data stream to a target data stream. For example, you might want to send high-severity logs to a specific data stream to help with categorization.\n\nThis section shows you how to use a reroute processor to send the high-severity logs (`WARN` or `ERROR`) from the following example logs to a specific data stream and keep the regular logs (`DEBUG` and `INFO`) in the default data stream:\n\n```\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\n2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\n2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\n2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\n```\n\nWhen routing data to different data streams, we recommend picking a field with a limited number of distinct values to prevent an excessive increase in the number of data streams. For more details, refer to the [Size your shards](/guide/en/elasticsearch/reference/8.17/size-your-shards.html) documentation.\n\nTo use a reroute processor:\n\n1. [Add a reroute processor to your ingest pipeline.](logs-parse.html#logs-stream-reroute-pipeline "Add a reroute processor to the ingest pipeline")\n2. [Add the example logs to your data stream.](logs-parse.html#logs-stream-reroute-add-logs "Add logs to a data stream")\n3. [Query your logs and verify the high-severity logs were routed to the new data stream.](logs-parse.html#logs-stream-reroute-verify "Verify the reroute processor worked")\n\n#### Add a reroute processor to the ingest pipeline\n\nAdd a reroute processor to your ingest pipeline with the following command:\n\n```\nPUT _ingest/pipeline/logs-example-default\n{\n  "description": "Extracts fields and reroutes WARN",\n  "processors": [\n    {\n      "dissect": {\n        "field": "message",\n        "pattern": "%{@timestamp} %{log.level} %{host.ip} %{message}"\n      },\n      "reroute": {\n        "tag": "high_severity_logs",\n        "if" : "ctx.log?.level == \'WARN\' || ctx.log?.level == \'ERROR\'",\n        "dataset": "critical"\n      }\n    }\n  ]\n}\n```\n\n|                |                                                                                                                                                                                                                       |\n| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [**](#CO110-1) | `tag`: Identifier for the processor that you can use for debugging and metrics. In the example, the tag is set to `high_severity_logs`.                                                                               |\n| [**](#CO110-2) | `if`: Conditionally runs the processor. In the example, `"ctx.log?.level == \'WARN\' \\|\\| ctx.log?.level == \'ERROR\'",` means the processor runs when the `log.level` field is `WARN` or `ERROR`.                        |\n| [**](#CO110-3) | `dataset`: the data stream dataset to route your document to if the previous condition is `true`. In the example, logs with a `log.level` of `WARN` or `ERROR` are routed to the `logs-critical-default` data stream. |\n\nIn addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-index-template "Configure a data stream with an index template") section.\n\n#### Add logs to a data stream\n\nAdd the example logs to your data stream with this command:\n\n```\nPOST logs-example-default/_bulk\n{ "create": {} }\n{ "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue." }\n{ "create": {} }\n{ "message": "2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture." }\n```\n\n#### Verify the reroute processor worked\n\nThe reroute processor should route any logs with a `log.level` of `WARN` or `ERROR` to the `logs-critical-default` data stream. Query the the data stream using the following command to verify the log data was routed as intended:\n\n```\nGET logs-critical-default/_search\n```\n\nYour should see similar results to the following showing that the high-severity logs are now in the `critical` dataset:\n\n```\n{\n  ...\n  "hits": {\n    ...\n    "hits": [\n        ...\n        "_source": {\n          "host": {\n            "ip": "192.168.1.101"\n          },\n          "@timestamp": "2023-08-08T13:45:12.123Z",\n          "message": "Disk usage exceeds 90%.",\n          "log": {\n            "level": "WARN"\n          },\n          "data_stream": {\n            "namespace": "default",\n            "type": "logs",\n            "dataset": "critical"\n          },\n          {\n        ...\n        "_source": {\n          "host": {\n            "ip": "192.168.1.103"\n           },\n          "@timestamp": "2023-08-08T13:45:14.003Z",\n          "message": "Database connection failed.",\n          "log": {\n            "level": "ERROR"\n          },\n          "data_stream": {\n            "namespace": "default",\n            "type": "logs",\n            "dataset": "critical"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n',
          title: 'Parse and organize logs',
          url: 'https://www.elastic.co/guide/en/observability/8.17/logs-parse.html',
          productName: 'observability',
          score: 130.12059,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/observability/8.17/logs-parse.html',
        title: 'Parse and organize logs',
        score: 130.12059,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Parse and organize logs\\n\\nIf your log data is unstructured or semi-structured, you can parse it and break it into meaningful fields. You can use those fields to explore and analyze your data. For example, you can find logs within a specific timestamp range or filter logs by log level to focus on potential issues.\\n\\nAfter parsing, you can use the structured fields to further organize your logs by configuring a reroute processor to send specific logs to different target data streams.\\n\\nRefer to the following sections for more on parsing and organizing your log data:\\n\\n* [Extract structured fields](logs-parse.html#logs-stream-parse \\"Extract structured fields\\"): Extract structured fields like timestamps, log levels, or IP addresses to make querying and filtering your data easier.\\n* [Reroute log data to specific data streams](logs-parse.html#logs-stream-reroute \\"Reroute log data to specific data streams\\"): Route data from the generic data stream to a target data stream for more granular control over data retention, permissions, and processing.\\n\\n### Extract structured fields\\n\\nMake your logs more useful by extracting structured fields from your unstructured log data. Extracting structured fields makes it easier to search, analyze, and filter your log data.\\n\\nFollow the steps below to see how the following unstructured log data is indexed by default:\\n\\n```\\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\n```\\n\\nStart by storing the document in the `logs-example-default` data stream:\\n\\n1. To open **Console**, find `Dev Tools` in the [global search field](/guide/en/kibana/8.17/introduction.html#kibana-navigation-search).\\n\\n2. In the **Console** tab, add the example log to Elasticsearch using the following command:\\n\\n   ```\\n   POST logs-example-default/_doc\\n   {\\n     \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\"\\n   }\\n   ```\\n\\n3. Then, you can retrieve the document with the following search:\\n\\n   ```\\n   GET /logs-example-default/_search\\n   ```\\n\\nThe results should look like this:\\n\\n```\\n{\\n  ...\\n  \\"hits\\": {\\n    ...\\n    \\"hits\\": [\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.09-000001\\",\\n        ...\\n        \\"_source\\": {\\n          \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\",\\n          \\"@timestamp\\": \\"2023-08-09T17:19:27.73312243Z\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nElasticsearch indexes the `message` field by default and adds a `@timestamp` field. Since there was no timestamp set, it’s set to `now`. At this point, you can search for phrases in the `message` field like `WARN` or `Disk usage exceeds`. For example, use the following command to search for the phrase `WARN` in the log’s `message` field:\\n\\n```\\nGET logs-example-default/_search\\n{\\n  \\"query\\": {\\n    \\"match\\": {\\n      \\"message\\": {\\n        \\"query\\": \\"WARN\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nWhile you can search for phrases in the `message` field, you can’t use this field to filter log data. Your message, however, contains all of the following potential fields you can extract and use to filter and aggregate your log data:\\n\\n* **@timestamp** (`2023-08-08T13:45:12.123Z`): Extracting this field lets you sort logs by date and time. This is helpful when you want to view your logs in the order that they occurred or identify when issues happened.\\n* **log.level** (`WARN`): Extracting this field lets you filter logs by severity. This is helpful if you want to focus on high-severity WARN or ERROR-level logs, and reduce noise by filtering out low-severity INFO-level logs.\\n* **host.ip** (`192.168.1.101`): Extracting this field lets you filter logs by the host IP addresses. This is helpful if you want to focus on specific hosts that you’re having issues with or if you want to find disparities between hosts.\\n* **message** (`Disk usage exceeds 90%.`): You can search for phrases or words in the message field.\\n\\nThese fields are part of the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16/ecs-reference.html). The ECS defines a common set of fields that you can use across Elasticsearch when storing data, including log and metric data.\\n\\n#### Extract the `@timestamp` field\\n\\nWhen you added the log to Elasticsearch in the previous section, the `@timestamp` field showed when the log was added. The timestamp showing when the log actually occurred was in the unstructured `message` field:\\n\\n```\\n        ...\\n        \\"_source\\": {\\n          \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\",\\n          \\"@timestamp\\": \\"2023-08-09T17:19:27.73312243Z\\"\\n        }\\n        ...\\n```\\n\\n|                |                                                                                        |\\n| :------------- | :------------------------------------------------------------------------------------- |\\n| [**](#CO105-1) | The timestamp in the `message` field shows when the log occurred.                      |\\n| [**](#CO105-2) | The timestamp in the `@timestamp` field shows when the log was added to Elasticsearch. |\\n\\nWhen looking into issues, you want to filter for logs by when the issue occurred not when the log was added to your project. To do this, extract the timestamp from the unstructured `message` field to the structured `@timestamp` field by completing the following:\\n\\n1. [Use an ingest pipeline to extract the `@timestamp` field](logs-parse.html#logs-stream-ingest-pipeline \\"Use an ingest pipeline to extract the @timestamp field\\")\\n2. [Test the pipeline with the simulate pipeline API](logs-parse.html#logs-stream-simulate-api \\"Test the pipeline with the simulate pipeline API\\")\\n3. [Configure a data stream with an index template](logs-parse.html#logs-stream-index-template \\"Configure a data stream with an index template\\")\\n4. [Create a data stream](logs-parse.html#logs-stream-create-data-stream \\"Create a data stream\\")\\n\\n##### Use an ingest pipeline to extract the `@timestamp` field\\n\\nIngest pipelines consist of a series of processors that perform common transformations on incoming documents before they are indexed. To extract the `@timestamp` field from the example log, use an ingest pipeline with a dissect processor. The [dissect processor](/guide/en/elasticsearch/reference/8.17/dissect-processor.html) extracts structured fields from unstructured log messages based on a pattern you set.\\n\\nElasticsearch can parse string timestamps that are in `yyyy-MM-dd\'T\'HH:mm:ss.SSSZ` and `yyyy-MM-dd` formats into date fields. Since the log example’s timestamp is in one of these formats, you don’t need additional processors. More complex or nonstandard timestamps require a [date processor](/guide/en/elasticsearch/reference/8.17/date-processor.html) to parse the timestamp into a date field.\\n\\nUse the following command to extract the timestamp from the `message` field into the `@timestamp` field:\\n\\n```\\nPUT _ingest/pipeline/logs-example-default\\n{\\n  \\"description\\": \\"Extracts the timestamp\\",\\n  \\"processors\\": [\\n    {\\n      \\"dissect\\": {\\n        \\"field\\": \\"message\\",\\n        \\"pattern\\": \\"%{@timestamp} %{message}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n|                |                                                                                                                                                                                                                                                                                                                                                                 |\\n| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [**](#CO106-1) | The name of the pipeline,`logs-example-default`, needs to match the name of your data stream. You’ll set up your data stream in the next section. For more information, refer to the [data stream naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme).                                                                            |\\n| [**](#CO106-2) | The field you’re extracting data from, `message` in this case.                                                                                                                                                                                                                                                                                                  |\\n| [**](#CO106-3) | The pattern of the elements in your log data. The `%{@timestamp} %{message}` pattern extracts the timestamp, `2023-08-08T13:45:12.123Z`, to the `@timestamp` field, while the rest of the message, `WARN 192.168.1.101 Disk usage exceeds 90%.`, stays in the `message` field. The dissect processor looks for the space as a separator defined by the pattern. |\\n\\n##### Test the pipeline with the simulate pipeline API\\n\\nThe [simulate pipeline API](/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html#ingest-verbose-param) runs the ingest pipeline without storing any documents. This lets you verify your pipeline works using multiple documents. Run the following command to test your ingest pipeline with the simulate pipeline API.\\n\\n```\\nPOST _ingest/pipeline/logs-example-default/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe results should show the `@timestamp` field extracted from the `message` field:\\n\\n```\\n{\\n  \\"docs\\": [\\n    {\\n      \\"doc\\": {\\n        \\"_index\\": \\"_index\\",\\n        \\"_id\\": \\"_id\\",\\n        \\"_version\\": \\"-3\\",\\n        \\"_source\\": {\\n          \\"message\\": \\"WARN 192.168.1.101 Disk usage exceeds 90%.\\",\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\"\\n        },\\n        ...\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nMake sure you’ve created the ingest pipeline using the `PUT` command in the previous section before using the simulate pipeline API.\\n\\n##### Configure a data stream with an index template\\n\\nAfter creating your ingest pipeline, run the following command to create an index template to configure your data stream’s backing indices:\\n\\n```\\nPUT _index_template/logs-example-default-template\\n{\\n  \\"index_patterns\\": [ \\"logs-example-*\\" ],\\n  \\"data_stream\\": { },\\n  \\"priority\\": 500,\\n  \\"template\\": {\\n    \\"settings\\": {\\n      \\"index.default_pipeline\\":\\"logs-example-default\\"\\n    }\\n  },\\n  \\"composed_of\\": [\\n    \\"logs@mappings\\",\\n    \\"logs@settings\\",\\n    \\"logs@custom\\",\\n    \\"ecs@mappings\\"\\n  ],\\n  \\"ignore_missing_component_templates\\": [\\"logs@custom\\"]\\n}\\n```\\n\\n|                |                                                                                                                                                                                                                                                                                                                                                    |\\n| :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [**](#CO107-1) | `index_pattern`: Needs to match your log data stream. Naming conventions for data streams are `<type>-<dataset>-<namespace>`. In this example, your logs data stream is named `logs-example-*`. Data that matches this pattern will go through your pipeline.                                                                                      |\\n| [**](#CO107-2) | `data_stream`: Enables data streams.                                                                                                                                                                                                                                                                                                               |\\n| [**](#CO107-3) | `priority`: Sets the priority of you Index Template. Index templates with higher priority take precedence over lower priority. If a data stream matches multiple index templates, Elasticsearch uses the template with the higher priority. Built-in templates have a priority of `200`, so use a priority higher than `200` for custom templates. |\\n| [**](#CO107-4) | `index.default_pipeline`: The name of your ingest pipeline. `logs-example-default` in this case.                                                                                                                                                                                                                                                   |\\n| [**](#CO107-5) | `composed_of`: Here you can set component templates. Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. Elastic has several built-in templates to help when ingesting your log data.                                                                                     |\\n\\nThe example index template above sets the following component templates:\\n\\n* `logs@mappings`: general mappings for log data streams that include disabling automatic date detection from `string` fields and specifying mappings for [`data_stream` ECS fields](/guide/en/ecs/8.16/ecs-data_stream.html).\\n\\n* `logs@settings`: general settings for log data streams including the following:\\n\\n  * The default lifecycle policy that rolls over when the primary shard reaches 50 GB or after 30 days.\\n  * The default pipeline uses the ingest timestamp if there is no specified `@timestamp` and places a hook for the `logs@custom` pipeline. If a `logs@custom` pipeline is installed, it’s applied to logs ingested into this data stream.\\n  * Sets the [`ignore_malformed`](/guide/en/elasticsearch/reference/8.17/ignore-malformed.html) flag to `true`. When ingesting a large batch of log data, a single malformed field like an IP address can cause the entire batch to fail. When set to true, malformed fields with a mapping type that supports this flag are still processed.\\n\\n* `logs@custom`: a predefined component template that is not installed by default. Use this name to install a custom component template to override or extend any of the default mappings or settings.\\n\\n* `ecs@mappings`: dynamic templates that automatically ensure your data stream mappings comply with the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16/ecs-reference.html).\\n\\n##### Create a data stream\\n\\nCreate your data stream using the [data stream naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme). Name your data stream to match the name of your ingest pipeline, `logs-example-default` in this case. Post the example log to your data stream with this command:\\n\\n```\\nPOST logs-example-default/_doc\\n{\\n  \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\"\\n}\\n```\\n\\nView your documents using this command:\\n\\n```\\nGET /logs-example-default/_search\\n```\\n\\nYou should see the pipeline has extracted the `@timestamp` field:\\n\\n```\\n{\\n...\\n{\\n  ...\\n  \\"hits\\": {\\n    ...\\n    \\"hits\\": [\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.09-000001\\",\\n        \\"_id\\": \\"RsWy3IkB8yCtA5VGOKLf\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"message\\": \\"WARN 192.168.1.101 Disk usage exceeds 90%.\\",\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\n|                |                                   |\\n| :------------- | :-------------------------------- |\\n| [**](#CO108-1) | The extracted `@timestamp` field. |\\n\\nYou can now use the `@timestamp` field to sort your logs by the date and time they happened.\\n\\n##### Troubleshoot the `@timestamp` field\\n\\nCheck the following common issues and solutions with timestamps:\\n\\n* **Timestamp failure**: If your data has inconsistent date formats, set `ignore_failure` to `true` for your date processor. This processes logs with correctly formatted dates and ignores those with issues.\\n* **Incorrect timezone**: Set your timezone using the `timezone` option on the [date processor](/guide/en/elasticsearch/reference/8.17/date-processor.html).\\n* **Incorrect timestamp format**: Your timestamp can be a Java time pattern or one of the following formats: ISO8601, UNIX, UNIX\\\\_MS, or TAI64N. For more information on timestamp formats, refer to the [mapping date format](/guide/en/elasticsearch/reference/8.17/mapping-date-format.html).\\n\\n#### Extract the `log.level` field\\n\\nExtracting the `log.level` field lets you filter by severity and focus on critical issues. This section shows you how to extract the `log.level` field from this example log:\\n\\n```\\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\n```\\n\\nTo extract and use the `log.level` field:\\n\\n1. [Add the `log.level` field to the dissect processor pattern in your ingest pipeline.](logs-parse.html#logs-stream-log-level-pipeline \\"Add log.level to your ingest pipeline\\")\\n2. [Test the pipeline with the simulate API.](logs-parse.html#logs-stream-log-level-simulate \\"Test the pipeline with the simulate API\\")\\n3. [Query your logs based on the `log.level` field.](logs-parse.html#logs-stream-log-level-query \\"Query logs based on log.level\\")\\n\\n##### Add `log.level` to your ingest pipeline\\n\\nAdd the `%{log.level}` option to the dissect processor pattern in the ingest pipeline you created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-ingest-pipeline \\"Use an ingest pipeline to extract the @timestamp field\\") section with this command:\\n\\n```\\nPUT _ingest/pipeline/logs-example-default\\n{\\n  \\"description\\": \\"Extracts the timestamp and log level\\",\\n  \\"processors\\": [\\n    {\\n      \\"dissect\\": {\\n        \\"field\\": \\"message\\",\\n        \\"pattern\\": \\"%{@timestamp} %{log.level} %{message}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nNow your pipeline will extract these fields:\\n\\n* The `@timestamp` field: `2023-08-08T13:45:12.123Z`\\n* The `log.level` field: `WARN`\\n* The `message` field: `192.168.1.101 Disk usage exceeds 90%.`\\n\\nIn addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-index-template \\"Configure a data stream with an index template\\") section.\\n\\n##### Test the pipeline with the simulate API\\n\\nTest that your ingest pipeline works as expected with the [simulate pipeline API](/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html#ingest-verbose-param):\\n\\n```\\nPOST _ingest/pipeline/logs-example-default/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe results should show the `@timestamp` and the `log.level` fields extracted from the `message` field:\\n\\n```\\n{\\n  \\"docs\\": [\\n    {\\n      \\"doc\\": {\\n        \\"_index\\": \\"_index\\",\\n        \\"_id\\": \\"_id\\",\\n        \\"_version\\": \\"-3\\",\\n        \\"_source\\": {\\n          \\"message\\": \\"192.168.1.101 Disk usage exceeds 90%.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"WARN\\"\\n          },\\n          \\"@timestamp\\": \\"2023-8-08T13:45:12.123Z\\",\\n        },\\n        ...\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n##### Query logs based on `log.level`\\n\\nOnce you’ve extracted the `log.level` field, you can query for high-severity logs like `WARN` and `ERROR`, which may need immediate attention, and filter out less critical `INFO` and `DEBUG` logs.\\n\\nLet’s say you have the following logs with varying severities:\\n\\n```\\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\n2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\\n2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\\n2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\\n```\\n\\nAdd them to your data stream using this command:\\n\\n```\\nPOST logs-example-default/_bulk\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\\" }\\n```\\n\\nThen, query for documents with a log level of `WARN` or `ERROR` with this command:\\n\\n```\\nGET logs-example-default/_search\\n{\\n  \\"query\\": {\\n    \\"terms\\": {\\n      \\"log.level\\": [\\"WARN\\", \\"ERROR\\"]\\n    }\\n  }\\n}\\n```\\n\\nThe results should show only the high-severity logs:\\n\\n```\\n{\\n...\\n  },\\n  \\"hits\\": {\\n  ...\\n    \\"hits\\": [\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.14-000001\\",\\n        \\"_id\\": \\"3TcZ-4kB3FafvEVY4yKx\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"message\\": \\"192.168.1.101 Disk usage exceeds 90%.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"WARN\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\"\\n        }\\n      },\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.14-000001\\",\\n        \\"_id\\": \\"3jcZ-4kB3FafvEVY4yKx\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"message\\": \\"192.168.1.103 Database connection failed.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"ERROR\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:14.003Z\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\n#### Extract the `host.ip` field\\n\\nExtracting the `host.ip` field lets you filter logs by host IP addresses allowing you to focus on specific hosts that you’re having issues with or find disparities between hosts.\\n\\nThe `host.ip` field is part of the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16/ecs-reference.html). Through the ECS, the `host.ip` field is mapped as an [`ip` field type](/guide/en/elasticsearch/reference/8.17/ip.html). `ip` field types allow range queries so you can find logs with IP addresses in a specific range. You can also query `ip` field types using Classless Inter-Domain Routing (CIDR) notation to find logs from a particular network or subnet.\\n\\nThis section shows you how to extract the `host.ip` field from the following example logs and query based on the extracted fields:\\n\\n```\\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\n2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\\n2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\\n2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\\n```\\n\\nTo extract and use the `host.ip` field:\\n\\n1. [Add the `host.ip` field to your dissect processor in your ingest pipeline.](logs-parse.html#logs-stream-host-ip-pipeline \\"Add host.ip to your ingest pipeline\\")\\n2. [Test the pipeline with the simulate API.](logs-parse.html#logs-stream-host-ip-simulate \\"Test the pipeline with the simulate API\\")\\n3. [Query your logs based on the `host.ip` field.](logs-parse.html#logs-stream-host-ip-query \\"Query logs based on host.ip\\")\\n\\n##### Add `host.ip` to your ingest pipeline\\n\\nAdd the `%{host.ip}` option to the dissect processor pattern in the ingest pipeline you created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-ingest-pipeline \\"Use an ingest pipeline to extract the @timestamp field\\") section:\\n\\n```\\nPUT _ingest/pipeline/logs-example-default\\n{\\n  \\"description\\": \\"Extracts the timestamp log level and host ip\\",\\n  \\"processors\\": [\\n    {\\n      \\"dissect\\": {\\n        \\"field\\": \\"message\\",\\n        \\"pattern\\": \\"%{@timestamp} %{log.level} %{host.ip} %{message}\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYour pipeline will extract these fields:\\n\\n* The `@timestamp` field: `2023-08-08T13:45:12.123Z`\\n* The `log.level` field: `WARN`\\n* The `host.ip` field: `192.168.1.101`\\n* The `message` field: `Disk usage exceeds 90%.`\\n\\nIn addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-index-template \\"Configure a data stream with an index template\\") section.\\n\\n##### Test the pipeline with the simulate API\\n\\nTest that your ingest pipeline works as expected with the [simulate pipeline API](/guide/en/elasticsearch/reference/8.17/simulate-pipeline-api.html#ingest-verbose-param):\\n\\n```\\nPOST _ingest/pipeline/logs-example-default/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_source\\": {\\n        \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe results should show the `host.ip`, `@timestamp`, and `log.level` fields extracted from the `message` field:\\n\\n```\\n{\\n  \\"docs\\": [\\n    {\\n      \\"doc\\": {\\n        ...\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.101\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\",\\n          \\"message\\": \\"Disk usage exceeds 90%.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"WARN\\"\\n          }\\n        },\\n        ...\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n##### Query logs based on `host.ip`\\n\\nYou can query your logs based on the `host.ip` field in different ways, including using CIDR notation and range queries.\\n\\nBefore querying your logs, add them to your data stream using this command:\\n\\n```\\nPOST logs-example-default/_bulk\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\\" }\\n```\\n\\n###### CIDR notation\\n\\nYou can use [CIDR notation](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation) to query your log data using a block of IP addresses that fall within a certain network segment. CIDR notations uses the format of `[IP address]/[prefix length]`. The following command queries IP addresses in the `192.168.1.0/24` subnet meaning IP addresses from `192.168.1.0` to `192.168.1.255`.\\n\\n```\\nGET logs-example-default/_search\\n{\\n  \\"query\\": {\\n    \\"term\\": {\\n      \\"host.ip\\": \\"192.168.1.0/24\\"\\n    }\\n  }\\n}\\n```\\n\\nBecause all of the example logs are in this range, you’ll get the following results:\\n\\n```\\n{\\n  ...\\n  },\\n  \\"hits\\": {\\n    ...\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.16-000001\\",\\n        \\"_id\\": \\"ak4oAIoBl7fe5ItIixuB\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.101\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\",\\n          \\"message\\": \\"Disk usage exceeds 90%.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"WARN\\"\\n          }\\n        }\\n      },\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.16-000001\\",\\n        \\"_id\\": \\"a04oAIoBl7fe5ItIixuC\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.103\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:14.003Z\\",\\n          \\"message\\": \\"Database connection failed.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"ERROR\\"\\n          }\\n        }\\n      },\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.16-000001\\",\\n        \\"_id\\": \\"bE4oAIoBl7fe5ItIixuC\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.104\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:15.004Z\\",\\n          \\"message\\": \\"Debugging connection issue.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"DEBUG\\"\\n          }\\n        }\\n      },\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.16-000001\\",\\n        \\"_id\\": \\"bU4oAIoBl7fe5ItIixuC\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.102\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:16.005Z\\",\\n          \\"message\\": \\"User changed profile picture.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"INFO\\"\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\n###### Range queries\\n\\nUse [range queries](/guide/en/elasticsearch/reference/8.17/query-dsl-range-query.html) to query logs in a specific range.\\n\\nThe following command searches for IP addresses greater than or equal to `192.168.1.100` and less than or equal to `192.168.1.102`.\\n\\n```\\nGET logs-example-default/_search\\n{\\n  \\"query\\": {\\n    \\"range\\": {\\n      \\"host.ip\\": {\\n        \\"gte\\": \\"192.168.1.100\\",\\n        \\"lte\\": \\"192.168.1.102\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\n|                |                                           |\\n| :------------- | :---------------------------------------- |\\n| [**](#CO109-1) | Greater than or equal to `192.168.1.100`. |\\n| [**](#CO109-2) | Less than or equal to `192.168.1.102`.    |\\n\\nYou’ll get the following results only showing logs in the range you’ve set:\\n\\n```\\n{\\n  ...\\n  },\\n  \\"hits\\": {\\n    ...\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.16-000001\\",\\n        \\"_id\\": \\"ak4oAIoBl7fe5ItIixuB\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.101\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\",\\n          \\"message\\": \\"Disk usage exceeds 90%.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"WARN\\"\\n          }\\n        }\\n      },\\n      {\\n        \\"_index\\": \\".ds-logs-example-default-2023.08.16-000001\\",\\n        \\"_id\\": \\"bU4oAIoBl7fe5ItIixuC\\",\\n        \\"_score\\": 1,\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.102\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:16.005Z\\",\\n          \\"message\\": \\"User changed profile picture.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"INFO\\"\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\n### Reroute log data to specific data streams\\n\\nBy default, an ingest pipeline sends your log data to a single data stream. To simplify log data management, use a [reroute processor](/guide/en/elasticsearch/reference/8.17/reroute-processor.html) to route data from the generic data stream to a target data stream. For example, you might want to send high-severity logs to a specific data stream to help with categorization.\\n\\nThis section shows you how to use a reroute processor to send the high-severity logs (`WARN` or `ERROR`) from the following example logs to a specific data stream and keep the regular logs (`DEBUG` and `INFO`) in the default data stream:\\n\\n```\\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\n2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\\n2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\\n2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\\n```\\n\\nWhen routing data to different data streams, we recommend picking a field with a limited number of distinct values to prevent an excessive increase in the number of data streams. For more details, refer to the [Size your shards](/guide/en/elasticsearch/reference/8.17/size-your-shards.html) documentation.\\n\\nTo use a reroute processor:\\n\\n1. [Add a reroute processor to your ingest pipeline.](logs-parse.html#logs-stream-reroute-pipeline \\"Add a reroute processor to the ingest pipeline\\")\\n2. [Add the example logs to your data stream.](logs-parse.html#logs-stream-reroute-add-logs \\"Add logs to a data stream\\")\\n3. [Query your logs and verify the high-severity logs were routed to the new data stream.](logs-parse.html#logs-stream-reroute-verify \\"Verify the reroute processor worked\\")\\n\\n#### Add a reroute processor to the ingest pipeline\\n\\nAdd a reroute processor to your ingest pipeline with the following command:\\n\\n```\\nPUT _ingest/pipeline/logs-example-default\\n{\\n  \\"description\\": \\"Extracts fields and reroutes WARN\\",\\n  \\"processors\\": [\\n    {\\n      \\"dissect\\": {\\n        \\"field\\": \\"message\\",\\n        \\"pattern\\": \\"%{@timestamp} %{log.level} %{host.ip} %{message}\\"\\n      },\\n      \\"reroute\\": {\\n        \\"tag\\": \\"high_severity_logs\\",\\n        \\"if\\" : \\"ctx.log?.level == \'WARN\' || ctx.log?.level == \'ERROR\'\\",\\n        \\"dataset\\": \\"critical\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n|                |                                                                                                                                                                                                                       |\\n| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [**](#CO110-1) | `tag`: Identifier for the processor that you can use for debugging and metrics. In the example, the tag is set to `high_severity_logs`.                                                                               |\\n| [**](#CO110-2) | `if`: Conditionally runs the processor. In the example, `\\"ctx.log?.level == \'WARN\' \\\\|\\\\| ctx.log?.level == \'ERROR\'\\",` means the processor runs when the `log.level` field is `WARN` or `ERROR`.                        |\\n| [**](#CO110-3) | `dataset`: the data stream dataset to route your document to if the previous condition is `true`. In the example, logs with a `log.level` of `WARN` or `ERROR` are routed to the `logs-critical-default` data stream. |\\n\\nIn addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the [Extract the `@timestamp` field](logs-parse.html#logs-stream-index-template \\"Configure a data stream with an index template\\") section.\\n\\n#### Add logs to a data stream\\n\\nAdd the example logs to your data stream with this command:\\n\\n```\\nPOST logs-example-default/_bulk\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.\\" }\\n{ \\"create\\": {} }\\n{ \\"message\\": \\"2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.\\" }\\n```\\n\\n#### Verify the reroute processor worked\\n\\nThe reroute processor should route any logs with a `log.level` of `WARN` or `ERROR` to the `logs-critical-default` data stream. Query the the data stream using the following command to verify the log data was routed as intended:\\n\\n```\\nGET logs-critical-default/_search\\n```\\n\\nYour should see similar results to the following showing that the high-severity logs are now in the `critical` dataset:\\n\\n```\\n{\\n  ...\\n  \\"hits\\": {\\n    ...\\n    \\"hits\\": [\\n        ...\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.101\\"\\n          },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:12.123Z\\",\\n          \\"message\\": \\"Disk usage exceeds 90%.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"WARN\\"\\n          },\\n          \\"data_stream\\": {\\n            \\"namespace\\": \\"default\\",\\n            \\"type\\": \\"logs\\",\\n            \\"dataset\\": \\"critical\\"\\n          },\\n          {\\n        ...\\n        \\"_source\\": {\\n          \\"host\\": {\\n            \\"ip\\": \\"192.168.1.103\\"\\n           },\\n          \\"@timestamp\\": \\"2023-08-08T13:45:14.003Z\\",\\n          \\"message\\": \\"Database connection failed.\\",\\n          \\"log\\": {\\n            \\"level\\": \\"ERROR\\"\\n          },\\n          \\"data_stream\\": {\\n            \\"namespace\\": \\"default\\",\\n            \\"type\\": \\"logs\\",\\n            \\"dataset\\": \\"critical\\"\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n","title":"Parse and organize logs","url":"https://www.elastic.co/guide/en/observability/8.17/logs-parse.html","productName":"observability","score":130.12059}',
        truncated: {
          truncatedText:
            '{"content":"## Parse and organize logs\\n\\nIf your log data is unstructured or semi-structured, you can parse it and break it into meaningful fields. You can use those fields to explore and analyze your data. For example, you can find logs within a specific timestamp range or filter logs by log level to focus on potential issues.\\n\\nAfter parsing, you can use the structured fields to further organize your logs by configuring a reroute processor to send specific logs to different target data streams.\\n\\nRefer to the following sections for more on parsing and organizing your log data:\\n\\n* [Extract structured fields](logs-parse.html#logs-stream-parse \\"Extract structured fields\\"): Extract structured fields like timestamps, log levels, or IP addresses to make querying and filtering your data easier.\\n* [Reroute log data to specific data streams](logs-parse.html#logs-stream-reroute \\"Reroute log data to specific data streams\\"): Route data from the generic data stream to a target data stream for more granular control over data retention, permissions, and processing.\\n\\n### Extract structured fields\\n\\nMake your logs more useful by extracting structured fields from your unstructured log data. Extracting structured fields makes it easier to search, analyze, and filter your log data.\\n\\nFollow the steps below to see how the following unstructured log data is indexed by default:\\n\\n```\\n2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.\\n```\\n\\nStart by storing the document in the `logs-example-default` data stream:\\n\\n1. To open **Console**, find `Dev Tools` in the [global search field](/guide/en/kibana/8.17/introduction.html#kibana-navigation-search).\\n\\n2. In the **Console** tab, add the example log to Elasticsearch using the following command... <truncated>',
          originalTokenCount: 9537,
          truncatedTokenCount: 400,
        },
        llmScore: 4,
      },
      {
        selected: false,
        document: {
          content:
            '## Delete pipeline API\n\nDeletes one or more existing ingest pipeline.\n\n```\nresp = client.ingest.delete_pipeline(\n    id="my-pipeline-id",\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.delete_pipeline(\n  id: \'my-pipeline-id\'\n)\nputs response\n```\n\n```\nconst response = await client.ingest.deletePipeline({\n  id: "my-pipeline-id",\n});\nconsole.log(response);\n```\n\n```\nDELETE /_ingest/pipeline/my-pipeline-id\n```\n\n### Request\n\n`DELETE /_ingest/pipeline/<pipeline>`\n\n### Prerequisites\n\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to use this API.\n\n### Path parameters\n\n* `<pipeline>`\n\n  (Required, string) Pipeline ID or wildcard expression of pipeline IDs used to limit the request.\n\n  To delete all ingest pipelines in a cluster, use a value of `*`.\n\n### Query parameters\n\n* `master_timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n* `timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for a response from all relevant nodes in the cluster after updating the cluster metadata. If no response is received before the timeout expires, the cluster metadata update still applies but the response will indicate that it was not completely acknowledged. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n### Examples\n\n#### Delete a specific ingest pipeline\n\n```\nresp = client.ingest.delete_pipeline(\n    id="pipeline-one",\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.delete_pipeline(\n  id: \'pipeline-one\'\n)\nputs response\n```\n\n```\nconst response = await client.ingest.deletePipeline({\n  id: "pipeline-one",\n});\nconsole.log(response);\n```\n\n```\nDELETE /_ingest/pipeline/pipeline-one\n```\n\n#### Delete ingest pipelines using a wildcard expression\n\n```\nresp = client.ingest.delete_pipeline(\n    id="pipeline-*",\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.delete_pipeline(\n  id: \'pipeline-*\'\n)\nputs response\n```\n\n```\nconst response = await client.ingest.deletePipeline({\n  id: "pipeline-*",\n});\nconsole.log(response);\n```\n\n```\nDELETE /_ingest/pipeline/pipeline-*\n```\n\n#### Delete all ingest pipelines\n\n```\nresp = client.ingest.delete_pipeline(\n    id="*",\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.delete_pipeline(\n  id: \'*\'\n)\nputs response\n```\n\n```\nconst response = await client.ingest.deletePipeline({\n  id: "*",\n});\nconsole.log(response);\n```\n\n```\nDELETE /_ingest/pipeline/*\n```\n',
          title: 'Delete pipeline API',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/delete-pipeline-api.html',
          productName: 'elasticsearch',
          score: 127.01923,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/delete-pipeline-api.html',
        title: 'Delete pipeline API',
        score: 127.01923,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Delete pipeline API\\n\\nDeletes one or more existing ingest pipeline.\\n\\n```\\nresp = client.ingest.delete_pipeline(\\n    id=\\"my-pipeline-id\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.delete_pipeline(\\n  id: \'my-pipeline-id\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.deletePipeline({\\n  id: \\"my-pipeline-id\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nDELETE /_ingest/pipeline/my-pipeline-id\\n```\\n\\n### Request\\n\\n`DELETE /_ingest/pipeline/<pipeline>`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to use this API.\\n\\n### Path parameters\\n\\n* `<pipeline>`\\n\\n  (Required, string) Pipeline ID or wildcard expression of pipeline IDs used to limit the request.\\n\\n  To delete all ingest pipelines in a cluster, use a value of `*`.\\n\\n### Query parameters\\n\\n* `master_timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n* `timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for a response from all relevant nodes in the cluster after updating the cluster metadata. If no response is received before the timeout expires, the cluster metadata update still applies but the response will indicate that it was not completely acknowledged. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n### Examples\\n\\n#### Delete a specific ingest pipeline\\n\\n```\\nresp = client.ingest.delete_pipeline(\\n    id=\\"pipeline-one\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.delete_pipeline(\\n  id: \'pipeline-one\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.deletePipeline({\\n  id: \\"pipeline-one\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nDELETE /_ingest/pipeline/pipeline-one\\n```\\n\\n#### Delete ingest pipelines using a wildcard expression\\n\\n```\\nresp = client.ingest.delete_pipeline(\\n    id=\\"pipeline-*\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.delete_pipeline(\\n  id: \'pipeline-*\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.deletePipeline({\\n  id: \\"pipeline-*\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nDELETE /_ingest/pipeline/pipeline-*\\n```\\n\\n#### Delete all ingest pipelines\\n\\n```\\nresp = client.ingest.delete_pipeline(\\n    id=\\"*\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.delete_pipeline(\\n  id: \'*\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.deletePipeline({\\n  id: \\"*\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nDELETE /_ingest/pipeline/*\\n```\\n","title":"Delete pipeline API","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/delete-pipeline-api.html","productName":"elasticsearch","score":127.01923}',
        truncated: {
          truncatedText:
            '{"content":"## Delete pipeline API\\n\\nDeletes one or more existing ingest pipeline.\\n\\n```\\nresp = client.ingest.delete_pipeline(\\n    id=\\"my-pipeline-id\\",\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.ingest.delete_pipeline(\\n  id: \'my-pipeline-id\'\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.ingest.deletePipeline({\\n  id: \\"my-pipeline-id\\",\\n});\\nconsole.log(response);\\n```\\n\\n```\\nDELETE /_ingest/pipeline/my-pipeline-id\\n```\\n\\n### Request\\n\\n`DELETE /_ingest/pipeline/<pipeline>`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster \\"Cluster privileges\\") to use this API.\\n\\n### Path parameters\\n\\n* `<pipeline>`\\n\\n  (Required, string) Pipeline ID or wildcard expression of pipeline IDs used to limit the request.\\n\\n  To delete all ingest pipelines in a cluster, use a value of `*`.\\n\\n### Query parameters\\n\\n* `master_timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\\n\\n* `timeout`\\n\\n  (Optional, [time units](api-conventions.html#time-units \\"Time units\\")) Period to wait for a response from all relevant nodes in the cluster after updating the cluster... <truncated>',
          originalTokenCount: 864,
          truncatedTokenCount: 400,
        },
        llmScore: 4,
      },
      {
        selected: false,
        document: {
          content:
            '## Simulate ingest API\n\nExecutes ingest pipelines against a set of provided documents, optionally with substitute pipeline definitions. This API is meant to be used for troubleshooting or pipeline development, as it does not actually index any data into Elasticsearch.\n\n```\nresp = client.perform_request(\n    "POST",\n    "/_ingest/_simulate",\n    headers={"Content-Type": "application/json"},\n    body={\n        "docs": [\n            {\n                "_index": "my-index",\n                "_id": "id",\n                "_source": {\n                    "foo": "bar"\n                }\n            },\n            {\n                "_index": "my-index",\n                "_id": "id",\n                "_source": {\n                    "foo": "rab"\n                }\n            }\n        ],\n        "pipeline_substitutions": {\n            "my-pipeline": {\n                "processors": [\n                    {\n                        "set": {\n                            "field": "field3",\n                            "value": "value3"\n                        }\n                    }\n                ]\n            }\n        },\n        "component_template_substitutions": {\n            "my-component-template": {\n                "template": {\n                    "mappings": {\n                        "dynamic": "true",\n                        "properties": {\n                            "field3": {\n                                "type": "keyword"\n                            }\n                        }\n                    },\n                    "settings": {\n                        "index": {\n                            "default_pipeline": "my-pipeline"\n                        }\n                    }\n                }\n            }\n        },\n        "index_template_substitutions": {\n            "my-index-template": {\n                "index_patterns": [\n                    "my-index-*"\n                ],\n                "composed_of": [\n                    "component_template_1",\n                    "component_template_2"\n                ]\n            }\n        },\n        "mapping_addition": {\n            "dynamic": "strict",\n            "properties": {\n                "foo": {\n                    "type": "keyword"\n                }\n            }\n        }\n    },\n)\nprint(resp)\n```\n\n```\nconst response = await client.simulate.ingest({\n  body: {\n    docs: [\n      {\n        _index: "my-index",\n        _id: "id",\n        _source: {\n          foo: "bar",\n        },\n      },\n      {\n        _index: "my-index",\n        _id: "id",\n        _source: {\n          foo: "rab",\n        },\n      },\n    ],\n    pipeline_substitutions: {\n      "my-pipeline": {\n        processors: [\n          {\n            set: {\n              field: "field3",\n              value: "value3",\n            },\n          },\n        ],\n      },\n    },\n    component_template_substitutions: {\n      "my-component-template": {\n        template: {\n          mappings: {\n            dynamic: "true",\n            properties: {\n              field3: {\n                type: "keyword",\n              },\n            },\n          },\n          settings: {\n            index: {\n              default_pipeline: "my-pipeline",\n            },\n          },\n        },\n      },\n    },\n    index_template_substitutions: {\n      "my-index-template": {\n        index_patterns: ["my-index-*"],\n        composed_of: ["component_template_1", "component_template_2"],\n      },\n    },\n    mapping_addition: {\n      dynamic: "strict",\n      properties: {\n        foo: {\n          type: "keyword",\n        },\n      },\n    },\n  },\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/_simulate\n{\n  "docs": [\n    {\n      "_index": "my-index",\n      "_id": "id",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "my-index",\n      "_id": "id",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ],\n  "pipeline_substitutions": { \n    "my-pipeline": {\n      "processors": [\n        {\n          "set": {\n            "field": "field3",\n            "value": "value3"\n          }\n        }\n      ]\n    }\n  },\n  "component_template_substitutions": { \n    "my-component-template": {\n      "template": {\n        "mappings": {\n          "dynamic": "true",\n          "properties": {\n            "field3": {\n              "type": "keyword"\n            }\n          }\n        },\n        "settings": {\n          "index": {\n            "default_pipeline": "my-pipeline"\n          }\n        }\n      }\n    }\n  },\n  "index_template_substitutions": { \n    "my-index-template": {\n      "index_patterns": ["my-index-*"],\n      "composed_of": ["component_template_1", "component_template_2"]\n    }\n  },\n  "mapping_addition": { \n    "dynamic": "strict",\n    "properties": {\n      "foo": {\n        "type": "keyword"\n      }\n    }\n  }\n}\n```\n\n|                |                                                                                                                                                                                                                                                                     |\n| :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [**](#CO870-1) | This replaces the existing `my-pipeline` pipeline with the contents given here for the duration of this request.                                                                                                                                                    |\n| [**](#CO870-2) | This replaces the existing `my-component-template` component template with the contents given here for the duration of this request. These templates can be used to change the pipeline(s) used, or to modify the mapping that will be used to validate the result. |\n| [**](#CO870-3) | This replaces the existing `my-index-template` index template with the contents given here for the duration of this request. These templates can be used to change the pipeline(s) used, or to modify the mapping that will be used to validate the result.         |\n| [**](#CO870-4) | This mapping is merged into the index’s final mapping just before validation. It is used only for the duration of this request.                                                                                                                                     |\n\n### Request\n\n`POST /_ingest/_simulate`\n\n`GET /_ingest/_simulate`\n\n`POST /_ingest/<target>/_simulate`\n\n`GET /_ingest/<target>/_simulate`\n\n### Prerequisites\n\n* If the Elasticsearch security features are enabled, you must have the `index` or `create` [index privileges](security-privileges.html#privileges-list-indices "Indices privileges") to use this API.\n\n### Description\n\nThe simulate ingest API simulates ingesting data into an index. It executes the default and final pipeline for that index against a set of documents provided in the body of the request. If a pipeline contains a [reroute processor](reroute-processor.html "Reroute processor"), it follows that reroute processor to the new index, executing that index’s pipelines as well the same way that a non-simulated ingest would. No data is indexed into Elasticsearch. Instead, the transformed document is returned, along with the list of pipelines that have been executed and the name of the index where the document would have been indexed if this were not a simulation. The transformed document is validated against the mappings that would apply to this index, and any validation error is reported in the result.\n\nThis API differs from the [simulate pipeline API](simulate-pipeline-api.html "Simulate pipeline API") in that you specify a single pipeline for that API, and it only runs that one pipeline. The simulate pipeline API is more useful for developing a single pipeline, while the simulate ingest API is more useful for troubleshooting the interaction of the various pipelines that get applied when ingesting into an index.\n\nBy default, the pipeline definitions that are currently in the system are used. However, you can supply substitute pipeline definitions in the body of the request. These will be used in place of the pipeline definitions that are already in the system. This can be used to replace existing pipeline definitions or to create new ones. The pipeline substitutions are only used within this request.\n\n### Path parameters\n\n* `<target>`\n\n  (Optional, string) The index to simulate ingesting into. This can be overridden by specifying an index on each document. If you provide a \\<target> in the request path, it is used for any documents that don’t explicitly specify an index argument.\n\n### Query parameters\n\n* `pipeline`\n\n  (Optional, string) Pipeline to use as the default pipeline. This can be used to override the default pipeline of the index being ingested into.\n\n### Request body\n\n* `docs`\n\n  (Required, array of objects) Sample documents to test in the pipeline.\n\n  Properties of `docs` objects\n\n  * `_id`\n\n    (Optional, string) Unique identifier for the document.\n\n  * `_index`\n\n    (Optional, string) Name of the index that the document will be ingested into.\n\n  * `_source`\n\n    (Required, object) JSON body for the document.\n\n* `pipeline_substitutions`\n\n  (Optional, map of strings to objects) Map of pipeline IDs to substitute pipeline definition objects.\n\n  Properties of pipeline definition objects\n\n  * `description`\n\n    (Optional, string) Description of the ingest pipeline.\n\n  * `on_failure`\n\n    (Optional, array of [processor](processors.html "Ingest processor reference") objects) Processors to run immediately after a processor failure.\n\n    Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\n\n  * `processors`\n\n    (Required, array of [processor](processors.html "Ingest processor reference") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\n\n  * `version`\n\n    (Optional, integer) Version number used by external systems to track ingest pipelines.\n\n    See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params "Query parameters") parameter above for how the version attribute is used.\n\n  * `_meta`\n\n    (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\n\n  * `deprecated`\n\n    (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\n\n* `component_template_substitutions`\n\n  (Optional, map of strings to objects) Map of component template names to substitute component template definition objects.\n\n  Properties of component template definition objects\n\n  * `template`\n\n    (Required, object) This is the template to be applied, may optionally include a `mappings`, `settings`, or `aliases` configuration.\n\n    Properties of `template`\n\n    * `aliases`\n\n      (Optional, object of objects) Aliases to add.\n\n      If the index template includes a `data_stream` object, these are data stream aliases. Otherwise, these are index aliases. Data stream aliases ignore the `index_routing`, `routing`, and `search_routing` options.\n\n      Properties of `aliases` objects\n\n      * `<alias>`\n\n        (Required, object) The key is the alias name. Index alias names support [date math](api-conventions.html#api-date-math-index-names "Date math support in index and index alias names").\n\n        The object body contains options for the alias. Supports an empty object.\n\n        Properties of `<alias>`\n\n        * `filter`\n\n          (Optional, [Query DSL object](query-dsl.html "Query DSL")) Query used to limit documents the alias can access.\n\n        * `index_routing`\n\n          (Optional, string) Value used to route indexing operations to a specific shard. If specified, this overwrites the `routing` value for indexing operations.\n\n        * `is_hidden`\n\n          (Optional, Boolean) If `true`, the alias is [hidden](api-conventions.html#multi-hidden "Hidden data streams and indices"). Defaults to `false`. All indices for the alias must have the same `is_hidden` value.\n\n        * `is_write_index`\n\n          (Optional, Boolean) If `true`, the index is the [write index](aliases.html#write-index "Write index") for the alias. Defaults to `false`.\n\n        * `routing`\n\n          (Optional, string) Value used to route indexing and search operations to a specific shard.\n\n        * `search_routing`\n\n          (Optional, string) Value used to route search operations to a specific shard. If specified, this overwrites the `routing` value for search operations.\n\n    * `mappings`\n\n      (Optional, [mapping object](mapping.html "Mapping")) Mapping for fields in the index. If specified, this mapping can include:\n\n      * Field names\n      * [Field data types](mapping-types.html "Field data types")\n      * [Mapping parameters](mapping-params.html "Mapping parameters")\n\n      See [Mapping](mapping.html "Mapping").\n\n    * `settings`\n\n      (Optional, [index setting object](index-modules.html#index-modules-settings "Index settings")) Configuration options for the index. See [Index settings](index-modules.html#index-modules-settings "Index settings").\n\n  * `version`\n\n    (Optional, integer) Version number used to manage component templates externally. This number is not automatically generated or incremented by Elasticsearch.\n\n  * `allow_auto_create`\n\n    (Optional, Boolean) This setting overrides the value of the [`action.auto_create_index`](docs-index_.html#index-creation "Automatically create data streams and indices") cluster setting. If set to `true` in a template, then indices can be automatically created using that template even if auto-creation of indices is disabled via `actions.auto_create_index`. If set to `false`, then indices or data streams matching the template must always be explicitly created, and may never be automatically created.\n\n  * `_meta`\n\n    (Optional, object) Optional user metadata about the component template. May have any contents. This map is not automatically generated by Elasticsearch.\n\n  * `deprecated`\n\n    (Optional, boolean) Marks this component template as deprecated. When a deprecated component template is referenced when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\n\n* `index_template_substitutions`\n\n  (Optional, map of strings to objects) Map of index template names to substitute index template definition objects.\n\n  Properties of index template definition objects\n\n  * `composed_of`\n\n    (Optional, array of strings) An ordered list of component template names. Component templates are merged in the order specified, meaning that the last component template specified has the highest precedence. See [Composing multiple component templates](indices-put-template.html#multiple-component-templates "Composing aliases, mappings, and settings") for an example.\n\n  * `data_stream`\n\n    (Optional, object) If this object is included, the template is used to create data streams and their backing indices. Supports an empty object.\n\n    Data streams require a matching index template with a `data_stream` object. See [create an index template](set-up-a-data-stream.html#create-index-template "Create an index template").\n\n    Properties of `data_stream`\n\n    * `allow_custom_routing`\n\n      (Optional, Boolean) If `true`, the data stream supports [custom routing](mapping-routing-field.html "_routing field"). Defaults to `false`.\n\n    * `hidden`\n\n      (Optional, Boolean) If `true`, the data stream is [hidden](api-conventions.html#multi-hidden "Hidden data streams and indices"). Defaults to `false`.\n\n    * `index_mode`\n\n      (Optional, string) Type of data stream to create. Valid values are `null` (standard data stream), `time_series` ([time series data stream](tsds.html "Time series data stream (TSDS)")) and `logsdb` ([logs data stream](logs-data-stream.html "Logs data stream")).\n\n      The template’s `index_mode` sets the `index.mode` of the backing index.\n\n  * `index_patterns`\n\n    (Required, array of strings) Array of wildcard (`*`) expressions used to match the names of data streams and indices during creation.\n\n    Elasticsearch includes several built-in index templates. To avoid naming collisions with these templates, see [Avoid index pattern collisions](index-templates.html#avoid-index-pattern-collisions "Avoid index pattern collisions").\n\n  * `_meta`\n\n    (Optional, object) Optional user metadata about the index template. May have any contents. This map is not automatically generated by Elasticsearch.\n\n  * `priority`\n\n    (Optional, integer) Priority to determine index template precedence when a new data stream or index is created. The index template with the highest priority is chosen. If no priority is specified the template is treated as though it is of priority 0 (lowest priority). This number is not automatically generated by Elasticsearch.\n\n  * `template`\n\n    (Optional, object) Template to be applied. It may optionally include an `aliases`, `mappings`, or `settings` configuration.\n\n    Properties of `template`\n\n    * `aliases`\n\n      (Optional, object of objects) Aliases to add.\n\n      If the index template includes a `data_stream` object, these are data stream aliases. Otherwise, these are index aliases. Data stream aliases ignore the `index_routing`, `routing`, and `search_routing` options.\n\n      Properties of `aliases` objects\n\n      * `<alias>`\n\n        (Required, object) The key is the alias name. Index alias names support [date math](api-conventions.html#api-date-math-index-names "Date math support in index and index alias names").\n\n        The object body contains options for the alias. Supports an empty object.\n\n        Properties of `<alias>`\n\n        * `filter`\n\n          (Optional, [Query DSL object](query-dsl.html "Query DSL")) Query used to limit documents the alias can access.\n\n        * `index_routing`\n\n          (Optional, string) Value used to route indexing operations to a specific shard. If specified, this overwrites the `routing` value for indexing operations.\n\n        * `is_hidden`\n\n          (Optional, Boolean) If `true`, the alias is [hidden](api-conventions.html#multi-hidden "Hidden data streams and indices"). Defaults to `false`. All indices for the alias must have the same `is_hidden` value.\n\n        * `is_write_index`\n\n          (Optional, Boolean) If `true`, the index is the [write index](aliases.html#write-index "Write index") for the alias. Defaults to `false`.\n\n        * `routing`\n\n          (Optional, string) Value used to route indexing and search operations to a specific shard.\n\n        * `search_routing`\n\n          (Optional, string) Value used to route search operations to a specific shard. If specified, this overwrites the `routing` value for search operations.\n\n    * `mappings`\n\n      (Optional, [mapping object](mapping.html "Mapping")) Mapping for fields in the index. If specified, this mapping can include:\n\n      * Field names\n      * [Field data types](mapping-types.html "Field data types")\n      * [Mapping parameters](mapping-params.html "Mapping parameters")\n\n      See [Mapping](mapping.html "Mapping").\n\n    * `settings`\n\n      (Optional, [index setting object](index-modules.html#index-modules-settings "Index settings")) Configuration options for the index. See [Index settings](index-modules.html#index-modules-settings "Index settings").\n\n  * `version`\n\n    (Optional, integer) Version number used to manage index templates externally. This number is not automatically generated by Elasticsearch.\n\n  * `deprecated`\n\n    (Optional, boolean) Marks this index template as deprecated. When creating or updating a non-deprecated index template that uses deprecated components, Elasticsearch will emit a deprecation warning.\n\n* `mapping_addition`\n\n  (Optional, [mapping object](mapping.html "Mapping")) Definition of a mapping that will be merged into the index’s mapping for validation during the course of this request.\n\n### Examples\n\n#### Use pre-existing pipeline definitions\n\nIn this example the index `index` has a default pipeline called `my-pipeline` and a final pipeline called `my-final-pipeline`. Since both documents are being ingested into `index`, both pipelines are executed using the pipeline definitions that are already in the system.\n\n```\nresp = client.perform_request(\n    "POST",\n    "/_ingest/_simulate",\n    headers={"Content-Type": "application/json"},\n    body={\n        "docs": [\n            {\n                "_index": "my-index",\n                "_id": "123",\n                "_source": {\n                    "foo": "bar"\n                }\n            },\n            {\n                "_index": "my-index",\n                "_id": "456",\n                "_source": {\n                    "foo": "rab"\n                }\n            }\n        ]\n    },\n)\nprint(resp)\n```\n\n```\nresponse = client.simulate.ingest(\n  body: {\n    docs: [\n      {\n        _index: \'my-index\',\n        _id: \'123\',\n        _source: {\n          foo: \'bar\'\n        }\n      },\n      {\n        _index: \'my-index\',\n        _id: \'456\',\n        _source: {\n          foo: \'rab\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.simulate.ingest({\n  body: {\n    docs: [\n      {\n        _index: "my-index",\n        _id: "123",\n        _source: {\n          foo: "bar",\n        },\n      },\n      {\n        _index: "my-index",\n        _id: "456",\n        _source: {\n          foo: "rab",\n        },\n      },\n    ],\n  },\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/_simulate\n{\n  "docs": [\n    {\n      "_index": "my-index",\n      "_id": "123",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "my-index",\n      "_id": "456",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ]\n}\n```\n\nThe API returns the following response:\n\n```\n{\n   "docs": [\n      {\n         "doc": {\n            "_id": "123",\n            "_index": "my-index",\n            "_version": -3,\n            "_source": {\n               "field1": "value1",\n               "field2": "value2",\n               "foo": "bar"\n            },\n            "executed_pipelines": [\n               "my-pipeline",\n               "my-final-pipeline"\n            ]\n         }\n      },\n      {\n         "doc": {\n            "_id": "456",\n            "_index": "my-index",\n            "_version": -3,\n            "_source": {\n               "field1": "value1",\n               "field2": "value2",\n               "foo": "rab"\n            },\n            "executed_pipelines": [\n               "my-pipeline",\n               "my-final-pipeline"\n            ]\n         }\n      }\n   ]\n}\n```\n\n#### Specify a pipeline substitution in the request body\n\nIn this example the index `my-index` has a default pipeline called `my-pipeline` and a final pipeline called `my-final-pipeline`. But a substitute definition of `my-pipeline` is provided in `pipeline_substitutions`. The substitute `my-pipeline` will be used in place of the `my-pipeline` that is in the system, and then the `my-final-pipeline` that is already defined in the system will be executed.\n\n```\nresp = client.perform_request(\n    "POST",\n    "/_ingest/_simulate",\n    headers={"Content-Type": "application/json"},\n    body={\n        "docs": [\n            {\n                "_index": "my-index",\n                "_id": "123",\n                "_source": {\n                    "foo": "bar"\n                }\n            },\n            {\n                "_index": "my-index",\n                "_id": "456",\n                "_source": {\n                    "foo": "rab"\n                }\n            }\n        ],\n        "pipeline_substitutions": {\n            "my-pipeline": {\n                "processors": [\n                    {\n                        "uppercase": {\n                            "field": "foo"\n                        }\n                    }\n                ]\n            }\n        }\n    },\n)\nprint(resp)\n```\n\n```\nresponse = client.simulate.ingest(\n  body: {\n    docs: [\n      {\n        _index: \'my-index\',\n        _id: \'123\',\n        _source: {\n          foo: \'bar\'\n        }\n      },\n      {\n        _index: \'my-index\',\n        _id: \'456\',\n        _source: {\n          foo: \'rab\'\n        }\n      }\n    ],\n    pipeline_substitutions: {\n      "my-pipeline": {\n        processors: [\n          {\n            uppercase: {\n              field: \'foo\'\n            }\n          }\n        ]\n      }\n    }\n  }\n)\nputs response\n```\n\n```\nconst response = await client.simulate.ingest({\n  body: {\n    docs: [\n      {\n        _index: "my-index",\n        _id: "123",\n        _source: {\n          foo: "bar",\n        },\n      },\n      {\n        _index: "my-index",\n        _id: "456",\n        _source: {\n          foo: "rab",\n        },\n      },\n    ],\n    pipeline_substitutions: {\n      "my-pipeline": {\n        processors: [\n          {\n            uppercase: {\n              field: "foo",\n            },\n          },\n        ],\n      },\n    },\n  },\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/_simulate\n{\n  "docs": [\n    {\n      "_index": "my-index",\n      "_id": "123",\n      "_source": {\n        "foo": "bar"\n      }\n    },\n    {\n      "_index": "my-index",\n      "_id": "456",\n      "_source": {\n        "foo": "rab"\n      }\n    }\n  ],\n  "pipeline_substitutions": {\n    "my-pipeline": {\n      "processors": [\n        {\n          "uppercase": {\n            "field": "foo"\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nThe API returns the following response:\n\n```\n{\n   "docs": [\n      {\n         "doc": {\n            "_id": "123",\n            "_index": "my-index",\n            "_version": -3,\n            "_source": {\n               "field2": "value2",\n               "foo": "BAR"\n            },\n            "executed_pipelines": [\n               "my-pipeline",\n               "my-final-pipeline"\n            ]\n         }\n      },\n      {\n         "doc": {\n            "_id": "456",\n            "_index": "my-index",\n            "_version": -3,\n            "_source": {\n               "field2": "value2",\n               "foo": "RAB"\n            },\n            "executed_pipelines": [\n               "my-pipeline",\n               "my-final-pipeline"\n            ]\n         }\n      }\n   ]\n}\n```\n\n#### Specify a component template substitution in the request body\n\nIn this example, imagine that the index `my-index` has a strict mapping with only the `foo` keyword field defined. Say that field mapping came from a component template named `my-mappings-template`. We want to test adding a new field, `bar`. So a substitute definition of `my-mappings-template` is provided in `component_template_substitutions`. The substitute `my-mappings-template` will be used in place of the existing mapping for `my-index` and in place of the `my-mappings-template` that is in the system.\n\n```\nresp = client.perform_request(\n    "POST",\n    "/_ingest/_simulate",\n    headers={"Content-Type": "application/json"},\n    body={\n        "docs": [\n            {\n                "_index": "my-index",\n                "_id": "123",\n                "_source": {\n                    "foo": "foo"\n                }\n            },\n            {\n                "_index": "my-index",\n                "_id": "456",\n                "_source": {\n                    "bar": "rab"\n                }\n            }\n        ],\n        "component_template_substitutions": {\n            "my-mappings_template": {\n                "template": {\n                    "mappings": {\n                        "dynamic": "strict",\n                        "properties": {\n                            "foo": {\n                                "type": "keyword"\n                            },\n                            "bar": {\n                                "type": "keyword"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    },\n)\nprint(resp)\n```\n\n```\nconst response = await client.simulate.ingest({\n  body: {\n    docs: [\n      {\n        _index: "my-index",\n        _id: "123",\n        _source: {\n          foo: "foo",\n        },\n      },\n      {\n        _index: "my-index",\n        _id: "456",\n        _source: {\n          bar: "rab",\n        },\n      },\n    ],\n    component_template_substitutions: {\n      "my-mappings_template": {\n        template: {\n          mappings: {\n            dynamic: "strict",\n            properties: {\n              foo: {\n                type: "keyword",\n              },\n              bar: {\n                type: "keyword",\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n});\nconsole.log(response);\n```\n\n```\nPOST /_ingest/_simulate\n{\n  "docs": [\n    {\n      "_index": "my-index",\n      "_id": "123",\n      "_source": {\n        "foo": "foo"\n      }\n    },\n    {\n      "_index": "my-index",\n      "_id": "456",\n      "_source": {\n        "bar": "rab"\n      }\n    }\n  ],\n  "component_template_substitutions": {\n    "my-mappings_template": {\n      "template": {\n        "mappings": {\n          "dynamic": "strict",\n          "properties": {\n            "foo": {\n              "type": "keyword"\n            },\n            "bar": {\n              "type": "keyword"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe API returns the following response:\n\n```\n{\n   "docs": [\n      {\n         "doc": {\n            "_id": "123",\n            "_index": "my-index",\n            "_version": -3,\n            "_source": {\n               "foo": "foo"\n            },\n            "executed_pipelines": []\n         }\n      },\n      {\n         "doc": {\n            "_id": "456",\n            "_index": "my-index",\n            "_version": -3,\n            "_source": {\n               "bar": "rab"\n            },\n            "executed_pipelines": []\n         }\n      }\n   ]\n}\n```\n',
          title: 'Simulate ingest API',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/simulate-ingest-api.html',
          productName: 'elasticsearch',
          score: 125.13092,
        },
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/simulate-ingest-api.html',
        title: 'Simulate ingest API',
        score: 125.13092,
        source: {
          product_documentation: {},
        },
        text: '{"content":"## Simulate ingest API\\n\\nExecutes ingest pipelines against a set of provided documents, optionally with substitute pipeline definitions. This API is meant to be used for troubleshooting or pipeline development, as it does not actually index any data into Elasticsearch.\\n\\n```\\nresp = client.perform_request(\\n    \\"POST\\",\\n    \\"/_ingest/_simulate\\",\\n    headers={\\"Content-Type\\": \\"application/json\\"},\\n    body={\\n        \\"docs\\": [\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"id\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"bar\\"\\n                }\\n            },\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"id\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"rab\\"\\n                }\\n            }\\n        ],\\n        \\"pipeline_substitutions\\": {\\n            \\"my-pipeline\\": {\\n                \\"processors\\": [\\n                    {\\n                        \\"set\\": {\\n                            \\"field\\": \\"field3\\",\\n                            \\"value\\": \\"value3\\"\\n                        }\\n                    }\\n                ]\\n            }\\n        },\\n        \\"component_template_substitutions\\": {\\n            \\"my-component-template\\": {\\n                \\"template\\": {\\n                    \\"mappings\\": {\\n                        \\"dynamic\\": \\"true\\",\\n                        \\"properties\\": {\\n                            \\"field3\\": {\\n                                \\"type\\": \\"keyword\\"\\n                            }\\n                        }\\n                    },\\n                    \\"settings\\": {\\n                        \\"index\\": {\\n                            \\"default_pipeline\\": \\"my-pipeline\\"\\n                        }\\n                    }\\n                }\\n            }\\n        },\\n        \\"index_template_substitutions\\": {\\n            \\"my-index-template\\": {\\n                \\"index_patterns\\": [\\n                    \\"my-index-*\\"\\n                ],\\n                \\"composed_of\\": [\\n                    \\"component_template_1\\",\\n                    \\"component_template_2\\"\\n                ]\\n            }\\n        },\\n        \\"mapping_addition\\": {\\n            \\"dynamic\\": \\"strict\\",\\n            \\"properties\\": {\\n                \\"foo\\": {\\n                    \\"type\\": \\"keyword\\"\\n                }\\n            }\\n        }\\n    },\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.simulate.ingest({\\n  body: {\\n    docs: [\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"id\\",\\n        _source: {\\n          foo: \\"bar\\",\\n        },\\n      },\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"id\\",\\n        _source: {\\n          foo: \\"rab\\",\\n        },\\n      },\\n    ],\\n    pipeline_substitutions: {\\n      \\"my-pipeline\\": {\\n        processors: [\\n          {\\n            set: {\\n              field: \\"field3\\",\\n              value: \\"value3\\",\\n            },\\n          },\\n        ],\\n      },\\n    },\\n    component_template_substitutions: {\\n      \\"my-component-template\\": {\\n        template: {\\n          mappings: {\\n            dynamic: \\"true\\",\\n            properties: {\\n              field3: {\\n                type: \\"keyword\\",\\n              },\\n            },\\n          },\\n          settings: {\\n            index: {\\n              default_pipeline: \\"my-pipeline\\",\\n            },\\n          },\\n        },\\n      },\\n    },\\n    index_template_substitutions: {\\n      \\"my-index-template\\": {\\n        index_patterns: [\\"my-index-*\\"],\\n        composed_of: [\\"component_template_1\\", \\"component_template_2\\"],\\n      },\\n    },\\n    mapping_addition: {\\n      dynamic: \\"strict\\",\\n      properties: {\\n        foo: {\\n          type: \\"keyword\\",\\n        },\\n      },\\n    },\\n  },\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"id\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ],\\n  \\"pipeline_substitutions\\": { \\n    \\"my-pipeline\\": {\\n      \\"processors\\": [\\n        {\\n          \\"set\\": {\\n            \\"field\\": \\"field3\\",\\n            \\"value\\": \\"value3\\"\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \\"component_template_substitutions\\": { \\n    \\"my-component-template\\": {\\n      \\"template\\": {\\n        \\"mappings\\": {\\n          \\"dynamic\\": \\"true\\",\\n          \\"properties\\": {\\n            \\"field3\\": {\\n              \\"type\\": \\"keyword\\"\\n            }\\n          }\\n        },\\n        \\"settings\\": {\\n          \\"index\\": {\\n            \\"default_pipeline\\": \\"my-pipeline\\"\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"index_template_substitutions\\": { \\n    \\"my-index-template\\": {\\n      \\"index_patterns\\": [\\"my-index-*\\"],\\n      \\"composed_of\\": [\\"component_template_1\\", \\"component_template_2\\"]\\n    }\\n  },\\n  \\"mapping_addition\\": { \\n    \\"dynamic\\": \\"strict\\",\\n    \\"properties\\": {\\n      \\"foo\\": {\\n        \\"type\\": \\"keyword\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\n|                |                                                                                                                                                                                                                                                                     |\\n| :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| [**](#CO870-1) | This replaces the existing `my-pipeline` pipeline with the contents given here for the duration of this request.                                                                                                                                                    |\\n| [**](#CO870-2) | This replaces the existing `my-component-template` component template with the contents given here for the duration of this request. These templates can be used to change the pipeline(s) used, or to modify the mapping that will be used to validate the result. |\\n| [**](#CO870-3) | This replaces the existing `my-index-template` index template with the contents given here for the duration of this request. These templates can be used to change the pipeline(s) used, or to modify the mapping that will be used to validate the result.         |\\n| [**](#CO870-4) | This mapping is merged into the index’s final mapping just before validation. It is used only for the duration of this request.                                                                                                                                     |\\n\\n### Request\\n\\n`POST /_ingest/_simulate`\\n\\n`GET /_ingest/_simulate`\\n\\n`POST /_ingest/<target>/_simulate`\\n\\n`GET /_ingest/<target>/_simulate`\\n\\n### Prerequisites\\n\\n* If the Elasticsearch security features are enabled, you must have the `index` or `create` [index privileges](security-privileges.html#privileges-list-indices \\"Indices privileges\\") to use this API.\\n\\n### Description\\n\\nThe simulate ingest API simulates ingesting data into an index. It executes the default and final pipeline for that index against a set of documents provided in the body of the request. If a pipeline contains a [reroute processor](reroute-processor.html \\"Reroute processor\\"), it follows that reroute processor to the new index, executing that index’s pipelines as well the same way that a non-simulated ingest would. No data is indexed into Elasticsearch. Instead, the transformed document is returned, along with the list of pipelines that have been executed and the name of the index where the document would have been indexed if this were not a simulation. The transformed document is validated against the mappings that would apply to this index, and any validation error is reported in the result.\\n\\nThis API differs from the [simulate pipeline API](simulate-pipeline-api.html \\"Simulate pipeline API\\") in that you specify a single pipeline for that API, and it only runs that one pipeline. The simulate pipeline API is more useful for developing a single pipeline, while the simulate ingest API is more useful for troubleshooting the interaction of the various pipelines that get applied when ingesting into an index.\\n\\nBy default, the pipeline definitions that are currently in the system are used. However, you can supply substitute pipeline definitions in the body of the request. These will be used in place of the pipeline definitions that are already in the system. This can be used to replace existing pipeline definitions or to create new ones. The pipeline substitutions are only used within this request.\\n\\n### Path parameters\\n\\n* `<target>`\\n\\n  (Optional, string) The index to simulate ingesting into. This can be overridden by specifying an index on each document. If you provide a \\\\<target> in the request path, it is used for any documents that don’t explicitly specify an index argument.\\n\\n### Query parameters\\n\\n* `pipeline`\\n\\n  (Optional, string) Pipeline to use as the default pipeline. This can be used to override the default pipeline of the index being ingested into.\\n\\n### Request body\\n\\n* `docs`\\n\\n  (Required, array of objects) Sample documents to test in the pipeline.\\n\\n  Properties of `docs` objects\\n\\n  * `_id`\\n\\n    (Optional, string) Unique identifier for the document.\\n\\n  * `_index`\\n\\n    (Optional, string) Name of the index that the document will be ingested into.\\n\\n  * `_source`\\n\\n    (Required, object) JSON body for the document.\\n\\n* `pipeline_substitutions`\\n\\n  (Optional, map of strings to objects) Map of pipeline IDs to substitute pipeline definition objects.\\n\\n  Properties of pipeline definition objects\\n\\n  * `description`\\n\\n    (Optional, string) Description of the ingest pipeline.\\n\\n  * `on_failure`\\n\\n    (Optional, array of [processor](processors.html \\"Ingest processor reference\\") objects) Processors to run immediately after a processor failure.\\n\\n    Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\\n\\n  * `processors`\\n\\n    (Required, array of [processor](processors.html \\"Ingest processor reference\\") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\\n\\n  * `version`\\n\\n    (Optional, integer) Version number used by external systems to track ingest pipelines.\\n\\n    See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params \\"Query parameters\\") parameter above for how the version attribute is used.\\n\\n  * `_meta`\\n\\n    (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\\n\\n  * `deprecated`\\n\\n    (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\\n\\n* `component_template_substitutions`\\n\\n  (Optional, map of strings to objects) Map of component template names to substitute component template definition objects.\\n\\n  Properties of component template definition objects\\n\\n  * `template`\\n\\n    (Required, object) This is the template to be applied, may optionally include a `mappings`, `settings`, or `aliases` configuration.\\n\\n    Properties of `template`\\n\\n    * `aliases`\\n\\n      (Optional, object of objects) Aliases to add.\\n\\n      If the index template includes a `data_stream` object, these are data stream aliases. Otherwise, these are index aliases. Data stream aliases ignore the `index_routing`, `routing`, and `search_routing` options.\\n\\n      Properties of `aliases` objects\\n\\n      * `<alias>`\\n\\n        (Required, object) The key is the alias name. Index alias names support [date math](api-conventions.html#api-date-math-index-names \\"Date math support in index and index alias names\\").\\n\\n        The object body contains options for the alias. Supports an empty object.\\n\\n        Properties of `<alias>`\\n\\n        * `filter`\\n\\n          (Optional, [Query DSL object](query-dsl.html \\"Query DSL\\")) Query used to limit documents the alias can access.\\n\\n        * `index_routing`\\n\\n          (Optional, string) Value used to route indexing operations to a specific shard. If specified, this overwrites the `routing` value for indexing operations.\\n\\n        * `is_hidden`\\n\\n          (Optional, Boolean) If `true`, the alias is [hidden](api-conventions.html#multi-hidden \\"Hidden data streams and indices\\"). Defaults to `false`. All indices for the alias must have the same `is_hidden` value.\\n\\n        * `is_write_index`\\n\\n          (Optional, Boolean) If `true`, the index is the [write index](aliases.html#write-index \\"Write index\\") for the alias. Defaults to `false`.\\n\\n        * `routing`\\n\\n          (Optional, string) Value used to route indexing and search operations to a specific shard.\\n\\n        * `search_routing`\\n\\n          (Optional, string) Value used to route search operations to a specific shard. If specified, this overwrites the `routing` value for search operations.\\n\\n    * `mappings`\\n\\n      (Optional, [mapping object](mapping.html \\"Mapping\\")) Mapping for fields in the index. If specified, this mapping can include:\\n\\n      * Field names\\n      * [Field data types](mapping-types.html \\"Field data types\\")\\n      * [Mapping parameters](mapping-params.html \\"Mapping parameters\\")\\n\\n      See [Mapping](mapping.html \\"Mapping\\").\\n\\n    * `settings`\\n\\n      (Optional, [index setting object](index-modules.html#index-modules-settings \\"Index settings\\")) Configuration options for the index. See [Index settings](index-modules.html#index-modules-settings \\"Index settings\\").\\n\\n  * `version`\\n\\n    (Optional, integer) Version number used to manage component templates externally. This number is not automatically generated or incremented by Elasticsearch.\\n\\n  * `allow_auto_create`\\n\\n    (Optional, Boolean) This setting overrides the value of the [`action.auto_create_index`](docs-index_.html#index-creation \\"Automatically create data streams and indices\\") cluster setting. If set to `true` in a template, then indices can be automatically created using that template even if auto-creation of indices is disabled via `actions.auto_create_index`. If set to `false`, then indices or data streams matching the template must always be explicitly created, and may never be automatically created.\\n\\n  * `_meta`\\n\\n    (Optional, object) Optional user metadata about the component template. May have any contents. This map is not automatically generated by Elasticsearch.\\n\\n  * `deprecated`\\n\\n    (Optional, boolean) Marks this component template as deprecated. When a deprecated component template is referenced when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\\n\\n* `index_template_substitutions`\\n\\n  (Optional, map of strings to objects) Map of index template names to substitute index template definition objects.\\n\\n  Properties of index template definition objects\\n\\n  * `composed_of`\\n\\n    (Optional, array of strings) An ordered list of component template names. Component templates are merged in the order specified, meaning that the last component template specified has the highest precedence. See [Composing multiple component templates](indices-put-template.html#multiple-component-templates \\"Composing aliases, mappings, and settings\\") for an example.\\n\\n  * `data_stream`\\n\\n    (Optional, object) If this object is included, the template is used to create data streams and their backing indices. Supports an empty object.\\n\\n    Data streams require a matching index template with a `data_stream` object. See [create an index template](set-up-a-data-stream.html#create-index-template \\"Create an index template\\").\\n\\n    Properties of `data_stream`\\n\\n    * `allow_custom_routing`\\n\\n      (Optional, Boolean) If `true`, the data stream supports [custom routing](mapping-routing-field.html \\"_routing field\\"). Defaults to `false`.\\n\\n    * `hidden`\\n\\n      (Optional, Boolean) If `true`, the data stream is [hidden](api-conventions.html#multi-hidden \\"Hidden data streams and indices\\"). Defaults to `false`.\\n\\n    * `index_mode`\\n\\n      (Optional, string) Type of data stream to create. Valid values are `null` (standard data stream), `time_series` ([time series data stream](tsds.html \\"Time series data stream (TSDS)\\")) and `logsdb` ([logs data stream](logs-data-stream.html \\"Logs data stream\\")).\\n\\n      The template’s `index_mode` sets the `index.mode` of the backing index.\\n\\n  * `index_patterns`\\n\\n    (Required, array of strings) Array of wildcard (`*`) expressions used to match the names of data streams and indices during creation.\\n\\n    Elasticsearch includes several built-in index templates. To avoid naming collisions with these templates, see [Avoid index pattern collisions](index-templates.html#avoid-index-pattern-collisions \\"Avoid index pattern collisions\\").\\n\\n  * `_meta`\\n\\n    (Optional, object) Optional user metadata about the index template. May have any contents. This map is not automatically generated by Elasticsearch.\\n\\n  * `priority`\\n\\n    (Optional, integer) Priority to determine index template precedence when a new data stream or index is created. The index template with the highest priority is chosen. If no priority is specified the template is treated as though it is of priority 0 (lowest priority). This number is not automatically generated by Elasticsearch.\\n\\n  * `template`\\n\\n    (Optional, object) Template to be applied. It may optionally include an `aliases`, `mappings`, or `settings` configuration.\\n\\n    Properties of `template`\\n\\n    * `aliases`\\n\\n      (Optional, object of objects) Aliases to add.\\n\\n      If the index template includes a `data_stream` object, these are data stream aliases. Otherwise, these are index aliases. Data stream aliases ignore the `index_routing`, `routing`, and `search_routing` options.\\n\\n      Properties of `aliases` objects\\n\\n      * `<alias>`\\n\\n        (Required, object) The key is the alias name. Index alias names support [date math](api-conventions.html#api-date-math-index-names \\"Date math support in index and index alias names\\").\\n\\n        The object body contains options for the alias. Supports an empty object.\\n\\n        Properties of `<alias>`\\n\\n        * `filter`\\n\\n          (Optional, [Query DSL object](query-dsl.html \\"Query DSL\\")) Query used to limit documents the alias can access.\\n\\n        * `index_routing`\\n\\n          (Optional, string) Value used to route indexing operations to a specific shard. If specified, this overwrites the `routing` value for indexing operations.\\n\\n        * `is_hidden`\\n\\n          (Optional, Boolean) If `true`, the alias is [hidden](api-conventions.html#multi-hidden \\"Hidden data streams and indices\\"). Defaults to `false`. All indices for the alias must have the same `is_hidden` value.\\n\\n        * `is_write_index`\\n\\n          (Optional, Boolean) If `true`, the index is the [write index](aliases.html#write-index \\"Write index\\") for the alias. Defaults to `false`.\\n\\n        * `routing`\\n\\n          (Optional, string) Value used to route indexing and search operations to a specific shard.\\n\\n        * `search_routing`\\n\\n          (Optional, string) Value used to route search operations to a specific shard. If specified, this overwrites the `routing` value for search operations.\\n\\n    * `mappings`\\n\\n      (Optional, [mapping object](mapping.html \\"Mapping\\")) Mapping for fields in the index. If specified, this mapping can include:\\n\\n      * Field names\\n      * [Field data types](mapping-types.html \\"Field data types\\")\\n      * [Mapping parameters](mapping-params.html \\"Mapping parameters\\")\\n\\n      See [Mapping](mapping.html \\"Mapping\\").\\n\\n    * `settings`\\n\\n      (Optional, [index setting object](index-modules.html#index-modules-settings \\"Index settings\\")) Configuration options for the index. See [Index settings](index-modules.html#index-modules-settings \\"Index settings\\").\\n\\n  * `version`\\n\\n    (Optional, integer) Version number used to manage index templates externally. This number is not automatically generated by Elasticsearch.\\n\\n  * `deprecated`\\n\\n    (Optional, boolean) Marks this index template as deprecated. When creating or updating a non-deprecated index template that uses deprecated components, Elasticsearch will emit a deprecation warning.\\n\\n* `mapping_addition`\\n\\n  (Optional, [mapping object](mapping.html \\"Mapping\\")) Definition of a mapping that will be merged into the index’s mapping for validation during the course of this request.\\n\\n### Examples\\n\\n#### Use pre-existing pipeline definitions\\n\\nIn this example the index `index` has a default pipeline called `my-pipeline` and a final pipeline called `my-final-pipeline`. Since both documents are being ingested into `index`, both pipelines are executed using the pipeline definitions that are already in the system.\\n\\n```\\nresp = client.perform_request(\\n    \\"POST\\",\\n    \\"/_ingest/_simulate\\",\\n    headers={\\"Content-Type\\": \\"application/json\\"},\\n    body={\\n        \\"docs\\": [\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"123\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"bar\\"\\n                }\\n            },\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"456\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"rab\\"\\n                }\\n            }\\n        ]\\n    },\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.simulate.ingest(\\n  body: {\\n    docs: [\\n      {\\n        _index: \'my-index\',\\n        _id: \'123\',\\n        _source: {\\n          foo: \'bar\'\\n        }\\n      },\\n      {\\n        _index: \'my-index\',\\n        _id: \'456\',\\n        _source: {\\n          foo: \'rab\'\\n        }\\n      }\\n    ]\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.simulate.ingest({\\n  body: {\\n    docs: [\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"123\\",\\n        _source: {\\n          foo: \\"bar\\",\\n        },\\n      },\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"456\\",\\n        _source: {\\n          foo: \\"rab\\",\\n        },\\n      },\\n    ],\\n  },\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"123\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"456\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n   \\"docs\\": [\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"123\\",\\n            \\"_index\\": \\"my-index\\",\\n            \\"_version\\": -3,\\n            \\"_source\\": {\\n               \\"field1\\": \\"value1\\",\\n               \\"field2\\": \\"value2\\",\\n               \\"foo\\": \\"bar\\"\\n            },\\n            \\"executed_pipelines\\": [\\n               \\"my-pipeline\\",\\n               \\"my-final-pipeline\\"\\n            ]\\n         }\\n      },\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"456\\",\\n            \\"_index\\": \\"my-index\\",\\n            \\"_version\\": -3,\\n            \\"_source\\": {\\n               \\"field1\\": \\"value1\\",\\n               \\"field2\\": \\"value2\\",\\n               \\"foo\\": \\"rab\\"\\n            },\\n            \\"executed_pipelines\\": [\\n               \\"my-pipeline\\",\\n               \\"my-final-pipeline\\"\\n            ]\\n         }\\n      }\\n   ]\\n}\\n```\\n\\n#### Specify a pipeline substitution in the request body\\n\\nIn this example the index `my-index` has a default pipeline called `my-pipeline` and a final pipeline called `my-final-pipeline`. But a substitute definition of `my-pipeline` is provided in `pipeline_substitutions`. The substitute `my-pipeline` will be used in place of the `my-pipeline` that is in the system, and then the `my-final-pipeline` that is already defined in the system will be executed.\\n\\n```\\nresp = client.perform_request(\\n    \\"POST\\",\\n    \\"/_ingest/_simulate\\",\\n    headers={\\"Content-Type\\": \\"application/json\\"},\\n    body={\\n        \\"docs\\": [\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"123\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"bar\\"\\n                }\\n            },\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"456\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"rab\\"\\n                }\\n            }\\n        ],\\n        \\"pipeline_substitutions\\": {\\n            \\"my-pipeline\\": {\\n                \\"processors\\": [\\n                    {\\n                        \\"uppercase\\": {\\n                            \\"field\\": \\"foo\\"\\n                        }\\n                    }\\n                ]\\n            }\\n        }\\n    },\\n)\\nprint(resp)\\n```\\n\\n```\\nresponse = client.simulate.ingest(\\n  body: {\\n    docs: [\\n      {\\n        _index: \'my-index\',\\n        _id: \'123\',\\n        _source: {\\n          foo: \'bar\'\\n        }\\n      },\\n      {\\n        _index: \'my-index\',\\n        _id: \'456\',\\n        _source: {\\n          foo: \'rab\'\\n        }\\n      }\\n    ],\\n    pipeline_substitutions: {\\n      \\"my-pipeline\\": {\\n        processors: [\\n          {\\n            uppercase: {\\n              field: \'foo\'\\n            }\\n          }\\n        ]\\n      }\\n    }\\n  }\\n)\\nputs response\\n```\\n\\n```\\nconst response = await client.simulate.ingest({\\n  body: {\\n    docs: [\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"123\\",\\n        _source: {\\n          foo: \\"bar\\",\\n        },\\n      },\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"456\\",\\n        _source: {\\n          foo: \\"rab\\",\\n        },\\n      },\\n    ],\\n    pipeline_substitutions: {\\n      \\"my-pipeline\\": {\\n        processors: [\\n          {\\n            uppercase: {\\n              field: \\"foo\\",\\n            },\\n          },\\n        ],\\n      },\\n    },\\n  },\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"123\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"bar\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"456\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"rab\\"\\n      }\\n    }\\n  ],\\n  \\"pipeline_substitutions\\": {\\n    \\"my-pipeline\\": {\\n      \\"processors\\": [\\n        {\\n          \\"uppercase\\": {\\n            \\"field\\": \\"foo\\"\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n   \\"docs\\": [\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"123\\",\\n            \\"_index\\": \\"my-index\\",\\n            \\"_version\\": -3,\\n            \\"_source\\": {\\n               \\"field2\\": \\"value2\\",\\n               \\"foo\\": \\"BAR\\"\\n            },\\n            \\"executed_pipelines\\": [\\n               \\"my-pipeline\\",\\n               \\"my-final-pipeline\\"\\n            ]\\n         }\\n      },\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"456\\",\\n            \\"_index\\": \\"my-index\\",\\n            \\"_version\\": -3,\\n            \\"_source\\": {\\n               \\"field2\\": \\"value2\\",\\n               \\"foo\\": \\"RAB\\"\\n            },\\n            \\"executed_pipelines\\": [\\n               \\"my-pipeline\\",\\n               \\"my-final-pipeline\\"\\n            ]\\n         }\\n      }\\n   ]\\n}\\n```\\n\\n#### Specify a component template substitution in the request body\\n\\nIn this example, imagine that the index `my-index` has a strict mapping with only the `foo` keyword field defined. Say that field mapping came from a component template named `my-mappings-template`. We want to test adding a new field, `bar`. So a substitute definition of `my-mappings-template` is provided in `component_template_substitutions`. The substitute `my-mappings-template` will be used in place of the existing mapping for `my-index` and in place of the `my-mappings-template` that is in the system.\\n\\n```\\nresp = client.perform_request(\\n    \\"POST\\",\\n    \\"/_ingest/_simulate\\",\\n    headers={\\"Content-Type\\": \\"application/json\\"},\\n    body={\\n        \\"docs\\": [\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"123\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"foo\\"\\n                }\\n            },\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"456\\",\\n                \\"_source\\": {\\n                    \\"bar\\": \\"rab\\"\\n                }\\n            }\\n        ],\\n        \\"component_template_substitutions\\": {\\n            \\"my-mappings_template\\": {\\n                \\"template\\": {\\n                    \\"mappings\\": {\\n                        \\"dynamic\\": \\"strict\\",\\n                        \\"properties\\": {\\n                            \\"foo\\": {\\n                                \\"type\\": \\"keyword\\"\\n                            },\\n                            \\"bar\\": {\\n                                \\"type\\": \\"keyword\\"\\n                            }\\n                        }\\n                    }\\n                }\\n            }\\n        }\\n    },\\n)\\nprint(resp)\\n```\\n\\n```\\nconst response = await client.simulate.ingest({\\n  body: {\\n    docs: [\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"123\\",\\n        _source: {\\n          foo: \\"foo\\",\\n        },\\n      },\\n      {\\n        _index: \\"my-index\\",\\n        _id: \\"456\\",\\n        _source: {\\n          bar: \\"rab\\",\\n        },\\n      },\\n    ],\\n    component_template_substitutions: {\\n      \\"my-mappings_template\\": {\\n        template: {\\n          mappings: {\\n            dynamic: \\"strict\\",\\n            properties: {\\n              foo: {\\n                type: \\"keyword\\",\\n              },\\n              bar: {\\n                type: \\"keyword\\",\\n              },\\n            },\\n          },\\n        },\\n      },\\n    },\\n  },\\n});\\nconsole.log(response);\\n```\\n\\n```\\nPOST /_ingest/_simulate\\n{\\n  \\"docs\\": [\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"123\\",\\n      \\"_source\\": {\\n        \\"foo\\": \\"foo\\"\\n      }\\n    },\\n    {\\n      \\"_index\\": \\"my-index\\",\\n      \\"_id\\": \\"456\\",\\n      \\"_source\\": {\\n        \\"bar\\": \\"rab\\"\\n      }\\n    }\\n  ],\\n  \\"component_template_substitutions\\": {\\n    \\"my-mappings_template\\": {\\n      \\"template\\": {\\n        \\"mappings\\": {\\n          \\"dynamic\\": \\"strict\\",\\n          \\"properties\\": {\\n            \\"foo\\": {\\n              \\"type\\": \\"keyword\\"\\n            },\\n            \\"bar\\": {\\n              \\"type\\": \\"keyword\\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThe API returns the following response:\\n\\n```\\n{\\n   \\"docs\\": [\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"123\\",\\n            \\"_index\\": \\"my-index\\",\\n            \\"_version\\": -3,\\n            \\"_source\\": {\\n               \\"foo\\": \\"foo\\"\\n            },\\n            \\"executed_pipelines\\": []\\n         }\\n      },\\n      {\\n         \\"doc\\": {\\n            \\"_id\\": \\"456\\",\\n            \\"_index\\": \\"my-index\\",\\n            \\"_version\\": -3,\\n            \\"_source\\": {\\n               \\"bar\\": \\"rab\\"\\n            },\\n            \\"executed_pipelines\\": []\\n         }\\n      }\\n   ]\\n}\\n```\\n","title":"Simulate ingest API","url":"https://www.elastic.co/guide/en/elasticsearch/reference/8.17/simulate-ingest-api.html","productName":"elasticsearch","score":125.13092}',
        truncated: {
          truncatedText:
            '{"content":"## Simulate ingest API\\n\\nExecutes ingest pipelines against a set of provided documents, optionally with substitute pipeline definitions. This API is meant to be used for troubleshooting or pipeline development, as it does not actually index any data into Elasticsearch.\\n\\n```\\nresp = client.perform_request(\\n    \\"POST\\",\\n    \\"/_ingest/_simulate\\",\\n    headers={\\"Content-Type\\": \\"application/json\\"},\\n    body={\\n        \\"docs\\": [\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"id\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"bar\\"\\n                }\\n            },\\n            {\\n                \\"_index\\": \\"my-index\\",\\n                \\"_id\\": \\"id\\",\\n                \\"_source\\": {\\n                    \\"foo\\": \\"rab\\"\\n                }\\n            }\\n        ],\\n        \\"pipeline_substitutions\\": {\\n            \\"my-pipeline\\": {\\n                \\"processors\\": [\\n                    {\\n                        \\"set\\": {\\n                            \\"field\\": \\"field3\\",\\n                            \\"value\\": \\"value3\\"\\n                        }\\n                    }\\n                ]\\n            }\\n        },\\n        \\"component_template_substitutions\\": {\\n            \\"my-component-template\\": {\\n                \\"template\\": {\\n                    \\"mappings\\": {\\n                        \\"dynamic\\": \\"true\\",\\n                        \\"properties\\": {\\n                            \\"field3\\": {\\n                                \\"type\\": \\"keyword\\"\\n                            }\\n                        }\\n                    },\\n                    \\"settings\\": {\\n                        \\"index\\": {\\n                            \\"default_pipeline\\": \\"my-pipeline\\"\\n                        }\\n                    }\\n                }\\n            }\\n        },\\n        \\"index_template_substitutions\\": {\\n            \\"my-index-template\\": {\\n                \\"index_patterns\\": [\\n                    \\"my-index-*\\"\\n               ... <truncated>',
          originalTokenCount: 7668,
          truncatedTokenCount: 400,
        },
        llmScore: 4,
      },
    ],
  },
  content: {
    screen_description:
      'The user is looking at http://localhost:5601/kbn/app/observabilityAIAssistant/conversations/5deab507-a44a-4554-ace1-f9fa7c388a77. The current time range is 2025-03-10T07:31:09.916Z - 2025-03-10T07:46:09.916Z.',
    entries: [
      {
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest.html',
        title: 'Ingest pipelines',
        llmScore: 7,
        relevanceScore: 157.81056,
        document: {
          content:
            '## Ingest pipelines\n\nIngest pipelines let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.\n\nA pipeline consists of a series of configurable tasks called [processors](processors.html "Ingest processor reference"). Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, Elasticsearch adds the transformed documents to your data stream or index.\n\n![Ingest pipeline diagram](images/ingest/ingest-process.svg)\n\nYou can create and manage ingest pipelines using Kibana’s **Ingest Pipelines** feature or the [ingest APIs](ingest-apis.html "Ingest APIs"). Elasticsearch stores pipelines in the [cluster state](cluster-state.html "Cluster state API").\n\n### Prerequisites\n\n* Nodes with the [`ingest`](modules-node.html#node-ingest-node "Ingest node") node role handle pipeline processing. To use ingest pipelines, your cluster must have at least one node with the `ingest` role. For heavy ingest loads, we recommend creating [dedicated ingest nodes](modules-node.html#node-ingest-node "Ingest node").\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to manage ingest pipelines. To use Kibana’s **Ingest Pipelines** feature, you also need the `cluster:monitor/nodes/info` cluster privileges.\n* Pipelines including the `enrich` processor require additional setup. See [*Enrich your data*](ingest-enriching-data.html "Enrich your data").\n\n### Create and manage pipelines\n\nIn Kibana, open the main menu and click **Stack Management > Ingest Pipelines**. From the list view, you can:\n\n* View a list of your pipelines and drill down into details\n* Edit or clone existing pipelines\n* Delete pipelines\n\n![Kibana’s Ingest Pipelines list view](images/ingest/ingest-pipeline-list.png)\n\nTo create a pipeline, click **Create pipeline > New pipeline**. For an example tutorial, see [*Example: Parse logs*](common-log-format-example.html "Example: Parse logs in the Common Log Format").\n\nThe **New pipeline from CSV** option lets you use a CSV to create an ingest pipeline that maps custom data to the [Elastic Common Schema (ECS)](/guide/en/ecs/8.16). Mapping your custom data to ECS makes the data easier to search and lets you reuse visualizations from other datasets. To get started, check [Map custom data to ECS](/guide/en/ecs/8.16/ecs-converting.html).\n\nYou can also use the [ingest APIs](ingest-apis.html "Ingest APIs") to create and manage pipelines. The following [create pipeline API](put-pipeline-api.html "Create or update pipeline API") request creates a pipeline containing two [`set`](set-processor.html "Set processor") processors followed by a [`lowercase`](lowercase-processor.html "Lowercase processor") processor. The processors run sequentially in the order specified.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    description="My optional pipeline description",\n    processors=[\n        {\n            "set": {\n                "description": "My optional processor description",\n                "field": "my-long-field",\n                "value": 10\n            }\n        },\n        {\n            "set": {\n                "description": "Set \'my-boolean-field\' to true",\n                "field": "my-boolean-field",\n                "value": True\n            }\n        },\n        {\n            "lowercase": {\n                "field": "my-keyword-field"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    description: \'My optional pipeline description\',\n    processors: [\n      {\n        set: {\n          description: \'My optional processor description\',\n          field: \'my-long-field\',\n          value: 10\n        }\n      },\n      {\n        set: {\n          description: "Set \'my-boolean-field\' to true",\n          field: \'my-boolean-field\',\n          value: true\n        }\n      },\n      {\n        lowercase: {\n          field: \'my-keyword-field\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  description: "My optional pipeline description",\n  processors: [\n    {\n      set: {\n        description: "My optional processor description",\n        field: "my-long-field",\n        value: 10,\n      },\n    },\n    {\n      set: {\n        description: "Set \'my-boolean-field\' to true",\n        field: "my-boolean-field",\n        value: true,\n      },\n    },\n    {\n      lowercase: {\n        field: "my-keyword-field",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "description": "My optional pipeline description",\n  "processors": [\n    {\n      "set": {\n        "description": "My optional processor description",\n        "field": "my-long-field",\n        "value": 10\n      }\n    },\n    {\n      "set": {\n        "description": "Set \'my-boolean-field\' to true",\n        "field": "my-boolean-field",\n        "value": true\n      }\n    },\n    {\n      "lowercase": {\n        "field": "my-keyword-field"\n      }\n    }\n  ]\n}\n```\n\n### Manage pipeline versions\n\nWhen you create or update a pipeline, you can specify an optional `version` integer. You can use this version number with the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params "Query parameters") parameter to conditionally update the pipeline. When the `if_version` parameter is specified, a successful update increments the pipeline’s version.\n\n```\nPUT _ingest/pipeline/my-pipeline-id\n{\n  "version": 1,\n  "processors": [ ... ]\n}\n```\n\nTo unset the `version` number using the API, replace or update the pipeline without specifying the `version` parameter.\n\n### Test a pipeline\n\nBefore using a pipeline in production, we recommend you test it using sample documents. When creating or editing a pipeline in Kibana, click **Add documents**. In the **Documents** tab, provide sample documents and click **Run the pipeline**.\n\n![Test a pipeline in Kibana](images/ingest/test-a-pipeline.png)\n\nYou can also test pipelines using the [simulate pipeline API](simulate-pipeline-api.html "Simulate pipeline API"). You can specify a configured pipeline in the request path. For example, the following request tests `my-pipeline`.\n\n```\nresp = client.ingest.simulate(\n    id="my-pipeline",\n    docs=[\n        {\n            "_source": {\n                "my-keyword-field": "FOO"\n            }\n        },\n        {\n            "_source": {\n                "my-keyword-field": "BAR"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.simulate(\n  id: \'my-pipeline\',\n  body: {\n    docs: [\n      {\n        _source: {\n          "my-keyword-field": \'FOO\'\n        }\n      },\n      {\n        _source: {\n          "my-keyword-field": \'BAR\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.simulate({\n  id: "my-pipeline",\n  docs: [\n    {\n      _source: {\n        "my-keyword-field": "FOO",\n      },\n    },\n    {\n      _source: {\n        "my-keyword-field": "BAR",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST _ingest/pipeline/my-pipeline/_simulate\n{\n  "docs": [\n    {\n      "_source": {\n        "my-keyword-field": "FOO"\n      }\n    },\n    {\n      "_source": {\n        "my-keyword-field": "BAR"\n      }\n    }\n  ]\n}\n```\n\nAlternatively, you can specify a pipeline and its processors in the request body.\n\n```\nresp = client.ingest.simulate(\n    pipeline={\n        "processors": [\n            {\n                "lowercase": {\n                    "field": "my-keyword-field"\n                }\n            }\n        ]\n    },\n    docs=[\n        {\n            "_source": {\n                "my-keyword-field": "FOO"\n            }\n        },\n        {\n            "_source": {\n                "my-keyword-field": "BAR"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.simulate(\n  body: {\n    pipeline: {\n      processors: [\n        {\n          lowercase: {\n            field: \'my-keyword-field\'\n          }\n        }\n      ]\n    },\n    docs: [\n      {\n        _source: {\n          "my-keyword-field": \'FOO\'\n        }\n      },\n      {\n        _source: {\n          "my-keyword-field": \'BAR\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.simulate({\n  pipeline: {\n    processors: [\n      {\n        lowercase: {\n          field: "my-keyword-field",\n        },\n      },\n    ],\n  },\n  docs: [\n    {\n      _source: {\n        "my-keyword-field": "FOO",\n      },\n    },\n    {\n      _source: {\n        "my-keyword-field": "BAR",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPOST _ingest/pipeline/_simulate\n{\n  "pipeline": {\n    "processors": [\n      {\n        "lowercase": {\n          "field": "my-keyword-field"\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_source": {\n        "my-keyword-field": "FOO"\n      }\n    },\n    {\n      "_source": {\n        "my-keyword-field": "BAR"\n      }\n    }\n  ]\n}\n```\n\nThe API returns transformed documents:\n\n```\n{\n  "docs": [\n    {\n      "doc": {\n        "_index": "_index",\n        "_id": "_id",\n        "_version": "-3",\n        "_source": {\n          "my-keyword-field": "foo"\n        },\n        "_ingest": {\n          "timestamp": "2099-03-07T11:04:03.000Z"\n        }\n      }\n    },\n    {\n      "doc": {\n        "_index": "_index",\n        "_id": "_id",\n        "_version": "-3",\n        "_source": {\n          "my-keyword-field": "bar"\n        },\n        "_ingest": {\n          "timestamp": "2099-03-07T11:04:04.000Z"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Add a pipeline to an indexing request\n\nUse the `pipeline` query parameter to apply a pipeline to documents in [individual](docs-index_.html "Index API") or [bulk](docs-bulk.html "Bulk API") indexing requests.\n\n```\nresp = client.index(\n    index="my-data-stream",\n    pipeline="my-pipeline",\n    document={\n        "@timestamp": "2099-03-07T11:04:05.000Z",\n        "my-keyword-field": "foo"\n    },\n)\nprint(resp)\n\nresp1 = client.bulk(\n    index="my-data-stream",\n    pipeline="my-pipeline",\n    operations=[\n        {\n            "create": {}\n        },\n        {\n            "@timestamp": "2099-03-07T11:04:06.000Z",\n            "my-keyword-field": "foo"\n        },\n        {\n            "create": {}\n        },\n        {\n            "@timestamp": "2099-03-07T11:04:07.000Z",\n            "my-keyword-field": "bar"\n        }\n    ],\n)\nprint(resp1)\n```\n\n```\nresponse = client.index(\n  index: \'my-data-stream\',\n  pipeline: \'my-pipeline\',\n  body: {\n    "@timestamp": \'2099-03-07T11:04:05.000Z\',\n    "my-keyword-field": \'foo\'\n  }\n)\nputs response\n\nresponse = client.bulk(\n  index: \'my-data-stream\',\n  pipeline: \'my-pipeline\',\n  body: [\n    {\n      create: {}\n    },\n    {\n      "@timestamp": \'2099-03-07T11:04:06.000Z\',\n      "my-keyword-field": \'foo\'\n    },\n    {\n      create: {}\n    },\n    {\n      "@timestamp": \'2099-03-07T11:04:07.000Z\',\n      "my-keyword-field": \'bar\'\n    }\n  ]\n)\nputs response\n```\n\n```\nconst response = await client.index({\n  index: "my-data-stream",\n  pipeline: "my-pipeline",\n  document: {\n    "@timestamp": "2099-03-07T11:04:05.000Z",\n    "my-keyword-field": "foo",\n  },\n});\nconsole.log(response);\n\nconst response1 = await client.bulk({\n  index: "my-data-stream",\n  pipeline: "my-pipeline",\n  operations: [\n    {\n      create: {},\n    },\n    {\n      "@timestamp": "2099-03-07T11:04:06.000Z",\n      "my-keyword-field": "foo",\n    },\n    {\n      create: {},\n    },\n    {\n      "@timestamp": "2099-03-07T11:04:07.000Z",\n      "my-keyword-field": "bar",\n    },\n  ],\n});\nconsole.log(response1);\n```\n\n```\nPOST my-data-stream/_doc?pipeline=my-pipeline\n{\n  "@timestamp": "2099-03-07T11:04:05.000Z",\n  "my-keyword-field": "foo"\n}\n\nPUT my-data-stream/_bulk?pipeline=my-pipeline\n{ "create":{ } }\n{ "@timestamp": "2099-03-07T11:04:06.000Z", "my-keyword-field": "foo" }\n{ "create":{ } }\n{ "@timestamp": "2099-03-07T11:04:07.000Z", "my-keyword-field": "bar" }\n```\n\nYou can also use the `pipeline` parameter with the [update by query](docs-update-by-query.html "Update By Query API") or [reindex](docs-reindex.html "Reindex API") APIs.\n\n```\nresp = client.update_by_query(\n    index="my-data-stream",\n    pipeline="my-pipeline",\n)\nprint(resp)\n\nresp1 = client.reindex(\n    source={\n        "index": "my-data-stream"\n    },\n    dest={\n        "index": "my-new-data-stream",\n        "op_type": "create",\n        "pipeline": "my-pipeline"\n    },\n)\nprint(resp1)\n```\n\n```\nresponse = client.update_by_query(\n  index: \'my-data-stream\',\n  pipeline: \'my-pipeline\'\n)\nputs response\n\nresponse = client.reindex(\n  body: {\n    source: {\n      index: \'my-data-stream\'\n    },\n    dest: {\n      index: \'my-new-data-stream\',\n      op_type: \'create\',\n      pipeline: \'my-pipeline\'\n    }\n  }\n)\nputs response\n```\n\n```\nconst response = await client.updateByQuery({\n  index: "my-data-stream",\n  pipeline: "my-pipeline",\n});\nconsole.log(response);\n\nconst response1 = await client.reindex({\n  source: {\n    index: "my-data-stream",\n  },\n  dest: {\n    index: "my-new-data-stream",\n    op_type: "create",\n    pipeline: "my-pipeline",\n  },\n});\nconsole.log(response1);\n```\n\n```\nPOST my-data-stream/_update_by_query?pipeline=my-pipeline\n\nPOST _reindex\n{\n  "source": {\n    "index": "my-data-stream"\n  },\n  "dest": {\n    "index": "my-new-data-stream",\n    "op_type": "create",\n    "pipeline": "my-pipeline"\n  }\n}\n```\n\n### Set a default pipeline\n\nUse the [`index.default_pipeline`](index-modules.html#index-default-pipeline) index setting to set a default pipeline. Elasticsearch applies this pipeline to indexing requests if no `pipeline` parameter is specified.\n\n### Set a final pipeline\n\nUse the [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting to set a final pipeline. Elasticsearch applies this pipeline after the request or default pipeline, even if neither is specified.\n\n### Pipelines for Beats\n\nTo add an ingest pipeline to an Elastic Beat, specify the `pipeline` parameter under `output.elasticsearch` in `<BEAT_NAME>.yml`. For example, for Filebeat, you’d specify `pipeline` in `filebeat.yml`.\n\n```\noutput.elasticsearch:\n  hosts: ["localhost:9200"]\n  pipeline: my-pipeline\n```\n\n### Pipelines for Fleet and Elastic Agent\n\nElastic Agent integrations ship with default ingest pipelines that preprocess and enrich data before indexing. [Fleet](/guide/en/fleet/8.17/index.html) applies these pipelines using [index templates](index-templates.html "Index templates") that include [pipeline index settings](ingest.html#set-default-pipeline "Set a default pipeline"). Elasticsearch matches these templates to your Fleet data streams based on the [stream’s naming scheme](/guide/en/fleet/8.17/data-streams.html#data-streams-naming-scheme).\n\nEach default integration pipeline calls a nonexistent, unversioned `*@custom` ingest pipeline. If unaltered, this pipeline call has no effect on your data. However, you can modify this call to create custom pipelines for integrations that persist across upgrades. Refer to [Tutorial: Transform data with custom ingest pipelines](/guide/en/fleet/8.17/data-streams-pipeline-tutorial.html) to learn more.\n\nFleet doesn’t provide a default ingest pipeline for the **Custom logs** integration, but you can specify a pipeline for this integration using an [index template](ingest.html#pipeline-custom-logs-index-template) or a [custom configuration](ingest.html#pipeline-custom-logs-configuration).\n\n**Option 1: Index template**\n\n1. [Create](ingest.html#create-manage-ingest-pipelines "Create and manage pipelines") and [test](ingest.html#test-pipeline "Test a pipeline") your ingest pipeline. Name your pipeline `logs-<dataset-name>-default`. This makes tracking the pipeline for your integration easier.\n\n   For example, the following request creates a pipeline for the `my-app` dataset. The pipeline’s name is `logs-my_app-default`.\n\n   \n\n   ```\n   PUT _ingest/pipeline/logs-my_app-default\n   {\n     "description": "Pipeline for `my_app` dataset",\n     "processors": [ ... ]\n   }\n   ```\n\n2. Create an [index template](index-templates.html "Index templates") that includes your pipeline in the [`index.default_pipeline`](index-modules.html#index-default-pipeline) or [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting. Ensure the template is [data stream enabled](set-up-a-data-stream.html#create-index-template "Create an index template"). The template’s index pattern should match `logs-<dataset-name>-*`.\n\n   You can create this template using Kibana’s [**Index Management**](index-mgmt.html#manage-index-templates "Manage index templates") feature or the [create index template API](indices-put-template.html "Create or update index template API").\n\n   For example, the following request creates a template matching `logs-my_app-*`. The template uses a component template that contains the `index.default_pipeline` index setting.\n\n   ```\n   resp = client.cluster.put_component_template(\n       name="logs-my_app-settings",\n       template={\n           "settings": {\n               "index.default_pipeline": "logs-my_app-default",\n               "index.lifecycle.name": "logs"\n           }\n       },\n   )\n   print(resp)\n\n   resp1 = client.indices.put_index_template(\n       name="logs-my_app-template",\n       index_patterns=[\n           "logs-my_app-*"\n       ],\n       data_stream={},\n       priority=500,\n       composed_of=[\n           "logs-my_app-settings",\n           "logs-my_app-mappings"\n       ],\n   )\n   print(resp1)\n   ```\n\n   ```\n   const response = await client.cluster.putComponentTemplate({\n     name: "logs-my_app-settings",\n     template: {\n       settings: {\n         "index.default_pipeline": "logs-my_app-default",\n         "index.lifecycle.name": "logs",\n       },\n     },\n   });\n   console.log(response);\n\n   const response1 = await client.indices.putIndexTemplate({\n     name: "logs-my_app-template",\n     index_patterns: ["logs-my_app-*"],\n     data_stream: {},\n     priority: 500,\n     composed_of: ["logs-my_app-settings", "logs-my_app-mappings"],\n   });\n   console.log(response1);\n   ```\n\n   \n\n   ```\n   # Creates a component template for index settings\n   PUT _component_template/logs-my_app-settings\n   {\n     "template": {\n       "settings": {\n         "index.default_pipeline": "logs-my_app-default",\n         "index.lifecycle.name": "logs"\n       }\n     }\n   }\n\n   # Creates an index template matching `logs-my_app-*`\n   PUT _index_template/logs-my_app-template\n   {\n     "index_patterns": ["logs-my_app-*"],\n     "data_stream": { },\n     "priority": 500,\n     "composed_of": ["logs-my_app-settings", "logs-my_app-mappings"]\n   }\n   ```\n\n3. When adding or editing your **Custom logs** integration in Fleet, click **Configure integration > Custom log file > Advanced options**.\n\n4. In **Dataset name**, specify your dataset’s name. Fleet will add new data for the integration to the resulting `logs-<dataset-name>-default` data stream.\n\n   For example, if your dataset’s name was `my_app`, Fleet adds new data to the `logs-my_app-default` data stream.\n\n   ![Set up custom log integration in Fleet](images/ingest/custom-logs.png)\n\n5. Use the [rollover API](indices-rollover-index.html "Rollover API") to roll over your data stream. This ensures Elasticsearch applies the index template and its pipeline settings to any new data for the integration.\n\n   ```\n   resp = client.indices.rollover(\n       alias="logs-my_app-default",\n   )\n   print(resp)\n   ```\n\n   ```\n   response = client.indices.rollover(\n     alias: \'logs-my_app-default\'\n   )\n   puts response\n   ```\n\n   ```\n   const response = await client.indices.rollover({\n     alias: "logs-my_app-default",\n   });\n   console.log(response);\n   ```\n\n   \n\n   ```\n   POST logs-my_app-default/_rollover/\n   ```\n\n**Option 2: Custom configuration**\n\n1. [Create](ingest.html#create-manage-ingest-pipelines "Create and manage pipelines") and [test](ingest.html#test-pipeline "Test a pipeline") your ingest pipeline. Name your pipeline `logs-<dataset-name>-default`. This makes tracking the pipeline for your integration easier.\n\n   For example, the following request creates a pipeline for the `my-app` dataset. The pipeline’s name is `logs-my_app-default`.\n\n   \n\n   ```\n   PUT _ingest/pipeline/logs-my_app-default\n   {\n     "description": "Pipeline for `my_app` dataset",\n     "processors": [ ... ]\n   }\n   ```\n\n2. When adding or editing your **Custom logs** integration in Fleet, click **Configure integration > Custom log file > Advanced options**.\n\n3. In **Dataset name**, specify your dataset’s name. Fleet will add new data for the integration to the resulting `logs-<dataset-name>-default` data stream.\n\n   For example, if your dataset’s name was `my_app`, Fleet adds new data to the `logs-my_app-default` data stream.\n\n4. In **Custom Configurations**, specify your pipeline in the `pipeline` policy setting.\n\n   ![Custom pipeline configuration for custom log integration](images/ingest/custom-logs-pipeline.png)\n\n**Elastic Agent standalone**\n\nIf you run Elastic Agent standalone, you can apply pipelines using an [index template](index-templates.html "Index templates") that includes the [`index.default_pipeline`](index-modules.html#index-default-pipeline) or [`index.final_pipeline`](index-modules.html#index-final-pipeline) index setting. Alternatively, you can specify the `pipeline` policy setting in your `elastic-agent.yml` configuration. See [Install standalone Elastic Agents](/guide/en/fleet/8.17/install-standalone-elastic-agent.html).\n\n### Pipelines for search indices\n\nWhen you create Elasticsearch indices for search use cases, for example, using the [web crawler](/guide/en/enterprise-search/8.17/crawler.html) or [connectors](es-connectors.html "Ingest content with Elastic connectors"), these indices are automatically set up with specific ingest pipelines. These processors help optimize your content for search. See [*Ingest pipelines in Search*](ingest-pipeline-search.html "Ingest pipelines in Search") for more information.\n\n### Access source fields in a processor\n\nProcessors have read and write access to an incoming document’s source fields. To access a field key in a processor, use its field name. The following `set` processor accesses `my-long-field`.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "field": "my-long-field",\n                "value": 10\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          field: \'my-long-field\',\n          value: 10\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        field: "my-long-field",\n        value: 10,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "field": "my-long-field",\n        "value": 10\n      }\n    }\n  ]\n}\n```\n\nYou can also prepend the `_source` prefix.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "field": "_source.my-long-field",\n                "value": 10\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          field: \'_source.my-long-field\',\n          value: 10\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        field: "_source.my-long-field",\n        value: 10,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "field": "_source.my-long-field",\n        "value": 10\n      }\n    }\n  ]\n}\n```\n\nUse dot notation to access object fields.\n\nIf your document contains flattened objects, use the [`dot_expander`](dot-expand-processor.html "Dot expander processor") processor to expand them first. Other ingest processors cannot access flattened objects.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "dot_expander": {\n                "description": "Expand \'my-object-field.my-property\'",\n                "field": "my-object-field.my-property"\n            }\n        },\n        {\n            "set": {\n                "description": "Set \'my-object-field.my-property\' to 10",\n                "field": "my-object-field.my-property",\n                "value": 10\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        dot_expander: {\n          description: "Expand \'my-object-field.my-property\'",\n          field: \'my-object-field.my-property\'\n        }\n      },\n      {\n        set: {\n          description: "Set \'my-object-field.my-property\' to 10",\n          field: \'my-object-field.my-property\',\n          value: 10\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      dot_expander: {\n        description: "Expand \'my-object-field.my-property\'",\n        field: "my-object-field.my-property",\n      },\n    },\n    {\n      set: {\n        description: "Set \'my-object-field.my-property\' to 10",\n        field: "my-object-field.my-property",\n        value: 10,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "dot_expander": {\n        "description": "Expand \'my-object-field.my-property\'",\n        "field": "my-object-field.my-property"\n      }\n    },\n    {\n      "set": {\n        "description": "Set \'my-object-field.my-property\' to 10",\n        "field": "my-object-field.my-property",\n        "value": 10\n      }\n    }\n  ]\n}\n```\n\nSeveral processor parameters support [Mustache](https://mustache.github.io) template snippets. To access field values in a template snippet, enclose the field name in triple curly brackets:`{{{field-name}}}`. You can use template snippets to dynamically set field names.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Set dynamic \'<service>\' field to \'code\' value",\n                "field": "{{{service}}}",\n                "value": "{{{code}}}"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "Set dynamic \'<service>\' field to \'code\' value",\n          field: \'{{{service}}}\',\n          value: \'{{{code}}}\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Set dynamic \'<service>\' field to \'code\' value",\n        field: "{{{service}}}",\n        value: "{{{code}}}",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Set dynamic \'<service>\' field to \'code\' value",\n        "field": "{{{service}}}",\n        "value": "{{{code}}}"\n      }\n    }\n  ]\n}\n```\n\n### Access metadata fields in a processor\n\nProcessors can access the following metadata fields by name:\n\n* `_index`\n* `_id`\n* `_routing`\n* `_dynamic_templates`\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Set \'_routing\' to \'geoip.country_iso_code\' value",\n                "field": "_routing",\n                "value": "{{{geoip.country_iso_code}}}"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "Set \'_routing\' to \'geoip.country_iso_code\' value",\n          field: \'_routing\',\n          value: \'{{{geoip.country_iso_code}}}\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Set \'_routing\' to \'geoip.country_iso_code\' value",\n        field: "_routing",\n        value: "{{{geoip.country_iso_code}}}",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Set \'_routing\' to \'geoip.country_iso_code\' value",\n        "field": "_routing",\n        "value": "{{{geoip.country_iso_code}}}"\n      }\n    }\n  ]\n}\n```\n\nUse a Mustache template snippet to access metadata field values. For example, `{{{_routing}}}` retrieves a document’s routing value.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Use geo_point dynamic template for address field",\n                "field": "_dynamic_templates",\n                "value": {\n                    "address": "geo_point"\n                }\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: \'Use geo_point dynamic template for address field\',\n          field: \'_dynamic_templates\',\n          value: {\n            address: \'geo_point\'\n          }\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Use geo_point dynamic template for address field",\n        field: "_dynamic_templates",\n        value: {\n          address: "geo_point",\n        },\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Use geo_point dynamic template for address field",\n        "field": "_dynamic_templates",\n        "value": {\n          "address": "geo_point"\n        }\n      }\n    }\n  ]\n}\n```\n\nThe set processor above tells ES to use the dynamic template named `geo_point` for the field `address` if this field is not defined in the mapping of the index yet. This processor overrides the dynamic template for the field `address` if already defined in the bulk request, but has no effect on other dynamic templates defined in the bulk request.\n\nIf you [automatically generate](docs-index_.html#create-document-ids-automatically "Create document IDs automatically") document IDs, you cannot use `{{{_id}}}` in a processor. Elasticsearch assigns auto-generated `_id` values after ingest.\n\n### Access ingest metadata in a processor\n\nIngest processors can add and access ingest metadata using the `_ingest` key.\n\nUnlike source and metadata fields, Elasticsearch does not index ingest metadata fields by default. Elasticsearch also allows source fields that start with an `_ingest` key. If your data includes such source fields, use `_source._ingest` to access them.\n\nPipelines only create the `_ingest.timestamp` ingest metadata field by default. This field contains a timestamp of when Elasticsearch received the document’s indexing request. To index `_ingest.timestamp` or other ingest metadata fields, use the `set` processor.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "Index the ingest timestamp as \'event.ingested\'",\n                "field": "event.ingested",\n                "value": "{{{_ingest.timestamp}}}"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "Index the ingest timestamp as \'event.ingested\'",\n          field: \'event.ingested\',\n          value: \'{{{_ingest.timestamp}}}\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "Index the ingest timestamp as \'event.ingested\'",\n        field: "event.ingested",\n        value: "{{{_ingest.timestamp}}}",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "Index the ingest timestamp as \'event.ingested\'",\n        "field": "event.ingested",\n        "value": "{{{_ingest.timestamp}}}"\n      }\n    }\n  ]\n}\n```\n\n### Handling pipeline failures\n\nA pipeline’s processors run sequentially. By default, pipeline processing stops when one of these processors fails or encounters an error.\n\nTo ignore a processor failure and run the pipeline’s remaining processors, set `ignore_failure` to `true`.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "rename": {\n                "description": "Rename \'provider\' to \'cloud.provider\'",\n                "field": "provider",\n                "target_field": "cloud.provider",\n                "ignore_failure": True\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        rename: {\n          description: "Rename \'provider\' to \'cloud.provider\'",\n          field: \'provider\',\n          target_field: \'cloud.provider\',\n          ignore_failure: true\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      rename: {\n        description: "Rename \'provider\' to \'cloud.provider\'",\n        field: "provider",\n        target_field: "cloud.provider",\n        ignore_failure: true,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "rename": {\n        "description": "Rename \'provider\' to \'cloud.provider\'",\n        "field": "provider",\n        "target_field": "cloud.provider",\n        "ignore_failure": true\n      }\n    }\n  ]\n}\n```\n\nUse the `on_failure` parameter to specify a list of processors to run immediately after a processor failure. If `on_failure` is specified, Elasticsearch afterward runs the pipeline’s remaining processors, even if the `on_failure` configuration is empty.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "rename": {\n                "description": "Rename \'provider\' to \'cloud.provider\'",\n                "field": "provider",\n                "target_field": "cloud.provider",\n                "on_failure": [\n                    {\n                        "set": {\n                            "description": "Set \'error.message\'",\n                            "field": "error.message",\n                            "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                            "override": False\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        rename: {\n          description: "Rename \'provider\' to \'cloud.provider\'",\n          field: \'provider\',\n          target_field: \'cloud.provider\',\n          on_failure: [\n            {\n              set: {\n                description: "Set \'error.message\'",\n                field: \'error.message\',\n                value: "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                override: false\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      rename: {\n        description: "Rename \'provider\' to \'cloud.provider\'",\n        field: "provider",\n        target_field: "cloud.provider",\n        on_failure: [\n          {\n            set: {\n              description: "Set \'error.message\'",\n              field: "error.message",\n              value:\n                "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              override: false,\n            },\n          },\n        ],\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "rename": {\n        "description": "Rename \'provider\' to \'cloud.provider\'",\n        "field": "provider",\n        "target_field": "cloud.provider",\n        "on_failure": [\n          {\n            "set": {\n              "description": "Set \'error.message\'",\n              "field": "error.message",\n              "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              "override": false\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\nNest a list of `on_failure` processors for nested error handling.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "rename": {\n                "description": "Rename \'provider\' to \'cloud.provider\'",\n                "field": "provider",\n                "target_field": "cloud.provider",\n                "on_failure": [\n                    {\n                        "set": {\n                            "description": "Set \'error.message\'",\n                            "field": "error.message",\n                            "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                            "override": False,\n                            "on_failure": [\n                                {\n                                    "set": {\n                                        "description": "Set \'error.message.multi\'",\n                                        "field": "error.message.multi",\n                                        "value": "Document encountered multiple ingest errors",\n                                        "override": True\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        rename: {\n          description: "Rename \'provider\' to \'cloud.provider\'",\n          field: \'provider\',\n          target_field: \'cloud.provider\',\n          on_failure: [\n            {\n              set: {\n                description: "Set \'error.message\'",\n                field: \'error.message\',\n                value: "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n                override: false,\n                on_failure: [\n                  {\n                    set: {\n                      description: "Set \'error.message.multi\'",\n                      field: \'error.message.multi\',\n                      value: \'Document encountered multiple ingest errors\',\n                      override: true\n                    }\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      rename: {\n        description: "Rename \'provider\' to \'cloud.provider\'",\n        field: "provider",\n        target_field: "cloud.provider",\n        on_failure: [\n          {\n            set: {\n              description: "Set \'error.message\'",\n              field: "error.message",\n              value:\n                "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              override: false,\n              on_failure: [\n                {\n                  set: {\n                    description: "Set \'error.message.multi\'",\n                    field: "error.message.multi",\n                    value: "Document encountered multiple ingest errors",\n                    override: true,\n                  },\n                },\n              ],\n            },\n          },\n        ],\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "rename": {\n        "description": "Rename \'provider\' to \'cloud.provider\'",\n        "field": "provider",\n        "target_field": "cloud.provider",\n        "on_failure": [\n          {\n            "set": {\n              "description": "Set \'error.message\'",\n              "field": "error.message",\n              "value": "Field \'provider\' does not exist. Cannot rename to \'cloud.provider\'",\n              "override": false,\n              "on_failure": [\n                {\n                  "set": {\n                    "description": "Set \'error.message.multi\'",\n                    "field": "error.message.multi",\n                    "value": "Document encountered multiple ingest errors",\n                    "override": true\n                  }\n                }\n              ]\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\nYou can also specify `on_failure` for a pipeline. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. Elasticsearch will not attempt to run the pipeline’s remaining processors.\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [ ... ],\n  "on_failure": [\n    {\n      "set": {\n        "description": "Index document to \'failed-<index>\'",\n        "field": "_index",\n        "value": "failed-{{{ _index }}}"\n      }\n    }\n  ]\n}\n```\n\nAdditional information about the pipeline failure may be available in the document metadata fields `on_failure_message`, `on_failure_processor_type`, `on_failure_processor_tag`, and `on_failure_pipeline`. These fields are accessible only from within an `on_failure` block.\n\nThe following example uses the metadata fields to include information about pipeline failures in documents.\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [ ... ],\n  "on_failure": [\n    {\n      "set": {\n        "description": "Record error information",\n        "field": "error_information",\n        "value": "Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}"\n      }\n    }\n  ]\n}\n```\n\n### Conditionally run a processor\n\nEach processor supports an optional `if` condition, written as a [Painless script](/guide/en/elasticsearch/painless/8.17/painless-guide.html). If provided, the processor only runs when the `if` condition is `true`.\n\n`if` condition scripts run in Painless’s [ingest processor context](/guide/en/elasticsearch/painless/8.17/painless-ingest-processor-context.html). In `if` conditions, `ctx` values are read-only.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents with \'network.name\' of \'Guest\'",\n                "if": "ctx?.network?.name == \'Guest\'"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        drop: {\n          description: "Drop documents with \'network.name\' of \'Guest\'",\n          if: "ctx?.network?.name == \'Guest\'"\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents with \'network.name\' of \'Guest\'",\n        if: "ctx?.network?.name == \'Guest\'",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents with \'network.name\' of \'Guest\'",\n        "if": "ctx?.network?.name == \'Guest\'"\n      }\n    }\n  ]\n}\n```\n\nIf the [`script.painless.regex.enabled`](circuit-breaker.html#script-painless-regex-enabled) cluster setting is enabled, you can use regular expressions in your `if` condition scripts. For supported syntax, see [Painless regular expressions](/guide/en/elasticsearch/painless/8.17/painless-regexes.html).\n\nIf possible, avoid using regular expressions. Expensive regular expressions can slow indexing speeds.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "set": {\n                "description": "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n                "if": "ctx.url?.scheme =~ /^http[^s]/",\n                "field": "url.insecure",\n                "value": True\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline\',\n  body: {\n    processors: [\n      {\n        set: {\n          description: "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n          if: \'ctx.url?.scheme =~ /^http[^s]/\',\n          field: \'url.insecure\',\n          value: true\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      set: {\n        description: "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n        if: "ctx.url?.scheme =~ /^http[^s]/",\n        field: "url.insecure",\n        value: true,\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "set": {\n        "description": "If \'url.scheme\' is \'http\', set \'url.insecure\' to true",\n        "if": "ctx.url?.scheme =~ /^http[^s]/",\n        "field": "url.insecure",\n        "value": true\n      }\n    }\n  ]\n}\n```\n\nYou must specify `if` conditions as valid JSON on a single line. However, you can use the [Kibana console](/guide/en/kibana/8.17/console-kibana.html#configuring-console)\'s triple quote syntax to write and debug larger scripts.\n\nIf possible, avoid using complex or expensive `if` condition scripts. Expensive condition scripts can slow indexing speeds.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents that don\'t contain \'prod\' tag",\n                "if": "\\n            Collection tags = ctx.tags;\\n            if(tags != null){\\n              for (String tag : tags) {\\n                if (tag.toLowerCase().contains(\'prod\')) {\\n                  return false;\\n                }\\n              }\\n            }\\n            return true;\\n        "\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents that don\'t contain \'prod\' tag",\n        if: "\\n            Collection tags = ctx.tags;\\n            if(tags != null){\\n              for (String tag : tags) {\\n                if (tag.toLowerCase().contains(\'prod\')) {\\n                  return false;\\n                }\\n              }\\n            }\\n            return true;\\n        ",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents that don\'t contain \'prod\' tag",\n        "if": """\n            Collection tags = ctx.tags;\n            if(tags != null){\n              for (String tag : tags) {\n                if (tag.toLowerCase().contains(\'prod\')) {\n                  return false;\n                }\n              }\n            }\n            return true;\n        """\n      }\n    }\n  ]\n}\n```\n\nYou can also specify a [stored script](modules-scripting-using.html#script-stored-scripts "Store and retrieve scripts") as the `if` condition.\n\n```\nresp = client.put_script(\n    id="my-prod-tag-script",\n    script={\n        "lang": "painless",\n        "source": "\\n      Collection tags = ctx.tags;\\n      if(tags != null){\\n        for (String tag : tags) {\\n          if (tag.toLowerCase().contains(\'prod\')) {\\n            return false;\\n          }\\n        }\\n      }\\n      return true;\\n    "\n    },\n)\nprint(resp)\n\nresp1 = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents that don\'t contain \'prod\' tag",\n                "if": {\n                    "id": "my-prod-tag-script"\n                }\n            }\n        }\n    ],\n)\nprint(resp1)\n```\n\n```\nconst response = await client.putScript({\n  id: "my-prod-tag-script",\n  script: {\n    lang: "painless",\n    source:\n      "\\n      Collection tags = ctx.tags;\\n      if(tags != null){\\n        for (String tag : tags) {\\n          if (tag.toLowerCase().contains(\'prod\')) {\\n            return false;\\n          }\\n        }\\n      }\\n      return true;\\n    ",\n  },\n});\nconsole.log(response);\n\nconst response1 = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents that don\'t contain \'prod\' tag",\n        if: {\n          id: "my-prod-tag-script",\n        },\n      },\n    },\n  ],\n});\nconsole.log(response1);\n```\n\n```\nPUT _scripts/my-prod-tag-script\n{\n  "script": {\n    "lang": "painless",\n    "source": """\n      Collection tags = ctx.tags;\n      if(tags != null){\n        for (String tag : tags) {\n          if (tag.toLowerCase().contains(\'prod\')) {\n            return false;\n          }\n        }\n      }\n      return true;\n    """\n  }\n}\n\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents that don\'t contain \'prod\' tag",\n        "if": { "id": "my-prod-tag-script" }\n      }\n    }\n  ]\n}\n```\n\nIncoming documents often contain object fields. If a processor script attempts to access a field whose parent object does not exist, Elasticsearch returns a `NullPointerException`. To avoid these exceptions, use [null safe operators](/guide/en/elasticsearch/painless/8.17/painless-operators-reference.html#null-safe-operator), such as `?.`, and write your scripts to be null safe.\n\nFor example, `ctx.network?.name.equalsIgnoreCase(\'Guest\')` is not null safe. `ctx.network?.name` can return null. Rewrite the script as `\'Guest\'.equalsIgnoreCase(ctx.network?.name)`, which is null safe because `Guest` is always non-null.\n\nIf you can’t rewrite a script to be null safe, include an explicit null check.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline",\n    processors=[\n        {\n            "drop": {\n                "description": "Drop documents that contain \'network.name\' of \'Guest\'",\n                "if": "ctx.network?.name != null && ctx.network.name.contains(\'Guest\')"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline",\n  processors: [\n    {\n      drop: {\n        description: "Drop documents that contain \'network.name\' of \'Guest\'",\n        if: "ctx.network?.name != null && ctx.network.name.contains(\'Guest\')",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline\n{\n  "processors": [\n    {\n      "drop": {\n        "description": "Drop documents that contain \'network.name\' of \'Guest\'",\n        "if": "ctx.network?.name != null && ctx.network.name.contains(\'Guest\')"\n      }\n    }\n  ]\n}\n```\n\n### Conditionally apply pipelines\n\nCombine an `if` condition with the [`pipeline`](pipeline-processor.html "Pipeline processor") processor to apply other pipelines to documents based on your criteria. You can use this pipeline as the [default pipeline](ingest.html#set-default-pipeline "Set a default pipeline") in an [index template](index-templates.html "Index templates") used to configure multiple data streams or indices.\n\n```\nresp = client.ingest.put_pipeline(\n    id="one-pipeline-to-rule-them-all",\n    processors=[\n        {\n            "pipeline": {\n                "description": "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n                "if": "ctx.service?.name == \'apache_httpd\'",\n                "name": "httpd_pipeline"\n            }\n        },\n        {\n            "pipeline": {\n                "description": "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n                "if": "ctx.service?.name == \'syslog\'",\n                "name": "syslog_pipeline"\n            }\n        },\n        {\n            "fail": {\n                "description": "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n                "if": "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n                "message": "This pipeline requires service.name to be either `syslog` or `apache_httpd`"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'one-pipeline-to-rule-them-all\',\n  body: {\n    processors: [\n      {\n        pipeline: {\n          description: "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n          if: "ctx.service?.name == \'apache_httpd\'",\n          name: \'httpd_pipeline\'\n        }\n      },\n      {\n        pipeline: {\n          description: "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n          if: "ctx.service?.name == \'syslog\'",\n          name: \'syslog_pipeline\'\n        }\n      },\n      {\n        fail: {\n          description: "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n          if: "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n          message: \'This pipeline requires service.name to be either `syslog` or `apache_httpd`\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "one-pipeline-to-rule-them-all",\n  processors: [\n    {\n      pipeline: {\n        description:\n          "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n        if: "ctx.service?.name == \'apache_httpd\'",\n        name: "httpd_pipeline",\n      },\n    },\n    {\n      pipeline: {\n        description: "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n        if: "ctx.service?.name == \'syslog\'",\n        name: "syslog_pipeline",\n      },\n    },\n    {\n      fail: {\n        description:\n          "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n        if: "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n        message:\n          "This pipeline requires service.name to be either `syslog` or `apache_httpd`",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/one-pipeline-to-rule-them-all\n{\n  "processors": [\n    {\n      "pipeline": {\n        "description": "If \'service.name\' is \'apache_httpd\', use \'httpd_pipeline\'",\n        "if": "ctx.service?.name == \'apache_httpd\'",\n        "name": "httpd_pipeline"\n      }\n    },\n    {\n      "pipeline": {\n        "description": "If \'service.name\' is \'syslog\', use \'syslog_pipeline\'",\n        "if": "ctx.service?.name == \'syslog\'",\n        "name": "syslog_pipeline"\n      }\n    },\n    {\n      "fail": {\n        "description": "If \'service.name\' is not \'apache_httpd\' or \'syslog\', return a failure message",\n        "if": "ctx.service?.name != \'apache_httpd\' && ctx.service?.name != \'syslog\'",\n        "message": "This pipeline requires service.name to be either `syslog` or `apache_httpd`"\n      }\n    }\n  ]\n}\n```\n\n### Get pipeline usage statistics\n\nUse the [node stats](cluster-nodes-stats.html "Nodes stats API") API to get global and per-pipeline ingest statistics. Use these stats to determine which pipelines run most frequently or spend the most time processing.\n\n```\nresp = client.nodes.stats(\n    metric="ingest",\n    filter_path="nodes.*.ingest",\n)\nprint(resp)\n```\n\n```\nresponse = client.nodes.stats(\n  metric: \'ingest\',\n  filter_path: \'nodes.*.ingest\'\n)\nputs response\n```\n\n```\nconst response = await client.nodes.stats({\n  metric: "ingest",\n  filter_path: "nodes.*.ingest",\n});\nconsole.log(response);\n```\n\n```\nGET _nodes/stats/ingest?filter_path=nodes.*.ingest\n```\n',
          title: 'Ingest pipelines',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest.html',
          productName: 'elasticsearch',
          score: 157.81056,
        },
      },
      {
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-pipeline-search.html',
        title: 'Ingest pipelines in Search',
        llmScore: 6,
        relevanceScore: 154.87169,
        document: {
          content:
            '## Ingest pipelines in Search\n\nYou can manage ingest pipelines through Elasticsearch APIs or Kibana UIs.\n\nThe **Content** UI under **Search** has a set of tools for creating and managing indices optimized for search use cases (non time series data). You can also manage your ingest pipelines in this UI.\n\n### Find pipelines in Content UI\n\nTo work with ingest pipelines using these UI tools, you’ll be using the **Pipelines** tab on your search-optimized Elasticsearch index.\n\nTo find this tab in the Kibana UI:\n\n1. Go to **Search > Content > Elasticsearch indices**.\n2. Select the index you want to work with. For example, `search-my-index`.\n3. On the index’s overview page, open the **Pipelines** tab.\n4. From here, you can follow the instructions to create custom pipelines, and set up ML inference pipelines.\n\nThe tab is highlighted in this screenshot:\n\n![ingest pipeline ent search ui](images/ingest/ingest-pipeline-ent-search-ui.png)\n\n### Overview\n\nThese tools can be particularly helpful by providing a layer of customization and post-processing of documents. For example:\n\n* providing consistent extraction of text from binary data types\n* ensuring consistent formatting\n* providing consistent sanitization steps (removing PII like phone numbers or SSN’s)\n\nIt can be a lot of work to set up and manage production-ready pipelines from scratch. Considerations such as error handling, conditional execution, sequencing, versioning, and modularization must all be taken into account.\n\nTo this end, when you create indices for search use cases, (including [Elastic web crawler](/guide/en/enterprise-search/8.17/crawler.html), [connectors](es-connectors.html "Ingest content with Elastic connectors"). , and API indices), each index already has a pipeline set up with several processors that optimize your content for search.\n\nThis pipeline is called `ent-search-generic-ingestion`. While it is a "managed" pipeline (meaning it should not be tampered with), you can view its details via the Kibana UI or the Elasticsearch API. You can also [read more about its contents below](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference").\n\nYou can control whether you run some of these processors. While all features are enabled by default, they are eligible for opt-out. For [Elastic crawler](/guide/en/enterprise-search/8.17/crawler.html) and [connectors](es-connectors.html "Ingest content with Elastic connectors"). , you can opt out (or back in) per index, and your choices are saved. For API indices, you can opt out (or back in) by including specific fields in your documents. [See below for details](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings-using-the-api "Using the API").\n\nAt the deployment level, you can change the default settings for all new indices. This will not effect existing indices.\n\nEach index also provides the capability to easily create index-specific ingest pipelines with customizable processing. If you need that extra flexibility, you can create a custom pipeline by going to your pipeline settings and choosing to "copy and customize". This will replace the index’s use of `ent-search-generic-ingestion` with 3 newly generated pipelines:\n\n1. `<index-name>`\n2. `<index-name>@custom`\n3. `<index-name>@ml-inference`\n\nLike `ent-search-generic-ingestion`, the first of these is "managed", but the other two can and should be modified to fit your needs. You can view these pipelines using the platform tools (Kibana UI, Elasticsearch API), and can also [read more about their content below](ingest-pipeline-search.html#ingest-pipeline-search-details-specific "Index-specific ingest pipelines").\n\n### Pipeline Settings\n\nAside from the pipeline itself, you have a few configuration options which control individual features of the pipelines.\n\n* **Extract Binary Content** - This controls whether or not binary documents should be processed and any textual content should be extracted.\n* **Reduce Whitespace** - This controls whether or not consecutive, leading, and trailing whitespaces should be removed. This can help to display more content in some search experiences.\n* **Run ML Inference** - Only available on index-specific pipelines. This controls whether or not the optional `<index-name>@ml-inference` pipeline will be run. Enabled by default.\n\nFor Elastic web crawler and connectors, you can opt in or out per index. These settings are stored in Elasticsearch in the `.elastic-connectors` index, in the document that corresponds to the specific index. These settings can be changed there directly, or through the Kibana UI at **Search > Content > Indices > \\<your index> > Pipelines > Settings**.\n\nYou can also change the deployment wide defaults. These settings are stored in the Elasticsearch mapping for `.elastic-connectors` in the `_meta` section. These settings can be changed there directly, or from the Kibana UI at **Search > Content > Settings** tab. Changing the deployment wide defaults will not impact any existing indices, but will only impact any newly created indices defaults. Those defaults will still be able to be overriden by the index-specific settings.\n\n#### Using the API\n\nThese settings are not persisted for indices that "Use the API". Instead, changing these settings will, in real time, change the example cURL request displayed. Notice that the example document in the cURL request contains three underscore-prefixed fields:\n\n```\n{\n  ...\n  "_extract_binary_content": true,\n  "_reduce_whitespace": true,\n  "_run_ml_inference": true\n}\n```\n\nOmitting one of these special fields is the same as specifying it with the value `false`.\n\nYou must also specify the pipeline in your indexing request. This is also shown in the example cURL request.\n\nIf the pipeline is not specified, the underscore-prefixed fields will actually be indexed, and will not impact any processing behaviors.\n\n### Details\n\n#### `ent-search-generic-ingestion` Reference\n\nYou can access this pipeline with the [Elasticsearch Ingest Pipelines API](get-pipeline-api.html "Get pipeline API") or via Kibana’s [Stack Management > Ingest Pipelines](ingest.html#create-manage-ingest-pipelines "Create and manage pipelines") UI.\n\nThis pipeline is a "managed" pipeline. That means that it is not intended to be edited. Editing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future. If you want to make customizations, we recommend you utilize index-specific pipelines (see below), specifically [the `<index-name>@custom` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-custom-reference "<index-name>@custom Reference").\n\n##### Processors\n\n1. `attachment` - this uses the [Attachment](attachment.html "Attachment processor") processor to convert any binary data stored in a document’s `_attachment` field to a nested object of plain text and metadata.\n2. `set_body` - this uses the [Set](set-processor.html "Set processor") processor to copy any plain text extracted from the previous step and persist it on the document in the `body` field.\n3. `remove_replacement_chars` - this uses the [Gsub](gsub-processor.html "Gsub processor") processor to remove characters like "�" from the `body` field.\n4. `remove_extra_whitespace` - this uses the [Gsub](gsub-processor.html "Gsub processor") processor to replace consecutive whitespace characters with single spaces in the `body` field. While not perfect for every use case (see below for how to disable), this can ensure that search experiences display more content and highlighting and less empty space for your search results.\n5. `trim` - this uses the [Trim](trim-processor.html "Trim processor") processor to remove any remaining leading or trailing whitespace from the `body` field.\n6. `remove_meta_fields` - this final step of the pipeline uses the [Remove](remove-processor.html "Remove processor") processor to remove special fields that may have been used elsewhere in the pipeline, whether as temporary storage or as control flow parameters.\n\n##### Control flow parameters\n\nThe `ent-search-generic-ingestion` pipeline does not always run all processors. It utilizes a feature of ingest pipelines to [conditionally run processors](ingest.html#conditionally-run-processor "Conditionally run a processor") based on the contents of each individual document.\n\n* `_extract_binary_content` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `attachment`, `set_body`, and `remove_replacement_chars` processors. Note that the document will also need an `_attachment` field populated with base64-encoded binary data in order for the `attachment` processor to have any output. If the `_extract_binary_content` field is missing or `false` on a source document, these processors will be skipped.\n* `_reduce_whitespace` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `remove_extra_whitespace` and `trim` processors. These processors only apply to the `body` field. If the `_reduce_whitespace` field is missing or `false` on a source document, these processors will be skipped.\n\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index’s Pipeline tab. To control what settings any new indices will have upon creation, see the deployment wide content settings. See [Pipeline Settings](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings").\n\n#### Index-specific ingest pipelines\n\nIn the Kibana UI for your index, by clicking on the Pipelines tab, then **Settings > Copy and customize**, you can quickly generate 3 pipelines which are specific to your index. These 3 pipelines replace `ent-search-generic-ingestion` for the index. There is nothing lost in this action, as the `<index-name>` pipeline is a superset of functionality over the `ent-search-generic-ingestion` pipeline.\n\nThe "copy and customize" button is not available at all Elastic subscription levels. Refer to the Elastic subscriptions pages for [Elastic Cloud](/subscriptions/cloud) and [self-managed](/subscriptions) deployments.\n\n##### `<index-name>` Reference\n\nThis pipeline looks and behaves a lot like the [`ent-search-generic-ingestion` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference"), but with [two additional processors](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-reference-processors "Processors").\n\nYou should not rename this pipeline.\n\nThis pipeline is a "managed" pipeline. That means that it is not intended to be edited. Editing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future. If you want to make customizations, we recommend you utilize [the `<index-name>@custom` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-specific-custom-reference "<index-name>@custom Reference").\n\n###### Processors\n\nIn addition to the processors inherited from the [`ent-search-generic-ingestion` pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference"), the index-specific pipeline also defines:\n\n* `index_ml_inference_pipeline` - this uses the [Pipeline](pipeline-processor.html "Pipeline processor") processor to run the `<index-name>@ml-inference` pipeline. This processor will only be run if the source document includes a `_run_ml_inference` field with the value `true`.\n* `index_custom_pipeline` - this uses the [Pipeline](pipeline-processor.html "Pipeline processor") processor to run the `<index-name>@custom` pipeline.\n\n###### Control flow parameters\n\nLike the `ent-search-generic-ingestion` pipeline, the `<index-name>` pipeline does not always run all processors. In addition to the `_extract_binary_content` and `_reduce_whitespace` control flow parameters, the `<index-name>` pipeline also supports:\n\n* `_run_ml_inference` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `index_ml_inference_pipeline` processor. If the `_run_ml_inference` field is missing or `false` on a source document, this processor will be skipped.\n\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index’s Pipeline tab. To control what settings any new indices will have upon creation, see the deployment wide content settings. See [Pipeline Settings](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings").\n\n##### `<index-name>@ml-inference` Reference\n\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the **Stack Management > Ingest Pipelines** page. Unlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT "managed".\n\nIt’s possible to add one or more ML inference pipelines to an index in the **Content** UI. This pipeline will serve as a container for all of the ML inference pipelines configured for the index. Each ML inference pipeline added to the index is referenced within `<index-name>@ml-inference` using a `pipeline` processor.\n\nYou should not rename this pipeline.\n\nThe `monitor_ml` Elasticsearch cluster permission is required in order to manage ML models and ML inference pipelines which use those models.\n\n##### `<index-name>@custom` Reference\n\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the **Stack Management > Ingest Pipelines** page. Unlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT "managed".\n\nYou are encouraged to make additions and edits to this pipeline, provided its name remains the same. This provides a convenient hook from which to add custom processing and transformations for your data. Be sure to read the [docs for ingest pipelines](ingest.html "Ingest pipelines") to see what options are available.\n\nYou should not rename this pipeline.\n\n### Upgrading notes\n\nExpand to see upgrading notes\n\n* `app_search_crawler` - Since 8.3, App Search web crawler has utilized this pipeline to power its binary content extraction. You can read more about this pipeline and its usage in the [App Search Guide](/guide/en/app-search/8.17/web-crawler-reference.html#web-crawler-reference-binary-content-extraction). When upgrading from 8.3 to 8.5+, be sure to note any changes that you made to the `app_search_crawler` pipeline. These changes should be re-applied to each index’s `<index-name>@custom` pipeline in order to ensure a consistent data processing experience. In 8.5+, the [index setting to enable binary content](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings") is required **in addition** to the configurations mentioned in the [App Search Guide](/guide/en/app-search/8.17/web-crawler-reference.html#web-crawler-reference-binary-content-extraction).\n* `ent_search_crawler` - Since 8.4, the Elastic web crawler has utilized this pipeline to power its binary content extraction. You can read more about this pipeline and its usage in the [Elastic web crawler Guide](/guide/en/enterprise-search/8.17/crawler-managing.html#crawler-managing-binary-content). When upgrading from 8.4 to 8.5+, be sure to note any changes that you made to the `ent_search_crawler` pipeline. These changes should be re-applied to each index’s `<index-name>@custom` pipeline in order to ensure a consistent data processing experience. In 8.5+, the [index setting to enable binary content](ingest-pipeline-search.html#ingest-pipeline-search-pipeline-settings "Pipeline Settings") is required **in addition** to the configurations mentioned in the [Elastic web crawler Guide](/guide/en/enterprise-search/8.17/crawler-managing.html#crawler-managing-binary-content).\n* `ent-search-generic-ingestion` - Since 8.5, Native Connectors, Connector Clients, and new (>8.4) Elastic web crawler indices will all make use of this pipeline by default. You can [read more about this pipeline](ingest-pipeline-search.html#ingest-pipeline-search-details-generic-reference "ent-search-generic-ingestion Reference") above. As this pipeline is "managed", any modifications that were made to `app_search_crawler` and/or `ent_search_crawler` should NOT be made to `ent-search-generic-ingestion`. Instead, if such customizations are desired, you should utilize [Index-specific ingest pipelines](ingest-pipeline-search.html#ingest-pipeline-search-details-specific "Index-specific ingest pipelines"), placing all modifications in the `<index-name>@custom` pipeline(s).\n',
          title: 'Ingest pipelines in Search',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-pipeline-search.html',
          productName: 'elasticsearch',
          score: 154.87169,
        },
      },
      {
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-apis.html',
        title: 'Ingest APIs',
        llmScore: 5,
        relevanceScore: 147.50217,
        document: {
          content:
            '## Ingest APIs\n\nUse ingest APIs to manage tasks and resources related to [ingest pipelines](ingest.html "Ingest pipelines") and processors.\n\n### Ingest pipeline APIs\n\nUse the following APIs to create, manage, and test ingest pipelines:\n\n* [Create or update pipeline](put-pipeline-api.html "Create or update pipeline API") to create or update a pipeline\n* [Get pipeline](get-pipeline-api.html "Get pipeline API") to retrieve a pipeline configuration\n* [Delete pipeline](delete-pipeline-api.html "Delete pipeline API") to delete a pipeline\n* [Simulate pipeline](simulate-pipeline-api.html "Simulate pipeline API") and [Simulate ingest](simulate-ingest-api.html "Simulate ingest API") to test ingest pipelines\n\n### Stat APIs\n\nUse the following APIs to get statistics about ingest processing:\n\n* [GeoIP stats](geoip-stats-api.html "GeoIP stats API") to get download statistics for IP geolocation databases used with the [`geoip` processor](geoip-processor.html "GeoIP processor").\n\n### Ingest IP Location Database APIs\n\nUse the following APIs to configure and manage commercial IP geolocation database downloads:\n\n* [Create or update IP geolocation database configuration](put-ip-location-database-api.html "Create or update IP geolocation database configuration API") to create or update a database configuration\n* [Get IP geolocation database configuration](get-ip-location-database-api.html "Get IP geolocation database configuration API") to retrieve a database configuration\n* [Delete IP geolocation database configuration](delete-ip-location-database-api.html "Delete IP geolocation database configuration API") to delete a database configuration\n',
          title: 'Ingest APIs',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/ingest-apis.html',
          productName: 'elasticsearch',
          score: 147.50217,
        },
      },
      {
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/put-pipeline-api.html',
        title: 'Create or update pipeline API',
        llmScore: 5,
        relevanceScore: 135.16127,
        document: {
          content:
            '## Create or update pipeline API\n\nCreates or updates an [ingest pipeline](ingest.html "Ingest pipelines"). Changes made using this API take effect immediately.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline-id",\n    description="My optional pipeline description",\n    processors=[\n        {\n            "set": {\n                "description": "My optional processor description",\n                "field": "my-keyword-field",\n                "value": "foo"\n            }\n        }\n    ],\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline-id\',\n  body: {\n    description: \'My optional pipeline description\',\n    processors: [\n      {\n        set: {\n          description: \'My optional processor description\',\n          field: \'my-keyword-field\',\n          value: \'foo\'\n        }\n      }\n    ]\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline-id",\n  description: "My optional pipeline description",\n  processors: [\n    {\n      set: {\n        description: "My optional processor description",\n        field: "my-keyword-field",\n        value: "foo",\n      },\n    },\n  ],\n});\nconsole.log(response);\n```\n\n```\nPUT _ingest/pipeline/my-pipeline-id\n{\n  "description" : "My optional pipeline description",\n  "processors" : [\n    {\n      "set" : {\n        "description" : "My optional processor description",\n        "field": "my-keyword-field",\n        "value": "foo"\n      }\n    }\n  ]\n}\n```\n\n### Request\n\n`PUT /_ingest/pipeline/<pipeline>`\n\n### Prerequisites\n\n* If the Elasticsearch security features are enabled, you must have the `manage_pipeline`, `manage_ingest_pipelines`, or `manage` [cluster privilege](security-privileges.html#privileges-list-cluster "Cluster privileges") to use this API.\n\n### Path parameters\n\n* `<pipeline>`\n\n  (Required, string) ID of the ingest pipeline to create or update.\n\n  To avoid naming collisions with built-in and Fleet-managed ingest pipelines, avoid using `@` as part of your own ingest pipelines names. The exception of that rule are the `*@custom` ingest pipelines that let you safely add a custom pipeline to managed pipelines. See also [Pipelines for Fleet and Elastic Agent](ingest.html#pipelines-for-fleet-elastic-agent "Pipelines for Fleet and Elastic Agent").\n\n### Query parameters\n\n* `if_version`\n\n  (Optional, integer) Perform the operation only if the pipeline has this version. If specified and the update is successful, the pipeline’s version is incremented.\n\n* `master_timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for the master node. If the master node is not available before the timeout expires, the request fails and returns an error. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n* `timeout`\n\n  (Optional, [time units](api-conventions.html#time-units "Time units")) Period to wait for a response from all relevant nodes in the cluster after updating the cluster metadata. If no response is received before the timeout expires, the cluster metadata update still applies but the response will indicate that it was not completely acknowledged. Defaults to `30s`. Can also be set to `-1` to indicate that the request should never timeout.\n\n### Request body\n\n* `description`\n\n  (Optional, string) Description of the ingest pipeline.\n\n* `on_failure`\n\n  (Optional, array of [processor](processors.html "Ingest processor reference") objects) Processors to run immediately after a processor failure.\n\n  Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline’s remaining processors.\n\n* `processors`\n\n  (Required, array of [processor](processors.html "Ingest processor reference") objects) Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.\n\n* `version`\n\n  (Optional, integer) Version number used by external systems to track ingest pipelines.\n\n  See the [`if_version`](put-pipeline-api.html#put-pipeline-api-query-params "Query parameters") parameter above for how the version attribute is used.\n\n* `_meta`\n\n  (Optional, object) Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.\n\n* `deprecated`\n\n  (Optional, boolean) Marks this ingest pipeline as deprecated. When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.\n\n### Examples\n\n#### Pipeline metadata\n\nYou can use the `_meta` parameter to add arbitrary metadata to a pipeline. This user-defined object is stored in the cluster state, so keeping it short is preferable.\n\nThe `_meta` parameter is optional and not automatically generated or used by Elasticsearch.\n\nTo unset `_meta`, replace the pipeline without specifying one.\n\n```\nresp = client.ingest.put_pipeline(\n    id="my-pipeline-id",\n    description="My optional pipeline description",\n    processors=[\n        {\n            "set": {\n                "description": "My optional processor description",\n                "field": "my-keyword-field",\n                "value": "foo"\n            }\n        }\n    ],\n    meta={\n        "reason": "set my-keyword-field to foo",\n        "serialization": {\n            "class": "MyPipeline",\n            "id": 10\n        }\n    },\n)\nprint(resp)\n```\n\n```\nresponse = client.ingest.put_pipeline(\n  id: \'my-pipeline-id\',\n  body: {\n    description: \'My optional pipeline description\',\n    processors: [\n      {\n        set: {\n          description: \'My optional processor description\',\n          field: \'my-keyword-field\',\n          value: \'foo\'\n        }\n      }\n    ],\n    _meta: {\n      reason: \'set my-keyword-field to foo\',\n      serialization: {\n        class: \'MyPipeline\',\n        id: 10\n      }\n    }\n  }\n)\nputs response\n```\n\n```\nconst response = await client.ingest.putPipeline({\n  id: "my-pipeline-id",\n  description: "My optional pipeline description",\n  processors: [\n    {\n      set: {\n        description: "My optional processor description",\n        field: "my-keyword-field",\n        value: "foo",\n      },\n    },\n  ],\n  _meta: {\n    reason: "set my-keyword-field to foo",\n    serialization: {\n      class: "MyPipeline",\n      id: 10,\n    },\n  },\n});\nconsole.log(response);\n```\n\n```\nPUT /_ingest/pipeline/my-pipeline-id\n{\n  "description" : "My optional pipeline description",\n  "processors" : [\n    {\n      "set" : {\n        "description" : "My optional processor description",\n        "field": "my-keyword-field",\n        "value": "foo"\n      }\n    }\n  ],\n  "_meta": {\n    "reason": "set my-keyword-field to foo",\n    "serialization": {\n      "class": "MyPipeline",\n      "id": 10\n    }\n  }\n}\n```\n\nTo check the `_meta`, use the [get pipeline](get-pipeline-api.html "Get pipeline API") API.\n',
          title: 'Create or update pipeline API',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/put-pipeline-api.html',
          productName: 'elasticsearch',
          score: 135.16127,
        },
      },
      {
        id: 'product_documentation/https://www.elastic.co/guide/en/elasticsearch/reference/8.17/processors.html',
        title: 'Ingest processor reference',
        llmScore: 5,
        relevanceScore: 134.2469,
        document: {
          content:
            '## Ingest processor reference\n\nAn [ingest pipeline](ingest.html "Ingest pipelines") is made up of a sequence of processors that are applied to documents as they are ingested into an index. Each processor performs a specific task, such as filtering, transforming, or enriching data.\n\nEach successive processor depends on the output of the previous processor, so the order of processors is important. The modified documents are indexed into Elasticsearch after all processors are applied.\n\nElasticsearch includes over 40 configurable processors. The subpages in this section contain reference documentation for each processor. To get a list of available processors, use the [nodes info](cluster-nodes-info.html "Nodes info API") API.\n\n```\nresp = client.nodes.info(\n    node_id="ingest",\n    filter_path="nodes.*.ingest.processors",\n)\nprint(resp)\n```\n\n```\nresponse = client.nodes.info(\n  node_id: \'ingest\',\n  filter_path: \'nodes.*.ingest.processors\'\n)\nputs response\n```\n\n```\nconst response = await client.nodes.info({\n  node_id: "ingest",\n  filter_path: "nodes.*.ingest.processors",\n});\nconsole.log(response);\n```\n\n```\nGET _nodes/ingest?filter_path=nodes.*.ingest.processors\n```\n\n### Ingest processors by category\n\nWe’ve categorized the available processors on this page and summarized their functions. This will help you find the right processor for your use case.\n\n* [Data enrichment processors](processors.html#ingest-process-category-data-enrichment "Data enrichment processors")\n* [Data transformation processors](processors.html#ingest-process-category-data-transformation "Data transformation processors")\n* [Data filtering processors](processors.html#ingest-process-category-data-filtering "Data filtering processors")\n* [Pipeline handling processors](processors.html#ingest-process-category-pipeline-handling "Pipeline handling processors")\n* [Array/JSON handling processors](processors.html#ingest-process-category-array-json-handling "Array/JSON handling processors")\n\n### Data enrichment processors\n\n#### General outcomes\n\n* [`append` processor](append-processor.html "Append processor")\n\n  Appends a value to a field.\n\n* [`date_index_name` processor](date-index-name-processor.html "Date index name processor")\n\n  Points documents to the right time-based index based on a date or timestamp field.\n\n* [`enrich` processor](enrich-processor.html "Enrich processor")\n\n  Enriches documents with data from another index.\n\nRefer to [Enrich your data](ingest-enriching-data.html "Enrich your data") for detailed examples of how to use the `enrich` processor to add data from your existing indices to incoming documents during ingest.\n\n* [`inference` processor](inference-processor.html "Inference processor")\n\n  Uses machine learning to classify and tag text fields.\n\n#### Specific outcomes\n\n* [`attachment` processor](attachment.html "Attachment processor")\n\n  Parses and indexes binary data, such as PDFs and Word documents.\n\n* [`circle` processor](ingest-circle-processor.html "Circle processor")\n\n  Converts a location field to a Geo-Point field.\n\n* [`community_id` processor](community-id-processor.html "Community ID processor")\n\n  Computes the Community ID for network flow data.\n\n* [`fingerprint` processor](fingerprint-processor.html "Fingerprint processor")\n\n  Computes a hash of the document’s content.\n\n* [`geo_grid` processor](ingest-geo-grid-processor.html "Geo-grid processor")\n\n  Converts geo-grid definitions of grid tiles or cells to regular bounding boxes or polygons which describe their shape.\n\n* [`geoip` processor](geoip-processor.html "GeoIP processor")\n\n  Adds information about the geographical location of an IPv4 or IPv6 address from a Maxmind database.\n\n* [`ip_location` processor](ip-location-processor.html "IP location processor")\n\n  Adds information about the geographical location of an IPv4 or IPv6 address from an ip geolocation database.\n\n* [`network_direction` processor](network-direction-processor.html "Network direction processor")\n\n  Calculates the network direction given a source IP address, destination IP address, and a list of internal networks.\n\n* [`registered_domain` processor](registered-domain-processor.html "Registered domain processor")\n\n  Extracts the registered domain (also known as the effective top-level domain or eTLD), sub-domain, and top-level domain from a fully qualified domain name (FQDN).\n\n* [`set_security_user` processor](ingest-node-set-security-user-processor.html "Set security user processor")\n\n  Sets user-related details (such as `username`, `roles`, `email`, `full_name`,`metadata`, `api_key`, `realm` and `authentication_type`) from the current authenticated user to the current document by pre-processing the ingest.\n\n* [`uri_parts` processor](uri-parts-processor.html "URI parts processor")\n\n  Parses a Uniform Resource Identifier (URI) string and extracts its components as an object.\n\n* [`urldecode` processor](urldecode-processor.html "URL decode processor")\n\n  URL-decodes a string.\n\n* [`user_agent` processor](user-agent-processor.html "User agent processor")\n\n  Parses user-agent strings to extract information about web clients.\n\n### Data transformation processors\n\n#### General outcomes\n\n* [`convert` processor](convert-processor.html "Convert processor")\n\n  Converts a field in the currently ingested document to a different type, such as converting a string to an integer.\n\n* [`dissect` processor](dissect-processor.html "Dissect processor")\n\n  Extracts structured fields out of a single text field within a document. Unlike the [grok processor](grok-processor.html "Grok processor"), dissect does not use regular expressions. This makes the dissect’s a simpler and often faster alternative.\n\n* [`grok` processor](grok-processor.html "Grok processor")\n\n  Extracts structured fields out of a single text field within a document, using the [Grok](grok.html "Grokking grok") regular expression dialect that supports reusable aliased expressions.\n\n* [`gsub` processor](gsub-processor.html "Gsub processor")\n\n  Converts a string field by applying a regular expression and a replacement.\n\n* [`redact` processor](redact-processor.html "Redact processor")\n\n  Uses the [Grok](grok.html "Grokking grok") rules engine to obscure text in the input document matching the given Grok patterns.\n\n* [`rename` processor](rename-processor.html "Rename processor")\n\n  Renames an existing field.\n\n* [`set` processor](set-processor.html "Set processor")\n\n  Sets a value on a field.\n\n#### Specific outcomes\n\n* [`bytes` processor](bytes-processor.html "Bytes processor")\n\n  Converts a human-readable byte value to its value in bytes (for example `1kb` becomes `1024`).\n\n* [`csv` processor](csv-processor.html "CSV processor")\n\n  Extracts a single line of CSV data from a text field.\n\n* [`date` processor](date-processor.html "Date processor")\n\n  Extracts and converts date fields.\n\n* [`dot_expand`](dot-expand-processor.html "Dot expander processor") processor\n\n  Expands a field with dots into an object field.\n\n* [`html_strip` processor](htmlstrip-processor.html "HTML strip processor")\n\n  Removes HTML tags from a field.\n\n* [`join` processor](join-processor.html "Join processor")\n\n  Joins each element of an array into a single string using a separator character between each element.\n\n* [`kv` processor](kv-processor.html "KV processor")\n\n  Parse messages (or specific event fields) containing key-value pairs.\n\n* [`lowercase` processor](lowercase-processor.html "Lowercase processor") and [`uppercase` processor](uppercase-processor.html "Uppercase processor")\n\n  Converts a string field to lowercase or uppercase.\n\n* [`split` processor](split-processor.html "Split processor")\n\n  Splits a field into an array of values.\n\n* [`trim` processor](trim-processor.html "Trim processor")\n\n  Trims whitespace from field.\n\n### Data filtering processors\n\n* [`drop` processor](drop-processor.html "Drop processor")\n\n  Drops the document without raising any errors.\n\n* [`remove` processor](remove-processor.html "Remove processor")\n\n  Removes fields from documents.\n\n### Pipeline handling processors\n\n* [`fail` processor](fail-processor.html "Fail processor")\n\n  Raises an exception. Useful for when you expect a pipeline to fail and want to relay a specific message to the requester.\n\n* [`pipeline` processor](pipeline-processor.html "Pipeline processor")\n\n  Executes another pipeline.\n\n* [`reroute` processor](reroute-processor.html "Reroute processor")\n\n  Reroutes documents to another target index or data stream.\n\n* [`terminate` processor](terminate-processor.html "Terminate processor")\n\n  Terminates the current ingest pipeline, causing no further processors to be run.\n\n### Array/JSON handling processors\n\n* [`for_each` processor](foreach-processor.html "Foreach processor")\n\n  Runs an ingest processor on each element of an array or object.\n\n* [`json` processor](json-processor.html "JSON processor")\n\n  Converts a JSON string into a structured JSON object.\n\n* [`script` processor](script-processor.html "Script processor")\n\n  Runs an inline or stored [script](modules-scripting.html "Scripting") on incoming documents. The script runs in the [painless `ingest` context](/guide/en/elasticsearch/painless/8.17/painless-ingest-processor-context.html).\n\n* [`sort` processor](sort-processor.html "Sort processor")\n\n  Sorts the elements of an array in ascending or descending order.\n\n### Add additional processors\n\nYou can install additional processors as [plugins](/guide/en/elasticsearch/plugins/8.17/ingest.html).\n\nYou must install any plugin processors on all nodes in your cluster. Otherwise, Elasticsearch will fail to create pipelines containing the processor.\n\nMark a plugin as mandatory by setting `plugin.mandatory` in `elasticsearch.yml`. A node will fail to start if a mandatory plugin is not installed.\n\n```\nplugin.mandatory: my-ingest-plugin\n```\n',
          title: 'Ingest processor reference',
          url: 'https://www.elastic.co/guide/en/elasticsearch/reference/8.17/processors.html',
          productName: 'elasticsearch',
          score: 134.2469,
        },
      },
    ],
  },
};
