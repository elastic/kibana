# COMPLETION

The COMPLETION command provides a general-purpose interface for text generation tasks using a Large Language Model (LLM). It supports a wide range of tasks such as question answering, summarization, translation, content rewriting, and creative generation. The command allows you to send prompts and context to an LLM directly within your queries to perform text generation tasks. Each row processed by the COMPLETION command generates a separate API call to the LLM endpoint.

## Syntax

For version 9.2.0 and above:
`COMPLETION [column =] prompt WITH { "inference_id" : "my_inference_endpoint" }`

For version 9.1.x only:
`COMPLETION [column =] prompt WITH my_inference_endpoint`

### Parameters

#### column

(Optional) The name of the output column containing the LLM's response. If not specified, the results will be stored in a column named `completion`. If the specified column already exists, it will be overwritten with the new results.

#### prompt

The input text or expression used to prompt the LLM. This can be a string literal or a reference to a column containing text.

#### my_inference_endpoint

The ID of the inference endpoint to use for the task. The inference endpoint must be configured with the `completion` task type.

## Examples

Use the default column name (results stored in `completion` column):

Generate a completion for a question and store the result in the default `completion` column. This query creates a row with a question, sends it to the LLM for text generation, and keeps both the original question and the generated completion in the output.

```esql
ROW question = "What is Elasticsearch?"
| COMPLETION question WITH { "inference_id" : "my_inference_endpoint" }
| KEEP question, completion
```

Specify the output column (results stored in `answer` column):

Generate a completion for a question and store the result in the `answer` column. This query creates a row with a question, sends it to the LLM, and stores the generated response in a column named `answer`, keeping both the question and the answer in the output.

```esql
ROW question = "What is Elasticsearch?"
| COMPLETION answer = question WITH { "inference_id" : "my_inference_endpoint" }
| KEEP question, answer
```

Summarize the top 10 highest-rated movies using a prompt:

This query retrieves the top 10 movies sorted by rating, constructs a custom prompt for each movie containing its title, synopsis, and actors, sends the prompt to the LLM to generate a summary, and stores the result in the `summary` column along with the movie title and rating.

```esql
FROM movies
| SORT rating DESC
| LIMIT 10
| EVAL prompt = CONCAT(
   "Summarize this movie using the following information: \n",
   "Title: ", title, "\n",
   "Synopsis: ", synopsis, "\n",
   "Actors: ", MV_CONCAT(actors, ", "), "\n",
  )
| COMPLETION summary = prompt WITH { "inference_id" : "my_inference_endpoint" }
| KEEP title, summary, rating
```

## Limitations

- Every row processed by the COMPLETION command generates a separate API call to the LLM endpoint, which may result in high consumption and costs.
- Starting in version 9.3.0, COMPLETION automatically limits processing to 100 rows by default to prevent accidental high consumption and costs. This limit is applied before the COMPLETION command executes. You can adjust or disable this limit using cluster settings.
- COMPLETION commands may time out when processing large datasets or complex prompts. The default timeout is 10 minutes, but you can increase this limit depending on your deployment type.
- For Elastic Cloud Serverless deployments, timeout settings require a manual override from Elastic Support.
- To avoid timeouts and high costs, reduce data volume with LIMIT or selective filters before the COMPLETION command, split complex operations into simpler queries, and monitor your LLM API usage.