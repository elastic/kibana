# COMPLETION

The COMPLETION command provides a general-purpose interface for text generation tasks using a Large Language Model (LLM). It supports a wide range of tasks such as question answering, summarization, translation, content rewriting, and creative generation. The command allows you to send prompts and context to an LLM directly within your queries to perform text generation tasks.

Every row processed by the COMPLETION command generates a separate API call to the LLM endpoint.

## Syntax

For version 9.2.0 and above:
`COMPLETION [column =] prompt WITH { "inference_id" : "my_inference_endpoint" }`

For version 9.1.x only:
`COMPLETION [column =] prompt WITH my_inference_endpoint`

### Parameters

#### column

Optional. The name of the output column containing the LLM's response. If not specified, the results will be stored in a column named `completion`. If the specified column already exists, it will be overwritten with the new results.

#### prompt

The input text or expression used to prompt the LLM. This can be a string literal or a reference to a column containing text.

#### my_inference_endpoint

The ID of the inference endpoint to use for the task. The inference endpoint must be configured with the `completion` task type.

## Examples

Use the default column name (results stored in `completion` column):

This query creates a row with a question about Elasticsearch, sends the question to the LLM using the COMPLETION command, and stores the LLM's response in the default `completion` column. The final output includes both the original question and the generated completion.

```esql
ROW question = "What is Elasticsearch?"
| COMPLETION question WITH { "inference_id" : "my_inference_endpoint" }
| KEEP question, completion
```

Specify the output column (results stored in `answer` column):

This query creates a row with a question about Elasticsearch, sends the question to the LLM, and stores the response in a custom column named `answer`. The output displays both the question and the LLM's answer.

```esql
ROW question = "What is Elasticsearch?"
| COMPLETION answer = question WITH { "inference_id" : "my_inference_endpoint" }
| KEEP question, answer
```

Summarize the top 10 highest-rated movies using a prompt:

This query selects the top 10 movies with the highest ratings, constructs a custom prompt for each movie containing its title, synopsis, and actors, and sends this prompt to the LLM to generate a summary. The results include the movie title, the generated summary, and the rating.

```esql
FROM movies
| SORT rating DESC
| LIMIT 10
| EVAL prompt = CONCAT(
   "Summarize this movie using the following information: \n",
   "Title: ", title, "\n",
   "Synopsis: ", synopsis, "\n",
   "Actors: ", MV_CONCAT(actors, ", "), "\n",
  )
| COMPLETION summary = prompt WITH { "inference_id" : "my_inference_endpoint" }
| KEEP title, summary, rating
```

## Limitations

- Every row processed by the COMPLETION command generates a separate API call to the LLM endpoint, which may result in high consumption and costs.
- Starting in version 9.3.0, COMPLETION automatically limits processing to 100 rows by default to prevent accidental high consumption and costs. This limit can be adjusted using the `esql.command.completion.limit` cluster setting.
- The command may time out when processing large datasets or complex prompts. The default timeout is 10 minutes, but it can be increased via cluster settings or query parameters, depending on your deployment type.
- For Elastic Cloud Serverless, increasing the timeout requires a manual override from Elastic Support.
- The inference endpoint must be configured with the `completion` task type.
- The command can be disabled entirely using the `esql.command.completion.enabled` cluster setting.