# COMPLETION

The COMPLETION command provides a general-purpose interface for text generation tasks using a Large Language Model (LLM). It supports a wide range of text generation tasks, such as question answering, summarization, translation, content rewriting, and creative generation. The command allows you to send prompts and context to an LLM directly within your queries to perform these tasks.

Every row processed by the COMPLETION command generates a separate API call to the LLM endpoint. By default, processing is limited to 100 rows to prevent accidental high consumption and costs. This limit can be adjusted or the command can be disabled via cluster settings. For earlier versions, it is recommended to test with small datasets, use filters and limits, and monitor usage to avoid unexpected costs.

To use this command, you must deploy your LLM model as an inference endpoint with the `completion` task type.

COMPLETION commands may time out when processing large datasets or complex prompts. The default timeout is 10 minutes, but this can be increased depending on your deployment type. If you do not want to increase the timeout, reduce data volume with LIMIT or filters, split complex operations, or configure your HTTP client's response timeout.

## Syntax

For version 9.2.0 and above:
`COMPLETION [column =] prompt WITH { "inference_id" : "my_inference_endpoint" }`

For version 9.1.x only:
`COMPLETION [column =] prompt WITH my_inference_endpoint`

### Parameters

#### column

(Optional) The name of the output column containing the LLM's response. If not specified, the results will be stored in a column named `completion`. If the specified column already exists, it will be overwritten with the new results.

#### prompt

The input text or expression used to prompt the LLM. This can be a string literal or a reference to a column containing text.

#### my_inference_endpoint

The ID of the inference endpoint to use for the task. The inference endpoint must be configured with the `completion` task type.

## Examples

Use the default column name to store the LLM's response in the `completion` column:

```esql
ROW question = "What is Elasticsearch?"
| COMPLETION question WITH { "inference_id" : "my_inference_endpoint" }
| KEEP question, completion
```
Sends the question "What is Elasticsearch?" to the LLM and stores the generated response in the `completion` column.

Specify the output column to store the LLM's response in the `answer` column:

```esql
ROW question = "What is Elasticsearch?"
| COMPLETION answer = question WITH { "inference_id" : "my_inference_endpoint" }
| KEEP question, answer
```
Sends the question "What is Elasticsearch?" to the LLM and stores the generated response in the `answer` column.

Summarize the top 10 highest-rated movies using a prompt:

```esql
FROM movies
| SORT rating DESC
| LIMIT 10
| EVAL prompt = CONCAT(
   "Summarize this movie using the following information: \n",
   "Title: ", title, "\n",
   "Synopsis: ", synopsis, "\n",
   "Actors: ", MV_CONCAT(actors, ", "), "\n",
  )
| COMPLETION summary = prompt WITH { "inference_id" : "my_inference_endpoint" }
| KEEP title, summary, rating
```
Generates a summary for each of the top 10 highest-rated movies by sending a custom prompt with movie details to the LLM and storing the result in the `summary` column.

## Limitations

- Every row processed by the COMPLETION command generates a separate API call to the LLM endpoint, which may result in high consumption and costs.
- By default, processing is limited to 100 rows. This limit can be adjusted with the `esql.command.completion.limit` cluster setting.
- The command may time out when processing large datasets or complex prompts. The default timeout is 10 minutes, and increasing it depends on your deployment type.
- The inference endpoint must be configured with the `completion` task type.
- For Elastic Cloud Serverless, timeout settings require a manual override from Elastic Support.