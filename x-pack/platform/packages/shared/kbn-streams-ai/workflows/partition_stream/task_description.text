Your task is to partition a single data stream into logical partitions. The single data stream may contain multiple partitions. Each partition MUST represent a single system.

**System definition**:

A "System" is a _single, specific_ software component, application, or service. Grouping multiple distinct systems (e.g., all databases) into one partition is incorrect. Think of it as the most granular source you want to isolate.

Some examples:
  - Specific infrastructure services: A load balancer (e.g., `lb-edge`), a stream processing unit (e.g., `stream-processor-alpha`), a central authentication service (e.g., `authn-core`), a distributed cache (`dist-cache-main`), a message broker (`msg-broker-prod`), a web server (`web-frontend-pool`), an application server (`app-server-backend`).
  - Specific application logs:
    - `order-processing-svc` (a microservice, treated as one system).
    - `inventory-update-worker` (a background job, treated as another distinct system).
    - `legacy-reporting-system` (a monolithic application, treated as one system).
  - Specific os/host log types (if distinguishable and needing separation): Potentially `kernel-logs-compute-node-123`, `auth-events-fileserver-abc` if they have distinct, consistent characteristics and represent different informational streams. Often, a field like `host.id` or `system.source_type` will help distinguish these.
  - Specific Cloud Service Logs: `cloud-provider-trails` (as one system), `blob-storage-access-events` (as another, distinct system if its logs are different), `serverless-function-invocations`.
  - Specific Network Device Logs: `firewall-dmz-primary` (as one system), `vpn-gateway-regional` (as another).
  - Specific Databases: `document-store-prod` (for its general logs, as one system), `timeseries-db-metrics` (for its logs, as another). If you had two different instances of the same database type (e.g., two separate `analytics-db` instances) with vastly different log structures or purposes that are identifiable, they _could_ even be separate systems.

/***************************************************************
*
*   For the prompt generating LLM:
*   
*   Include examples of how _not_ to partition (including in the
*   reasoning examples). E.g., too wide partitioning based on
*   initial clustering hints that are too coarse, or vice versa.
*   Emphasize that clustering is a hint.
*
***************************************************************/

These should all fall into a SINGLE partition PER unique system. Do not group distinct systems together in a single partition.

**The guiding principle is: if two data sources have different schemas or represent fundamentally different operational concerns, they are different Systems and need their own Partitions.** Even if initial clustering groups them due to some schema similarities, your deeper analysis should aim to separate them if they are indeed distinct systems.

You have access to a `partition_logs(index=string, partitions=Partition[])` tool. The current partitioning and the clusters within each partition will be included in the system prompt. It allows you to test your partitioning. Partinioning works as follows:

- Each partition has a `name`. the name should be alphanumerical chars + dashes. No underscores or dots. `name` will be appended to its parent (`index`), e.g. `redis-prod` will become `logs.redis-prod`.
- Each partition has a `condition` under which the document gets routed to its defined target.
- They are executed top-to-bottom, which means that the document will get routed to the first partition where the condition matches.
- The schema for `condition` is referenced below as `condition_schema`.

When calling `partition_logs`, you need to define the complete set of suggested partitions you want to add and validate. It will cluster logs based on DBScan and Jaccard similarity, **for each suggested partition**, **and the documents that do not match any partition and fall through**. The clustering in a partition give you a *hint* that the documents might need to be partitioned further. Ideally every document falls into a logical partition. If needed, keep adding partitions until all documents have been partitioned and there is a low amount of per-partition clusters. The count in each cluster is based on sampled documents - look at them relatively instead of absolutely.

After calling `complete`, you MUST finalize the process by call the `cluster_logs` tool once more with the finalized partitions.

/***************************************************************
*
*   For the prompt generating LLM:
*   
*   Reference the schema below using `{{{condition_schema}}}` in
*   a section at the end called "System variables".
*   Mustache will be used to render it into the system prompt.
*
***************************************************************/

`condition_schema`:
```json
{{{condition_schema}}}
```
