Create an ingest pipeline for this data stream. Use an **iterative eval/dev loop**: design, simulate, fix errors, repeat.

## Your Workflow

1. **Analyze** the dataset analysis below to understand the data structure
2. **Design** a pipeline with appropriate processors
3. **Call `simulate_pipeline`** to test it against sample data
4. **Read the errors** carefully and fix the issues
5. **Repeat** steps 3-4 until simulation passes
6. **Call `commit_pipeline`** when done

**Expect multiple iterations.** The first attempt rarely works perfectly. Read error messages - they tell you exactly what to fix.

**Less is more.** When fixing errors, simplify rather than add complexity. Remove problematic processors instead of adding workarounds. Prioritize reliable parsing of common cases over handling rare edge cases.

## Tools

- **`simulate_pipeline(pipeline)`** - Runs your COMPLETE pipeline against the ORIGINAL raw documents. Returns errors and metrics. The dataset analysis below shows parsed output - but simulation starts from raw data, so include the parsing processor in your pipeline.
- **`commit_pipeline(message)`** - Finalize when simulation passes.

## Key Rules

{{#parsing_processor}}
**Pre-generated parsing processor provided** - Use as a starting point:
```
{{{parsing_processor}}}
```
**Important:** The dataset analysis below shows documents AFTER this processor runs. But `simulate_pipeline` runs against ORIGINAL raw documents - so you MUST include this processor (or a modified version) as the first step in your pipeline to get the extracted fields. Add processors after it for date normalization, type conversions, and cleanup.
{{/parsing_processor}}

{{^parsing_processor}}
**No parsing processor provided** - Build the full pipeline from scratch.
{{/parsing_processor}}

**Target Schema:** {{{fields_schema}}}
- ECS: `log.level`, `service.name`, `host.name`, `@timestamp`
- OTel: `severity_text`, `resource.attributes.*`, `body.text`, `attributes.*`

**Required:**
- Valid `@timestamp` on â‰¥99% of documents
- Correct field types per schema
- `on_failure` handlers for error resilience

---

## Dataset: {{stream.name}}

{{{stream.description}}}

---

## Dataset Analysis

{{{initial_dataset_analysis}}}

---

Now analyze the data and start your eval/dev loop. Call `simulate_pipeline` with your first attempt, then iterate based on the results.
