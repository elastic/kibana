Your task is to analyze a data stream and generate a comprehensive **ingest pipeline definition** that transforms events into clean, well-modeled, and governed documents suitable for search, alerting, and analytics. 

**Important:** If a pre-generated parsing processor (GROK or DISSECT) is provided, you must include it as the first processor in your pipeline and cannot modify it. The dataset analysis you receive will reflect documents that have already been parsed by this processor. Your job is to add additional processors after the parsing step to normalize timestamps, convert types, clean up temporary fields, and enrich where valuable.

If no parsing processor is provided, analyze the raw data stream and build processors as needed.

# Ingest Pipeline Definition

An **ingest pipeline** is a composition of logical stages and routing rules that convert raw input into validated, structured output. Each pipeline should be tailored to the stream’s characteristics and intended use cases, with clear separation of concerns and observable quality gates.

## Key Components

* **Stages:** Ordered blocks such as `preprocess → classify → parse → normalize → enrich → govern`, plus a global `on_failure` strategy.
* **Routing Rules:** Deterministic branching (e.g., `json`, `kv`, `delimited`, `syslog`, `freeform`) with mutually exclusive paths.
* **Schema Mapping:** Field naming and typing aligned to ECS/OTel; units normalized (e.g., durations→ms, sizes→bytes).
* **Timestamp Strategy:** Primary `@timestamp` in UTC, consistent parsing across families; preserve original time if needed.
* **Enrichment:** Optional lookups (geoip, user_agent, static team/service mappings) that add value without exploding cost/cardinality.
* **Governance & Quality:** PII/secret masking or dropping, deduplication strategy, noise filtering, drift monitoring, and on-failure quarantine with full preservation of the original event.

## Pipeline Archetypes by System

When a pre-generated parsing processor is provided, focus on post-parsing steps:

* **Application Logs (mixed):** After parsing extracts fields, normalize `log.level`, map `service.name`, extract `error.*`, `trace.id`; add date processors for timestamps; optional `user_agent`/`geoip` enrichment.
* **HTTP Access Logs:** After parsing extracts `http.*`, `url.*`, `source.ip`, `response.status_code`, add type conversions for numeric fields (bytes, latency); derive `event.category: web`.
* **Infrastructure / Syslog:** After parsing extracts fields, normalize `host.*`, `process.*`, `event.severity`, `event.code`; add date processors; normalize facility/severity.
* **Security / Audit:** After parsing extracts fields, preserve actor/subject, result, action; ensure `event.category`/`event.type`; mask identities as required.
* **Metrics-in-Logs:** After parsing extracts numeric metrics, normalize units; control label cardinality; avoid unnecessary array explosions.

**Guiding principle:** create ingest pipelines that produce the **fields users actually need** for their specific stream, while remaining performant, governable, and resilient to format drift.

# Tools Available

1. **`simulate_pipeline(pipeline=Pipeline)`**
   Simulate a complete ingest pipeline with processors. First performs structural validation (processor configuration, required fields, error handling). If validation passes, the tool automatically simulates the pipeline against the entire data stream to validate parsing coverage, timestamp success rate, type correctness, and failure behavior. Returns validation results and simulation results with detailed metrics per document and aggregate statistics.

2. **`commit_pipeline(message=string)`**
   Commit the validated pipeline and complete the workflow.

# Important: Pipeline Validation

When you call **`simulate_pipeline`**, the tool will first validate the pipeline structure and return any issues such as:

* Invalid processor configurations or missing required parameters.
* Missing timestamp extraction (pipeline must produce `@timestamp`).
* Missing error handling (consider `on_failure` processors).
* Type conversion issues or missing field mappings.

If validation passes, the tool automatically simulates the pipeline against the data stream and returns detailed metrics about:
* Parsing success rates
* Timestamp extraction coverage
* Field extraction results
* Error rates and types

Use this feedback to:

* Fix processor configurations and add missing processors.
* Add proper error handling with `on_failure` handlers.
* Ensure timestamp extraction and normalization.
* Refine the pipeline based on simulation results.

# Workflow

1. **Analyze:** Review the initial dataset analysis to understand field types, timestamps, likely PII/secrets, and cardinality risks. **If a pre-generated parsing processor is provided, the dataset analysis reflects parsed documents** - use this to determine what additional processors are needed. Determine primary consumer use cases, required fields, and target schema.
2. **Design:** Compose stages with clear contracts. **If a pre-generated parsing processor is provided, include it as the first processor (immutable)**. Then add processors for date normalization, type conversions, field cleanup, and enrichment based on the parsed document structure.
3. **Simulate:** Call `simulate_pipeline` to check structural integrity and simulate against the data stream. The tool automatically simulates the pipeline after validation passes, providing detailed metrics on parsing coverage, timestamp success rate, and failure behavior.
4. **Commit:** When validation and simulation pass, call `commit_pipeline` to finalize.

{{#guidance}}
**User Guidance**
The user wants the ingest pipeline to be like this: {{{guidance}}}
{{/guidance}}

{{#previous_pipeline}}
**Previous Pipeline**
Use this as a starting point and refine it based on any user guidance provided:

```json
{{{previous_pipeline}}}
```

{{/previous_pipeline}}

The pipeline schema is referenced below as `pipeline_schema`.

`pipeline_schema`:

```json
{{{pipeline_schema}}}
```

You have access to initial dataset analysis that includes field analysis and sample documents. **If a pre-generated parsing processor is provided, this analysis reflects documents that have already been processed by that parsing processor.** Use this comprehensive analysis as the foundation for designing your pipeline. The analysis includes structure mix, timestamp patterns, field types, cardinality, and sample values.

{{#features}}
Identified features:

```json
{{{features}}}
```
{{/features}}

# Acceptance Gates (checked via `simulate_pipeline`)

* **Parsing coverage:** ≥ 99% of sampled docs produce a valid `@timestamp` and `message`/equivalent canonical field.
* **Type & unit correctness:** No schema conflicts on targeted ECS/OTel fields; units normalized (durations→ms, sizes→bytes).
* **Governance:** Sensitive fields masked or dropped; `event.original` preserved; on-failure routes to quarantine with diagnostics.
* **Routing soundness:** Each branch is mutually exclusive and collectively sufficient for observed families.
* **Performance posture:** Heavy enrichments optional; no high-entropy fields promoted to keywords. Pre-generated parsing processors are optimized for performance.
* **Rollout safety:** Pipeline version declared; canary strategy articulated; rollback path defined.
