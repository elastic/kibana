#!/usr/bin/env node

/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

/* eslint-disable no-console */

/**
 * Create a new dataset entry from clipboard data generated by copyStreamsSuggestion().
 *
 * Usage:
 * 1. In Kibana, generate a suggestion (grok, dissect, or pipeline)
 * 2. Run: copyStreamsSuggestion() in the browser console
 * 3. Pipe clipboard to this script:
 *    pbpaste | node --require ./src/setup_node_env/ ./x-pack/platform/packages/shared/kbn-evals-suite-streams/scripts/create_dataset_from_clipboard.ts
 *
 * The script will:
 * - Read JSON from stdin
 * - Generate a dataset entry
 * - Append it to the appropriate dataset file
 */

import { readFile, writeFile } from 'fs/promises';
import { join } from 'path';
import chalk from 'chalk';

interface ClipboardData {
  raw_samples: Array<Record<string, unknown>>; // Original input documents
  processed_samples: Array<Record<string, unknown>>; // After simulation
  suggestion: unknown;
  suggestionType: 'grok' | 'dissect' | 'pipeline';
}

interface GrokSuggestion {
  grokProcessor: {
    patterns: string[];
    description?: string;
  };
  simulationResult: {
    processors_metrics: Record<string, { parsed_rate: number; detected_fields: string[] }>;
  };
}

interface DissectSuggestion {
  dissectProcessor: {
    pattern: string;
    processor: {
      dissect: {
        append_separator?: string;
      };
    };
    description?: string;
  };
  simulationResult: {
    processors_metrics: Record<string, { parsed_rate: number; detected_fields: string[] }>;
  };
}

interface PipelineSuggestion {
  steps: Array<{
    action: string;
    [key: string]: unknown;
  }>;
}

const DATASET_MARKER = '  // ðŸ”§ NEW DATASETS GO HERE - Added by create_dataset_from_clipboard.ts';

async function readStdin(): Promise<string> {
  return new Promise((resolve, reject) => {
    let data = '';

    process.stdin.setEncoding('utf8');

    process.stdin.on('data', (chunk) => {
      data += chunk;
    });

    process.stdin.on('end', () => {
      resolve(data);
    });

    process.stdin.on('error', (error) => {
      reject(error);
    });
  });
}

function extractMessages(samples: Array<Record<string, unknown>>): string[] {
  const messages: string[] = [];

  for (const sample of samples) {
    // Try common message field locations
    const message =
      (sample['body.text'] as string) ||
      (sample.body as any)?.text ||
      (sample.message as string) ||
      (sample['@message'] as string);

    if (message && typeof message === 'string') {
      messages.push(message);
    }
  }

  return messages;
}

function generateStreamName(rawSamples: Array<Record<string, unknown>>): string {
  // Try to infer from common fields
  const firstSample = rawSamples[0] || {};
  const dataStream = (firstSample['data_stream.dataset'] as string) || 'unknown';
  return `logs.${dataStream.replace(/\./g, '_')}`;
}

function generateGrokDatasetEntry(
  data: ClipboardData,
  suggestion: GrokSuggestion,
  streamName: string
): string {
  const messages = extractMessages(data.raw_samples);
  const processorMetrics = Object.values(suggestion.simulationResult.processors_metrics)[0] || {
    parsed_rate: 0,
    detected_fields: [],
  };

  const detectedFields = processorMetrics.detected_fields.filter(
    (field) => !field.startsWith('@timestamp') && field !== 'body.text'
  );

  const timestamp = new Date().toISOString().split('T')[0];
  const id = `custom_${streamName.replace(/\./g, '_')}_${timestamp}`;

  return `
${DATASET_MARKER}
      {
        input: {
          stream_name: '${streamName}',
          connector_id: '', // Will be filled at runtime
          field_to_parse: 'body.text',
          sample_messages: ${JSON.stringify(messages.slice(0, 10), null, 12).replace(
            /^/gm,
            '          '
          )},
        },
        output: {
          source_id: '${id}',
          source_type: 'synthetic' as const,
          sample_messages: [],
          expected_fields: {
            // TODO: Review and adjust these fields based on your expectations
            other_fields: ${JSON.stringify(
              detectedFields.slice(0, 5).map((field) => ({
                name: field,
                type: 'keyword' as const,
                required: true,
                example_values: [],
                description: `Extracted field: ${field}`,
              })),
              null,
              14
            ).replace(/^/gm, '            ')},
          },
          pattern_characteristics: {
            expected_min_fields: ${Math.max(1, detectedFields.length - 2)},
            expected_max_fields: ${detectedFields.length + 3},
          },
          reference_patterns: {
            grok: ${JSON.stringify(suggestion.grokProcessor.patterns, null, 14).replace(
              /^/gm,
              '            '
            )},
            source: 'Generated by AI suggestion - ${timestamp}',
          },
        },
        metadata: {
          difficulty: 'medium' as const,
          notes: 'Generated from copyStreamsSuggestion(). Parse rate: ${(
            processorMetrics.parsed_rate * 100
          ).toFixed(1)}%. Review and adjust expected_fields.',
        },
      },`;
}

function generateDissectDatasetEntry(
  data: ClipboardData,
  suggestion: DissectSuggestion,
  streamName: string
): string {
  const messages = extractMessages(data.raw_samples);
  const processorMetrics = Object.values(suggestion.simulationResult.processors_metrics)[0] || {
    parsed_rate: 0,
    detected_fields: [],
  };

  const detectedFields = processorMetrics.detected_fields.filter(
    (field) => !field.startsWith('@timestamp') && field !== 'body.text'
  );

  const timestamp = new Date().toISOString().split('T')[0];
  const id = `custom_${streamName.replace(/\./g, '_')}_${timestamp}`;

  return `
${DATASET_MARKER}
      {
        input: {
          stream_name: '${streamName}',
          connector_id: '', // Will be filled at runtime
          field_to_parse: 'body.text',
          sample_messages: ${JSON.stringify(messages.slice(0, 10), null, 12).replace(
            /^/gm,
            '          '
          )},
        },
        output: {
          source_id: '${id}',
          source_type: 'synthetic' as const,
          sample_messages: [],
          expected_fields: {
            // TODO: Review and adjust these fields based on your expectations
            other_fields: ${JSON.stringify(
              detectedFields.slice(0, 5).map((field) => ({
                name: field,
                type: 'keyword' as const,
                required: true,
                example_values: [],
                description: `Extracted field: ${field}`,
              })),
              null,
              14
            ).replace(/^/gm, '            ')},
          },
          pattern_characteristics: {
            expected_min_fields: ${Math.max(1, detectedFields.length - 2)},
            expected_max_fields: ${detectedFields.length + 3},
          },
          reference_patterns: {
            dissect: '${suggestion.dissectProcessor.pattern}',
            source: 'Generated by AI suggestion - ${timestamp}',
          },
        },
        metadata: {
          difficulty: 'medium' as const,
          notes: 'Generated from copyStreamsSuggestion(). Parse rate: ${(
            processorMetrics.parsed_rate * 100
          ).toFixed(1)}%. Review and adjust expected_fields.',
        },
      },`;
}

function generatePipelineDatasetEntry(
  data: ClipboardData,
  suggestion: PipelineSuggestion,
  streamName: string
): string {
  const timestamp = new Date().toISOString().split('T')[0];
  const system = streamName.split('.')[1] || 'custom';

  // Extract processor types from pipeline
  const processorTypes = suggestion.steps.map((step) => step.action);
  const hasGrok = processorTypes.includes('grok');
  const hasDissect = processorTypes.includes('dissect');

  return `
${DATASET_MARKER}
      {
        input: {
          stream_name: '${streamName}',
          system: '${system}',
          sample_documents: ${JSON.stringify(data.raw_samples, null, 12).replace(
            /^/gm,
            '          '
          )},
        },
        output: {
          source_id: '${system}-custom-${timestamp}',
          system: '${system}',
          expected_processors: {
            ${
              hasGrok || hasDissect
                ? `parsing: {
              type: '${hasGrok ? 'grok' : 'dissect'}' as const,
              should_parse_field: 'body.text',
              expected_fields: [], // TODO: Add expected fields from your samples
            },`
                : ''
            }
            normalization: [], // TODO: Add expected normalization processors
          },
          quality_thresholds: {
            min_parse_rate: 0.8,
            min_field_count: 3,
            max_field_count: 15,
            required_semantic_fields: ['@timestamp'], // TODO: Add required OTel/ECS fields
          },
          schema_expectations: {
            expected_schema_fields: [], // TODO: Add expected OTel field names
          },
        },
        metadata: {
          difficulty: 'medium' as const,
          notes: 'Generated from copyStreamsSuggestion() on ${timestamp}. Review and complete expected fields and thresholds.',
        },
      },`;
}

async function insertDatasetEntry(
  filePath: string,
  entry: string,
  datasetType: 'grok' | 'dissect' | 'pipeline'
): Promise<void> {
  const content = await readFile(filePath, 'utf-8');

  // Find where to insert based on dataset type
  let insertPosition: number;
  let searchPattern: string;

  if (datasetType === 'pipeline') {
    // For pipeline datasets, look for the end of examples array
    searchPattern = 'examples: [';
    insertPosition = content.indexOf(searchPattern);
    if (insertPosition === -1) {
      throw new Error(`Could not find insertion point in ${filePath}`);
    }
    // Move to end of opening bracket
    insertPosition = content.indexOf('[', insertPosition) + 1;
  } else {
    // For grok/dissect, look for the type-specific section and marker
    // Find the dataset constant declaration based on type
    const datasetConstant =
      datasetType === 'grok' ? 'GROK_PATTERN_DATASETS' : 'DISSECT_PATTERN_DATASETS';
    const constantPosition = content.indexOf(`export const ${datasetConstant}`);
    if (constantPosition === -1) {
      throw new Error(`Could not find ${datasetConstant} in ${filePath}`);
    }

    // Look for the marker AFTER the dataset constant declaration
    const markerPosition = content.indexOf(DATASET_MARKER, constantPosition);
    if (markerPosition !== -1) {
      // Insert at the marker position (replace the marker, it will be re-added)
      insertPosition = markerPosition;
      const newContent = content.slice(0, insertPosition) + entry + content.slice(insertPosition);
      await writeFile(filePath, newContent, 'utf-8');
      console.log(
        chalk.green(`âœ… Dataset entry inserted into ${datasetConstant} section in ${filePath}`)
      );
      return;
    }

    // Otherwise, find the examples array after the constant
    searchPattern = 'examples: [';
    insertPosition = content.indexOf(searchPattern, constantPosition);
    if (insertPosition === -1) {
      throw new Error(`Could not find examples array in ${datasetConstant}`);
    }
    insertPosition = content.indexOf('[', insertPosition) + 1;
  }

  // Insert the new entry
  const newContent = content.slice(0, insertPosition) + entry + content.slice(insertPosition);

  await writeFile(filePath, newContent, 'utf-8');
  console.log(chalk.green(`âœ… Dataset entry added to ${filePath}`));
}

async function main() {
  console.log(chalk.bold('ðŸ“‹ Create Dataset from Clipboard\n'));
  console.log(chalk.cyan('ðŸ’¡ To use this script, run:'));
  console.log(
    chalk.gray(
      '   pbpaste | node --require ./src/setup_node_env/ ./x-pack/platform/packages/shared/kbn-evals-suite-streams/scripts/create_dataset_from_clipboard.ts'
    )
  );
  console.log();
  console.log(chalk.bold('ðŸ“– Reading from stdin...\n'));

  let inputText: string;
  try {
    inputText = await readStdin();

    if (!inputText || inputText.trim().length === 0) {
      console.error(chalk.red('âŒ No input received from stdin'));
      console.error(chalk.yellow('\nðŸ’¡ Did you forget to pipe the clipboard? Try:'));
      console.error(
        chalk.gray(
          '   pbpaste | node --require ./src/setup_node_env/ ./x-pack/platform/packages/shared/kbn-evals-suite-streams/scripts/create_dataset_from_clipboard.ts'
        )
      );
      process.exit(1);
    }
  } catch (error) {
    console.error(chalk.red(`âŒ Error reading stdin: ${error}`));
    process.exit(1);
  }

  let data: ClipboardData;
  try {
    data = JSON.parse(inputText);
  } catch (error) {
    console.error(chalk.red('âŒ Input does not contain valid JSON'));
    console.error(chalk.gray('First 200 chars:'), inputText.slice(0, 200));
    process.exit(1);
  }

  // Validate structure
  if (!data.suggestionType || !data.raw_samples || !data.suggestion) {
    console.error(chalk.red('âŒ Invalid clipboard data structure'));
    console.error(chalk.gray('Expected: { suggestionType, raw_samples, suggestion }'));
    console.error(chalk.gray('Got:'), Object.keys(data));
    process.exit(1);
  }

  console.log(chalk.blue(`ðŸ“Š Suggestion type: ${data.suggestionType}`));
  console.log(chalk.blue(`ðŸ“ Raw samples: ${data.raw_samples.length}`));
  console.log();

  const streamName = generateStreamName(data.raw_samples);
  console.log(chalk.blue(`ðŸ·ï¸  Inferred stream name: ${streamName}\n`));

  const evalsDir = join(__dirname, '..', 'evals');
  let entry: string;
  let targetFile: string;

  try {
    switch (data.suggestionType) {
      case 'grok':
        entry = generateGrokDatasetEntry(data, data.suggestion as GrokSuggestion, streamName);
        targetFile = join(evalsDir, 'pattern_extraction_datasets.ts');
        await insertDatasetEntry(targetFile, entry, 'grok');
        break;

      case 'dissect':
        entry = generateDissectDatasetEntry(data, data.suggestion as DissectSuggestion, streamName);
        targetFile = join(evalsDir, 'pattern_extraction_datasets.ts');
        await insertDatasetEntry(targetFile, entry, 'dissect');
        break;

      case 'pipeline':
        entry = generatePipelineDatasetEntry(
          data,
          data.suggestion as PipelineSuggestion,
          streamName
        );
        targetFile = join(evalsDir, 'pipeline_suggestion_datasets.ts');
        await insertDatasetEntry(targetFile, entry, 'pipeline');
        break;

      default:
        console.error(chalk.red(`âŒ Unknown suggestion type: ${data.suggestionType}`));
        process.exit(1);
    }

    console.log();
    console.log(chalk.green('âœ¨ Dataset entry created successfully!'));
    console.log();
    console.log(chalk.yellow('âš ï¸  Next steps:'));
    console.log(chalk.gray('  1. Review the generated entry in the dataset file'));
    console.log(chalk.gray('  2. Fill in TODO comments with appropriate values'));
    console.log(chalk.gray('  3. Adjust expected fields and thresholds as needed'));
    console.log(chalk.gray('  4. Run the evaluation to test the new dataset'));
    console.log();
  } catch (error) {
    console.error(chalk.red('âŒ Error creating dataset entry:'), error);
    process.exit(1);
  }
}

main().catch((error) => {
  console.error(chalk.red('Fatal error:'), error);
  process.exit(1);
});
