:plugin: csv
:type: filter

///////////////////////////////////////////
START - GENERATED VARIABLES, DO NOT EDIT!
///////////////////////////////////////////
:version: %VERSION%
:release_date: %RELEASE_DATE%
:changelog_url: %CHANGELOG_URL%
:include_path: ../../../../logstash/docs/include
///////////////////////////////////////////
END - GENERATED VARIABLES, DO NOT EDIT!
///////////////////////////////////////////

[id="plugins-{type}s-{plugin}"]

=== Csv filter plugin

include::{include_path}/plugin_header.asciidoc[]

==== Description

The CSV filter takes an event field containing CSV data, parses it,
and stores it as individual fields with optionally-specified field names.
This filter can parse data with any separator, not just commas.

[id="plugins-{type}s-{plugin}-options"]
==== Csv Filter Configuration Options

This plugin supports the following configuration options plus the <<plugins-{type}s-{plugin}-common-options>> described later.

[cols="<,<,<",options="header",]
|=======================================================================
|Setting |Input type|Required
| <<plugins-{type}s-{plugin}-autodetect_column_names>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-autogenerate_column_names>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-columns>> |<<array,array>>|No
| <<plugins-{type}s-{plugin}-convert>> |<<hash,hash>>|No
| <<plugins-{type}s-{plugin}-quote_char>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-separator>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-skip_empty_columns>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-skip_empty_rows>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-skip_header>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-source>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-target>> |<<string,string>>|No
|=======================================================================

Also see <<plugins-{type}s-{plugin}-common-options>> for a list of options supported by all
filter plugins.

&nbsp;

[id="plugins-{type}s-{plugin}-autodetect_column_names"]
===== `autodetect_column_names` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Define whether column names should be auto-detected from the header column or not.
Defaults to false.

Logstash pipeline workers must be set to `1` for this option to work.

[id="plugins-{type}s-{plugin}-autogenerate_column_names"]
===== `autogenerate_column_names` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`

Define whether column names should autogenerated or not.
Defaults to true. If set to false, columns not having a header specified will not be parsed.

[id="plugins-{type}s-{plugin}-columns"]
===== `columns` 

  * Value type is <<array,array>>
  * Default value is `[]`

Define a list of column names (in the order they appear in the CSV,
as if it were a header line). If `columns` is not configured, or there
are not enough columns specified, the default column names are
"column1", "column2", etc. In the case that there are more columns
in the data than specified in this column list, extra columns will be auto-numbered:
(e.g. "user_defined_1", "user_defined_2", "column3", "column4", etc.)

[id="plugins-{type}s-{plugin}-convert"]
===== `convert` 

  * Value type is <<hash,hash>>
  * Default value is `{}`

Define a set of datatype conversions to be applied to columns.
Possible conversions are integer, float, date, date_time, boolean

Example:

[source,ruby]
    filter {
      csv {
        convert => {
          "column1" => "integer"
          "column2" => "boolean"
        }
      }
    }

[id="plugins-{type}s-{plugin}-quote_char"]
===== `quote_char` 

  * Value type is <<string,string>>
  * Default value is `"\""`

Define the character used to quote CSV fields. If this is not specified
the default is a double quote `"`.
Optional.

[id="plugins-{type}s-{plugin}-separator"]
===== `separator` 

  * Value type is <<string,string>>
  * Default value is `","`

Define the column separator value. If this is not specified, the default
is a comma `,`. If you want to define a tabulation as a separator, you need
to set the value to the actual tab character and not `\t`.
Optional.

[id="plugins-{type}s-{plugin}-skip_empty_columns"]
===== `skip_empty_columns` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Define whether empty columns should be skipped.
Defaults to false. If set to true, columns containing no value will not get set.

[id="plugins-{type}s-{plugin}-skip_empty_rows"]
===== `skip_empty_rows` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Define whether empty rows could potentially be skipped.
Defaults to false. If set to true, rows containing no value will be tagged with "_csvskippedemptyfield".
This tag can referenced by users if they wish to cancel events using an 'if' conditional statement.

[id="plugins-{type}s-{plugin}-skip_header"]
===== `skip_header`

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Define whether the header should be skipped.
Defaults to false. If set to true, the header will be skipped.
Assumes that header is not repeated within further rows as such rows will also be skipped.
If skip_header is set without autodetect_column_names being set then columns should be set which
will result in the skipping of any row that exactly matches the specified column values.
If skip_header and autodetect_column_names are specified then columns should not be specified, in this case
autodetect_column_names will fill the columns setting in the background, from the first event seen, and any
subsequent values that match what was autodetected will be skipped.

Logstash pipeline workers must be set to `1` for this option to work.

[id="plugins-{type}s-{plugin}-source"]
===== `source` 

  * Value type is <<string,string>>
  * Default value is `"message"`

The CSV data in the value of the `source` field will be expanded into a
data structure.

[id="plugins-{type}s-{plugin}-target"]
===== `target` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Define target field for placing the data.
Defaults to writing to the root of the event.



[id="plugins-{type}s-{plugin}-common-options"]
include::{include_path}/{type}.asciidoc[]