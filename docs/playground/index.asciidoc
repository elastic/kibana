[role="xpack"]
[[playground]]
= Playground

preview::[]

// Variable (attribute) definition 
:x:                    Playground 

Use {x} to combine your Elasticsearch data with the power of large language models (LLMs) for retrieval augmented generation (RAG).
The chat interface translates your natural language questions into {es} queries, retrieves the most relevant results from your {es} documents, and passes those documents to the LLM to generate tailored responses.

Once you start chatting, use the UI to view and modify the Elasticsearch queries that search your data.
You can also view the underlying Python code that powers the chat interface, and download this code to integrate into your own application.

Learn how to get started on this page.
Refer to the following for more advanced topics:

* <<playground-context>>
* <<playground-query>>
* <<playground-troubleshooting>> 

[float]
[[playground-how-it-works]]
== How {x} works

Here's a simpified overview of how {x} works:

* User *creates a connection* to LLM provider
* User *selects a model* to use for generating responses
* User *define the model's behavior and tone* with initial instructions
** *Example*: "_You are a friendly assistant for question-answering tasks. Keep responses as clear and concise as possible._"
* User *selects {es} indices* to search
* User *enters a question* in the chat interface
* {x} *autogenerates an {es} query* to retrieve relevant documents
** User can *view and modify underlying {es} query* in the UI
* {x} *auto-selects relevant fields* from retrieved documents to pass to the LLM
** User can *edit fields targeted*
* {x} passes *filtered documents* to the LLM
** The LLM generates a response based on the original query, initial instructions, chat history, and {es} context
* User can *view the Python code* that powers the chat interface
** User can also *Download the code* to integrate into application

[float]
[[playground-availability-prerequisites]]
== Availability and prerequisites

For Elastic Cloud and self-managed deployments {x} is available in the *Search* space in {kib}, under *Content* > *{x}*.

For Elastic Serverless, {x} is available in your {es} project UI.
// TODO: Confirm URL path for Serverless

To use {x}, you'll need the following:

1. An Elastic *v8.14.0+* deployment or {es} *Serverless* project. (Start a https://cloud.elastic.co/registration[free trial]).
2. At least one *{es} index* with documents to search.
** See <<playground-getting-started-ingest, ingest data>> if you'd like to ingest sample data.
3. An account with a *supported LLM provider*. {x} supports the following:
+
[cols="2a,2a,1a", options="header"]
|===
| Provider | Models | Notes

| *Amazon Bedrock*
a|
* Anthropic: Claude 3 Sonnet
* Anthropic: Claude 3 Haiku
a|
Does not currently support streaming.

| *OpenAI*
a|
* GPT-3 turbo
* GPT-4 turbo
a|

| *Azure OpenAI*
a|
* GPT-3 turbo
* GPT-4 turbo
a|

|===

[float]
[[playground-getting-started]]
== Getting started

[float]
[[playground-getting-started-connect]]
=== Connect to LLM provider

To get started with {x}, you need to create a <<action-types,connector>> for your LLM provider.
Follow these steps on the {x} landing page:

. Under *Connect to LLM*, click *Create connector*.
. Select your *LLM provider*.
. *Name* your connector.
. Select a *URL endpoint* (or use the default).
. Enter *access credentials* for your LLM provider.

[TIP]
====
If you need to update a connector, or add a new one, click the wrench button (ðŸ”§) under *Model settings*.
====

[float]
[[playground-getting-started-ingest]]
=== Ingest data (optional)

_You can skip this step if you already have data in one or more {es} indices._

There are many options for ingesting data into {es}, including:

* The {enterprise-search-ref}/crawler.html[Elastic crawler] for web content (*NOTE*: Not yet available in _Serverless_)
* {enterprise-search-ref}/connectors.html[Elastic connectors] for data synced from third-party sources
* The {es} {ref}/docs-bulk.html[Bulk API] for JSON documents
+
.*Expand* for example
[%collapsible]
==============
To add a few documents to an index called `books` run the following in Dev Tools Console:

[source,console]
----
POST /_bulk
{ "index" : { "_index" : "books" } }
{"name": "Snow Crash", "author": "Neal Stephenson", "release_date": "1992-06-01", "page_count": 470}
{ "index" : { "_index" : "books" } }
{"name": "Revelation Space", "author": "Alastair Reynolds", "release_date": "2000-03-15", "page_count": 585}
{ "index" : { "_index" : "books" } }
{"name": "1984", "author": "George Orwell", "release_date": "1985-06-01", "page_count": 328}
{ "index" : { "_index" : "books" } }
{"name": "Fahrenheit 451", "author": "Ray Bradbury", "release_date": "1953-10-15", "page_count": 227}
{ "index" : { "_index" : "books" } }
{"name": "Brave New World", "author": "Aldous Huxley", "release_date": "1932-06-01", "page_count": 268}
{ "index" : { "_index" : "books" } }
{"name": "The Handmaids Tale", "author": "Margaret Atwood", "release_date": "1985-06-01", "page_count": 311}
----
==============

We've also provided some Jupyter notebooks to easily ingest sample data into {es}. 
Find these in the https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/ingestion-and-chunking[elasticsearch-labs] repository.
These notebooks use the official {es} Python client.
// TODO: [The above link will be broken until https://github.com/elastic/elasticsearch-labs/pull/232 is merged]

[float]
[[playground-getting-started-index]]
=== Select {es} indices

Once you've connected to your LLM provider, it's time to choose the data you want to search.
Follow the steps under *Select indices*:

. Select one or more {es} indices under *Add index*.
. Click *Start* to launch the chat interface.
+
[.screenshot]
image::select-indices.png[width=400]

Learn more about the underlying {es} queries used to search your data in <<playground-query>>.

[float]
[[playground-getting-started-setup-chat]]
=== Set up the chat interface

You can start chatting with your data immediately, but you might want to tweak some defaults first.

[.screenshot]
image::chat-interface.png[]

You can adjust the following under *Model settings*:

* *Model*. The model used for generating responses.
* *Instructions*. Also known as the _system prompt_, these initial instructions and guidelines define the behavior of the model throughout the conversation. Be *clear and specific* for best results.
* *Include citations*. A toggle to include citations from the relevant {es} documents in responses.

{x} also uses another LLM under the hood, to encode all previous questions and responses, and make them available to the main model.
This ensures the model has "conversational memory".

Under *Indices*, you can edit which {es} indices will be searched.
This will affect the underlying {es} query.

[TIP]
====
Click *âœ¨ Regenerate* to resend the last query to the model for a fresh response.

Click *âŸ³ Clear chat* to clear chat history and start a new conversation.
====

[float]
[[playground-next-steps]]
=== Next steps

Once you've got {x} up and running, and you've tested out the chat interface, you might want to explore some more advanced topics:

* <<playground-context>>
* <<playground-query>>
* <<playground-troubleshooting>> 

include::playground-context.asciidoc[]
include::playground-query.asciidoc[]
include::playground-troubleshooting.asciidoc[]

