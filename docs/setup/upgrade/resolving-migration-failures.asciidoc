[[resolve-migrations-failures]]
=== Resolve migration failures

Migrating {kib} primarily involves migrating saved object documents to be compatible
with the new version.

[float]
==== Saved object migration failures

If {kib} unexpectedly terminates while migrating a saved object index, {kib} automatically attempts to
perform the migration again when the process restarts. Do not delete any saved objects indices to
to fix a failed migration. Unlike previous versions, {kib} 7.12.0 and
later does not require deleting indices to release a failed migration lock.

If upgrade migrations fail repeatedly, refer to
<<preventing-migration-failures, preparing for migration>>.
When you address the root cause for the migration failure,
{kib} automatically retries the migration.
If you're unable to resolve a failed migration, contact Support.


[float]
[[upgrade-migrations-old-indices]]
==== Old `.kibana_N` indices

After the migrations complete, multiple {kib} indices are created in {es}: (`.kibana_1`, `.kibana_2`, `.kibana_7.12.0` etc).
{kib} only uses the index that the `.kibana` and `.kibana_task_manager` aliases point to.
The other {kib} indices can be safely deleted, but are left around as a matter of historical record, and to facilitate rolling {kib} back to a previous version.

[float]
==== Known issues with {fleet} beta
If you see a`timeout_exception` or `receive_timeout_transport_exception` error,
it might be from a known known issue in 7.12.0 if you tried the {fleet} beta.
Upgrade migrations fail because of a large number of documents in the `.kibana` index,
which causes {kib} to log errors such as:

[source,sh]
--------------------------------------------
Error: Unable to complete saved object migrations for the [.kibana] index. Please check the health of your Elasticsearch cluster and try again. Error: [receive_timeout_transport_exception]: [instance-0000000002][10.32.1.112:19541][cluster:monitor/task/get] request_id [2648] timed out after [59940ms]

Error: Unable to complete saved object migrations for the [.kibana] index. Please check the health of your Elasticsearch cluster and try again. Error: [timeout_exception]: Timed out waiting for completion of [org.elasticsearch.index.reindex.BulkByScrollTask@6a74c54]
--------------------------------------------

For instructions on how to mitigate the known issue, refer to https://github.com/elastic/kibana/issues/95321[the GitHub issue].


[float]
==== Corrupt saved objects
To find and remedy problems caused by corrupt documents, we highly recommend testing your {kib} upgrade in a development cluster,
especially when there are custom integrations that create saved objects in your environment.

Saved objects that are corrupted through manual editing or integrations cause migration
failures with a log message, such as `Unable to migrate the corrupt Saved Object document ...`.
For a successful upgrade migration, you must fix or delete corrupt documents.

For example, you receive the following error message:

[source,sh]
--------------------------------------------
Unable to migrate the corrupt saved object document with _id: 'marketing_space:dashboard:e3c5fc71-ac71-4805-bcab-2bcc9cc93275'. To allow migrations to proceed, please delete this document from the [.kibana_7.12.0_001] index.
--------------------------------------------

To delete the documents that cause migrations to fail, take the following steps:

. Remove the write block which the migration system has placed on the previous index:
+
[source,sh]
--------------------------------------------
PUT .kibana_7.12.1_001/_settings
{
  "index": {
    "blocks.write": false
  }
}
--------------------------------------------

. Delete the corrupt document:
+
[source,sh]
--------------------------------------------
DELETE .kibana_7.12.0_001/_doc/marketing_space:dashboard:e3c5fc71-ac71-4805-bcab-2bcc9cc93275
--------------------------------------------

. Restart {kib}.
+
The dashboard with the `e3c5fc71-ac71-4805-bcab-2bcc9cc93275` ID that belongs to the `marketing_space` space **is no longer available**.

[float]
[[unknown-saved-object-types]]
==== Documents for unknown saved objects
Migrations will fail if saved objects belong to an unknown
saved object type. Unknown saved objects are typically caused by
to the {es} index, or by disabling a plugin that had previously
created a saved object.

We recommend using the {kibana-ref-all}/7.17/upgrade-assistant.html[Upgrade Assistant]
to discover and remedy any unknown saved object types. {kib} version 7.17.0 deployments containing unknown saved
object types will also log the following warning message:

[source,sh]
--------------------------------------------
CHECK_UNKNOWN_DOCUMENTS Upgrades will fail for 8.0+ because documents were found for unknown saved object types. To ensure that future upgrades will succeed, either re-enable plugins or delete these documents from the ".kibana_7.17.0_001" index after the current upgrade completes.
--------------------------------------------

If you fail to remedy this, your upgrade to 8.0+ will fail with a message like:

[source,sh]
--------------------------------------------
Unable to complete saved object migrations for the [.kibana] index: Migration failed because documents were found for unknown saved object types. To proceed with the migration, please delete these documents from the ".kibana_7.17.0_001" index.
--------------------------------------------

[float]
==== Incompatible settings or mappings
Matching index templates that specify `settings.refresh_interval` or
`mappings` are known to interfere with {kib} upgrades.
This can happen when index templates are defined manually.

To make sure the index templates won't apply to new `.kibana*` indices, narrow down the {data-sources} of any user-defined index templates.

[float]
==== Incompatible `xpack.tasks.index` configuration setting
In {kib} 7.5.0 and earlier, when the task manager index is set to `.tasks`
with the configuration setting `xpack.tasks.index: ".tasks"`,
upgrade migrations fail. In {kib} 7.5.1 and later, the incompatible configuration
setting prevents upgrade migrations from starting.

[float]
==== Repeated time-out requests that eventually fail
Migrations get stuck in a loop of retry attempts waiting for index yellow status that's never reached.
In the CLONE_TEMP_TO_TARGET or CREATE_REINDEX_TEMP steps, you might see a log entry similar to:

[source,sh]
--------------------------------------------
"Action failed with [index_not_yellow_timeout] Timeout waiting for the status of the [.kibana_8.1.0_001] index to become "yellow". Retrying attempt 1 in 2 seconds."
--------------------------------------------
The process is waiting for a yellow index status. There are two known causes:

* Cluster hits the low watermark for disk usage
* Cluster has <<routing-allocation-disabled,routing allocation disabled>>

Before retrying the migration, inspect the output of the `_cluster/allocation/explain?index=${targetIndex}` API to identify why the index isn't yellow:

[source,sh]
--------------------------------------------
GET _cluster/allocation/explain
{
  "index": ".kibana_8.1.0_001",
  "shard": 0,
  "primary": true,
}
--------------------------------------------
If the cluster exceeded the low watermark for disc usage, the output should contain a message similar to this:

[source,sh]
--------------------------------------------
"The node is above the low watermark cluster setting [cluster.routing.allocation.disk.watermark.low=85%], using more disk space than the maximum allowed [85.0%], actual free: [11.692661332965082%]"
--------------------------------------------
Refer to the {es} guide for how to {ref}/fix-common-cluster-issues.html#_error_disk_usage_exceeded_flood_stage_watermark_index_has_read_only_allow_delete_block[fix common cluster issues].

If routing allocation is the issue, you should see an entry similar to this:

[source,sh]
--------------------------------------------
"allocate_explanation" : "cannot allocate because allocation is not permitted to any of the nodes"
--------------------------------------------

[float]
[[routing-allocation-disabled]]
==== Routing allocation disabled or restricted
Upgrade migrations fail because routing allocation is disabled or restricted (`cluster.routing.allocation.enable: none/primaries/new_primaries`), which causes {kib} to log errors such as:

[source,sh]
--------------------------------------------
Unable to complete saved object migrations for the [.kibana] index: The elasticsearch cluster has cluster routing allocation incorrectly set for migrations to continue. To proceed, please remove the cluster routing allocation settings with PUT /_cluster/settings {"transient": {"cluster.routing.allocation.enable": null}, "persistent": {"cluster.routing.allocation.enable": null}}
--------------------------------------------

To get around the issue, remove the transient and persisted routing allocation settings:
[source,sh]
--------------------------------------------
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.enable": null
  }, 
  "persistent": {
    "cluster.routing.allocation.enable": null
  }
}
--------------------------------------------
