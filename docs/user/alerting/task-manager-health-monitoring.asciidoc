[role="xpack"]
[[task-manager-health-monitoring]]
== Task Manager Health Monitoring

The {kib} Task Manager has an internal monitoring mechanism in which keeps track of a variety of metrics which can be consumed via a `health` api endpoint, or via the {kib} Server Log.

The `health` api is exposed at `/api/task_manager/_health`, and is designed to be fast enough to provide a reliable monitoriable endpoint.
Consuming this endpoint doesn't actually cause additional load by performing health checks, but rather it returns the latest observations made by the system. This design enables the consumption of by external services at a regular cadance without the concern of additional load to the system.

Monitoring this `health` api is the recommended method for monitoring the health of the {kib} Task Management, which enables such mission critical services such as Alerting and Actions.

[float]
[[task-manager-configuring-health-monitoring]]
=== Configuring the Monitored Health Statistics

The Health Endpoint monitors Task Manager's performance out of the box, but certain performance considerination are deployment specific and are configurable as a result.

Health thresholds are configurable via the <<task-manager-health-settings,`xpack.task_manager.monitored_task_execution_thresholds`>> setting.
This configuration specifies the threshold of failed task executions which, once exceeded, sets a health status of `warn` or `error` on the corresponding task type execution status.
This setting can be set as either the default level, which is applied to all task types in the system, or at a custom task type specific level. 

By default, this setting is configured to mark the health of every task type as `warning` when it exceeds 80% failed executions, and as `error` at 90%.
This value can be set to any number between 0 to 100, and a threshold is hit when the value *exceeds* this number.
This means that you can avoid setting the status to `error` by setting the threshold at 100, or hit `error` the moment any task failes by setting the threshold to 0 (as it will exceed 0 once a single failer occurs).

Custom configurations allow you to set lower thresholds for task types you might consider critical, such as alerting tasks, which you might wish to detect sooner in an external monitoring service.

[source,yml]
----
xpack.task_manager.monitored_task_execution_thresholds:
  default: # <1>
    error_threshold: 70
    warn_threshold: 50
  custom:
    "alerting:.index-threshold": # <2>
      error_threshold: 50
      warn_threshold: 0
----
<1> default configuration, setting a system wide `warn` threshold at 50%, and `error` at 70% failure rate
<2> custom configuration for the `alerting:.index-threshold` task type, setting a system wide `warn` threshold at 0% (which will set a `warn` status the moment any task of that type fails), and `error` at 50% failure rate

[float]
[[task-manager-consuming-health-stats]]
=== Consuming Health Stats

The `health` api is best consumed by via the `/api/task_manager/_health` endpoint.

Additionally, the metrics are logged out into the {kib} `DEBUG` logger at a regular cadence.
If you wish to enable Task Manager DEBUG logging in your {kib} instance, you will need to add the following to your `Kibana.yml`:
```
logging:
  loggers:
      - context: plugins.taskManager
        appenders: [console]
        level: debug
```

Please bear in mind that these stats are logged as often as your <<task-manager-settings,`xpack.task_manager.poll_interval`>> setting, which means it could add substantial noise to your logs.

We would recommend only enabling this level of logging temporarily.

[float]
[[making-sense-of-task-manager-health-stats]]
=== Making Sense of Task Manager Health Stats

The Health Monitoring api exposes three sections: `configuration`, `workload` and `runtime`:

[cols="2"]
|===

a| Configuration

| This section summarizes Task Manager's current configuration, including dynamic configurations which change over time, such as `poll_interval` and `max_workers` which can adjust in reaction to changing load on the system.

a| Workload

| This section summarizes the work load across the cluster, listing the tasks in the system, their types and what their current status is.

a| Runtime

| This section tracks excution performance of Task Manager, tracking task _drift_, worker _load_, and execution stats broken down by type, including duration and execution results

|===

Each section has a `timestamp` and a `status` which indicates when the last update to this setion took place and whether the health of this section was evaluated as `OK`, `Warning` or `Error`.

The root has its own `status` which indicate the `status` of the system overall.
