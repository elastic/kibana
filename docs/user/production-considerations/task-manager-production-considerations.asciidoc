[role="xpack"]
[[task-manager]]
== Task Manager

{kib} Task Manager is leveraged by features such as Alerting, Actions, and Reporting to run mission critical work as persistent background tasks.
These background tasks distribute work across multiple {kib} instances.
This has three major benefits:

* *Persistence*: all task state and scheduling is stored in {es}, so if {kib} is restarted, tasks will pick up where they left off. Task definitions are stored in the index specified by `xpack.task_manager.index` (defaults to `.kibana_task_manager`).  It is important to have at least 1 replica of this index for production deployments, since if you lose this index all scheduled tasks are also lost.
* *Scaling*: multiple {kib} instances can read from and update the same task queue in {es}, allowing the work load to be distributed across instances. In cases where a {kib} instance no longer has capacity to run tasks, capacity can be increased by adding additional {kib} instances.
* *Load Balancing*: Task Manager is equipped with a reactive self-healing mechanism, which allows it to reduce the amount of work it executes in reaction to an increased load related error rate in {es}. Additionally, when Task Manager experiences an increase in recurring tasks, it attempts to space out the work in such a manner as to better balance the load.


[float]
[[task-manager-background-tasks]]
=== Running Background Tasks

{kib} background tasks are managed by:

* Polling an {es} task index for overdue tasks at 3 second intervals.  This interval can be changed using the `xpack.task_manager.poll_interval` setting.
* Tasks are then claiming them by updating them in the {es} index, using optimistic concurrency control to prevent conflicts. Each {kib} instance can run a maximum of 10 concurrent tasks, so a maximum of 10 tasks are claimed each interval. 
* Tasks are run on the {kib} server. 
* Task Manager ensures that tasks:
** Are only executed once
** Are retried when they fail (if configured to do so)
** Are rescheduled to run again at a future point in time (if configured to do so)

[IMPORTANT]
==============================================
It is possible for tasks to be run late or at an inconsistent schedule.

This is usually a symptom of the specific usage or scaling strategy of the cluster in question.

Such issues can be addressed by tweaking the {kib} Task Manager settings or the cluster scaling strategy to better suit the unique use case.

For details on the settings that can influence the performance and throughput of Task Manager, see <<task-manager-settings-kb, Task Manager Settings>>.

For detailed troubleshooting guidance, see <<task-manager-troubleshooting>>.
==============================================

[float]
=== Deployment considerations

{es} and {kib} instances use the system clock to determine the current time. To ensure schedules are triggered when expected, you should synchronize the clocks of all nodes in the cluster using a time service such as http://www.ntp.org/[Network Time Protocol].

[float]
[[task-manager-scaling-guidance]]
=== Scaling Guidance

How you deploy {kib} largely depends on your use case. With that in mind, it is difficult to predict the throughout a deplyment might require in order to support {kib} Task Management is difficult, as features can schedule an unpredictable number of tasks at a variety of scheduled cadences.

That said, there is a relatively straight forward method you can follow to produce a rough estimate based on your expected usage.

[float]
[[task-manager-default-scaling]]
==== Default Scale

By default {kib} polls for tasks at a rate of 10 tasks every 3 seconds.
This means that we can reasonably expect a single {kib} instance to support up to 200 _tasks per minute_ (hence forth termed as `200/tpm`).

In practice, a {kib} instance will only ever achieve that upper bound of `200/tpm` if the duration of task execution is below the polling rate of 3 seconds. For the most part, the duration of tasks is below that threshold, but it can vary greatly as {es} and {kib} usage grow and task complexity increases (such as Alerts executing heavy queries across large datasets).

By <<task-manager-health-evaluate-the-workload,evaluating the workload>> a rough estimate can be made as to the required throughput as a _tasks per minute_ measurement.

For example, suppose your current workload reveals a required throughput of `440/tpm`, this scale could be addressed by provisioning 3 {kib} instances, providing an upper throughput of `600/tpm`. This scale would provide aproximated 25% additional capacity to handle ad-hoc non-recurring tasks and potential growth in recurring tasks.

It is highly recommended that you maintain at least 20% additional capacity, beyond your expected workload, as spikes in ad hoc tasks is possible at times of high activity (such as a spike in _Actions_ in response to an active _Alert_).

For details on monitoring the health of {kib} Task Manager, follow the guidance under <<task-manager-health-monitoring>>.

[float]
[[task-manager-scaling-horizontally]]
==== Scaling Horizontaly

At times the sustainable approach might be to expand the throughput of your cluster by provisioning additional {kib} instances.
By default, each additional {kib} instance will add an additional 10 tasks that your cluster can run concurrenctly, but you can also scale each {kib} instance vertically, if your diagnosis indicates they can handle the additional workload.

[float]
[[task-manager-scaling-vertically]]
==== Scaling Vertically

Other times it, might be preferable to increase the throughput of individual {kib} instances.

Tweak the *Max Workers* via the <<task-manager-settings,`xpack.task_manager.max_workers`>> setting, which would allow each {kib} to pull a higher number of tasks per interval, but keep in mind that this could impact the performance of each {kib} instance as _their_ workload would be higher.

Tweak the *Poll Interval* via the <<task-manager-settings,`xpack.task_manager.poll_interval`>> setting, which would allow each {kib} to pull scheduled tasks at a higher rate, but keep in mind that this could impact the performance of the {es} cluster as _their_ workload would be higher.

[float]
[[task-manager-choosing-scaling-strategy]]
==== Choosing a Scaling Strategy

Each scaling strategy comes with its own considerations, and as a consequence, the appropriate strategy largely depends on your use case

Scaling {kib} instances vertically causes higher resource utilization in each {kib} instance, as it will perform more concurrent work.
Scaling {kib} instances horizontally requires a higher degree of coordination which can impact overall performance.

Finding the sweet spot between both strategies requires experimentation, but in general our recommended strategy would be to follow these steps:

1. Produce a <<task-manager-rough-throughput-estimation,rough throughput estimate>> as a guide to provisioning as many {kib} instances as needed. We recommend including any growth in tasks you predict experiencing in the near future, and a buffer to better address ad-hoc tasks.
2. After provisioning a deployment as per the rough estimate, assess whether the provisioned {kib} instances achieve the required throughput by evaluating the <<task-manager-health-monitoring>> as described under <<task-manager-theory-insufficient-throughput,"Insufficient Throughtput to Handle the Scheduled Workload">>
3. If the throughput is insufficient, and {kib} instances are exhibiting low resource utilization, incrementally scale vertically all the while <<kibana-page,monitoring>> the impact of these changes.
4. If the throughput is insufficient, and {kib} instances are exhibiting high resource utilization, incrementally scale horizontally by provisioning new {kib} instances and reassess.

{kib} Task Manager, like the rest of the Elastic Stack, has been designed to scale horizontally, and we recommend taking advantage of this ability to ensure mission ciritcal services such as Alerting and Reporting always have the capacity they need.

Scaling horizontally requires a higher degree of coordination between {kib} instances. One way by which {kib} Task Manager coordinates with other instances is delaying its polling schedule to avoid conflicting with other instances.
By using <<task-manager-health-monitoring>> to evaluate the <<task-manager-health-evaluate-the-runtime,date of the `last_polling_delay`>> across a deployment, we can estimate the frequency at which Task Manager resets its delay mechanism.
A higher frequency suggests {kib} instances conflict at a high rate, which can be addressed by scaling vertically rather than horizontally, in effect reducing the required coordination.

[float]
[[task-manager-rough-throughput-estimation]]
==== Rough Throughput Estimation

Predicting the required throughout a deployment might need to support {kib} Task Management is difficult, as features can schedule an unpredictable number of tasks at a variety of scheduled cadences.
That said, a rough lower bound can be estimated which is then used as a guide.

Throughput is best thought of as a measurements in _tasks per minute_.

As <<task-manager-default-scaling,mentioned above>>, a default {kib} instance can support up to `200/tpm`.

Given a hypothetical deployment of 100 recurring tasks, estimating the required throughput depends entirely on their scheduled cadence.
Suppose we expect to run 50 tasks at a cadence of `10s`, the other 50 at `20m` and in addition, we expect a couple dozen non-recurring tasks every minute.

A non-recurring task requires a single execution, which means that a single {kib} instance could execute all 100 tasks in less than a minute, utilizing only half of its capacity. As these tasks are only executed once, the {kib} instance will sit idle once all tasks have executed.
For that reason, we don't tend to include non-recurring tasks in our _tasks per minute_ calculation. Instead, we ensure a _buffer_ included in the final _lower bound_ to incurr the unpredictable cost of ad-hoc non-recurring tasks.

A recurring task requires as many executions as its cadence can fit into a minute. This means that a recurring task with a `10s` schedule will require as `6/tpm`, as it will be executed 6 times per minute. On the other hand, a recurring tasks with a `20m` schedule, will only execute 3 times per hour, meaning it would only require a throughput of `0.05/tpm`, a number so small it is difficult to take it into account.

For this reason, we recommend grouping tasks by _tasks per minute_ and _tasks per hour_, as demonstrated under <<task-manager-health-evaluate-the-workload,"Evaluate your workload">>, averaging the _per hour_ measurement across all minutes.

Given our predicted workload, we estimate a lower bound throughput of `340/tpm` (`6/tpm` * 50 + `3/tph` * 50 + 20% buffer).
As a default {kib} instance provides us with a throughput of `200/tpm`, a good starting point for our deployment would appear to be to provision 2 {kib} instances. We would then monitor their performance and reassess as the required throughput becomes clearer.

We recognize this is nothing more than a _rough_ estimate, but this rough _tasks per minute_ provides the lower bound that is needed in order to execute tasks on time.
Once you have calculated the rough _tasks per minute_ estimate, we recommend adding a 20% buffer for non-recurring tasks. How much of a buffer is required largely depends on your use case, and as a result we recommend <<task-manager-health-evaluate-the-workload,evaluating your workload>> as it grows to ensure enough of a buffer is provisioned.
