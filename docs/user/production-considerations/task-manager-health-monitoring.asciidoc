[role="xpack"]
[[task-manager-health-monitoring]]
== Task Manager health monitoring

++++
<titleabbrev>health monitoring</titleabbrev>
++++

The {kib} Task Manager has an internal monitoring mechanism keeping track of a variety of metrics which can be consumed via both a health monitoring API, and the {kib} Server Log.

The health monitoring API is designed to be fast, providing a reliable endpoint which can be monitored.
Consuming this endpoint doesn't cause additional load, but rather it returns the latest health checks made by the system. This design enables consumption by external services at a regular cadance without the concern of additional load to the system.

Each {kib} instance exposes its own endpoint at:

[source,sh]
--------------------------------------------------
$ curl -X GET api/task_manager/_health
--------------------------------------------------
// KIBANA

Monitoring the `_health` endpoint of each {kib} instance in the cluster is the recommended method of ensuring confidence in mission critical services such as Alerting and Actions.

[float]
[[task-manager-configuring-health-monitoring]]
=== Configuring the Monitored Health Statistics

The health monitoring api monitors Task Manager's performance out of the box, but certain performance considerination are deployment specific and are configurable as a result.

A health threshold is the threshold for failed task executions.  Once a task exceeds this threshold, a status of `warn` or `error` is set on the task type execution. To configure a health threshold, use the <<task-manager-health-settings,`xpack.task_manager.monitored_task_execution_thresholds`>> setting.  You can apply this this setting to all task types in the system, or to a custom task type. 

By default, this setting is configured to mark the health of every task type as `warning` when it exceeds 80% failed executions, and as `error` at 90%.
This value can be set to any number between 0 to 100, and a threshold is hit when the value *exceeds* this number.
This means that you can avoid setting the status to `error` by setting the threshold at 100, or hit `error` the moment any task fails by setting the threshold to 0 (as it will exceed 0 once a single failure occurs).

Custom configurations allow you to set lower thresholds for task types you might consider critical, such as alerting tasks, which you might wish to detect sooner in an external monitoring service.

[source,yml]
----
xpack.task_manager.monitored_task_execution_thresholds:
  default: # <1>
    error_threshold: 70
    warn_threshold: 50
  custom:
    "alerting:.index-threshold": # <2>
      error_threshold: 50
      warn_threshold: 0
----
<1> default configuration, setting a system wide `warn` threshold at 50%, and `error` at 70% failure rate
<2> custom configuration for the `alerting:.index-threshold` task type, setting a system wide `warn` threshold at 0% (which will set a `warn` status the moment any task of that type fails), and `error` at 50% failure rate

[float]
[[task-manager-consuming-health-stats]]
=== Consuming Health Stats

The `health` api is best consumed by via the `/api/task_manager/_health` endpoint.

Additionally, the metrics are logged out into the {kib} `DEBUG` logger at a regular cadence.
If you wish to enable Task Manager DEBUG logging in your {kib} instance, you will need to add the following to your `Kibana.yml`:
```
logging:
  loggers:
      - context: plugins.taskManager
        appenders: [console]
        level: debug
```

Please bear in mind that these stats are logged as often as your <<task-manager-settings,`xpack.task_manager.poll_interval`>> setting, which means it could add substantial noise to your logs.

We would recommend only enabling this level of logging temporarily.

[float]
[[making-sense-of-task-manager-health-stats]]
=== Making Sense of Task Manager Health Stats

The health monitoring API exposes three sections: `configuration`, `workload` and `runtime`:

[cols="2"]
|===

a| Configuration

| This section summarizes Task Manager's current configuration, including dynamic configurations which change over time, such as `poll_interval` and `max_workers` which can adjust in reaction to changing load on the system.

a| Workload

| This section summarizes the work load across the cluster, listing the tasks in the system, their types and what their current status is.

a| Runtime

| This section tracks excution performance of Task Manager, tracking task _drift_, worker _load_, and execution stats broken down by type, including duration and execution results

|===

Each section has a `timestamp` and a `status` which indicates when the last update to this setion took place and whether the health of this section was evaluated as `OK`, `Warning` or `Error`.

The root has its own `status` which indicate the `status` of the system overall.

By monitoring the `status` of the system overall, and the `status` of specific task types of interest, it is easy to evaluate the health of the {kib} Task Management system.
