[role="xpack"]
[[task-manager-troubleshooting]]
== Task Manager Troubleshooting

Task Manager is used by a wide ange of services in {kib}, such as <<alerting-production-considerations, Alerting>>, Reporting and Telemetry.
Unexpected behavior in these services might be a downstream issue originating in Task Manager.

Use the information in this section to troubleshoot common problems and find answers for frequently asked questions.

For issues that you cannot fix yourself … we’re here to help.
If you are an existing Elastic customer with a support contract, please create a ticket in the
https://support.elastic.co/customers/s/login/[Elastic Support portal].
Or post in the https://discuss.elastic.co/[Elastic forum].


[float]
[[task-manager-health-scheduled-tasks-small-schedule-interval-run-late]]
=== Scheduled Tasks with Small Schedule Intervals Run Late

*Symptom*:

Tasks are scheduled to run every 2 seconds but they seem to be running too late

*Resolution*:

Task Manager polls for tasks at the cadence specified by the <<task-manager-settings,`xpack.task_manager.poll_interval`>> setting, which is 3 seconds by default. This means that a Task could run late if it uses a schedule which is smaller than this setting.

This can be addressed by adjusting the <<task-manager-settings,`xpack.task_manager.poll_interval`>> setting, though this will add additional load to both {kib} and {es} instances in the cluster, as they will peform more queries.


[float]
[[task-manager-health-tasks-run-late]]
=== Tasks Run Late

*Symptoms*:

By far the most common symptom of an underlying problem in Task manager is that tasks appear to be running late.
For instance, recurring Tasks might run at an inconsistent cadence, or long after their scheduled time.

*Resolution*:

By default {kib} polls for tasks at a rate of 10 tasks every 3 seconds.

This means that if there are many tasks that have been scheduled to run at the same time, pending tasks will queue in {es}, and be pulled 10 at a time from the queue at 3 second intervals. The consequence of which is that only 10 tasks can be run concurrently per {kib} instance.

As a result of these capacity limitations tasks can run late.
We refer to this sort of delay as _drift_, the underlying cause of which vary from deployment to deployment, and for that reason there are no hard and fast rules to addressing them.

A possible cause for both consistent and inconsistent _drift_ is *an excess of concurrent tasks* relative to the available capacity of {kib} instances in the cluster, which can be addressed by expanding the throughput of the cluster.
An additional possible cause for consistent _drift_ is that *Long running tasks* may overrun their scheduled cadence. Such a scenario would ocurr if a task is scheduled to execute every 2 seconds, but takes 4 seconds to complete, in which case it will consistenly suffer from a _drift_ of, at least, 2 seconds.

There are several options for addressing _drift_ in the system, each with their own considerations
Before we dig deeper into diagnosing the underlying cause, it is worth understanding the options on the table:

[float]
[[task-manager-health-resolution-scale-horizontally]]
==== Scale Horizontaly

At times the sustainbale approach might be to expand the throughput of your cluster by provisioning additional {kib} instances.
By default, each additional {kib} instance will add an additional 10 tasks that your cluster can run concurrenctly, but you can also scale each {kib} instance vertically, if your dignosis indicates they can handle the additional workload.

[float]
[[task-manager-health-resolution-scale-vertically]]
==== Scale Vertically

Other times it, might be preferable to increase the throughput of individual {kib} instances.

Tweak the *Max Workers* via the <<task-manager-settings,`xpack.task_manager.max_workers`>> setting, which would allow each {kib} to pull a higher number of tasks per interval, but keep in mind that this could impact the performance of each {kib} instance as their workload would be higher.

Tweak the *Poll Interval* via the <<task-manager-settings,`xpack.task_manager.poll_interval`>> setting, which would allow each {kib} to pull scheduled tasks at a higher rate, but keep in mind that this could impact the performance of the {es} cluster *as their workload would be higher*.

[float]
==== Diagnosing A Root Cause

The following is a step-by-step guide to making sense of the output from the <<task-manager-health-monitoring>> endpoint. 

Retrieve the latest monitored health stats of a {kib} instance Task Manager:

[source,sh]
--------------------------------------------------
$ curl -X GET api/task_manager/_health
--------------------------------------------------
// KIBANA

The API returns the following:

[source,json]
--------------------------------------------------
{
  "id": "15415ecf-cdb0-4fef-950a-f824bd277fe4",
  "timestamp": "2021-02-16T11:38:10.077Z",
  "status": "OK",
  "last_update": "2021-02-16T11:38:09.934Z",
  "stats": {
    "configuration": {
      "timestamp": "2021-02-16T11:29:05.055Z",
      "value": {
        "request_capacity": 1000,
        "max_poll_inactivity_cycles": 10,
        "monitored_aggregated_stats_refresh_rate": 60000,
        "monitored_stats_running_average_window": 50,
        "monitored_task_execution_thresholds": {
          "default": {
            "error_threshold": 90,
            "warn_threshold": 80
          },
          "custom": {}
        },
        "poll_interval": 3000,
        "max_workers": 10
      },
      "status": "OK"
    },
    "runtime": {
      "timestamp": "2021-02-16T11:38:09.934Z",
      "value": {
        "polling": {
          "last_successful_poll": "2021-02-16T11:38:09.934Z",
          "last_polling_delay": "2021-02-16T11:29:05.053Z",
          "duration": {
            "p50": 13,
            "p90": 128,
            "p95": 143,
            "p99": 168
          },
          "claim_conflicts": {
            "p50": 0,
            "p90": 0,
            "p95": 0,
            "p99": 0
          },
          "claim_mismatches": {
            "p50": 0,
            "p90": 0,
            "p95": 0,
            "p99": 0
          },
          "result_frequency_percent_as_number": {
            "Failed": 0,
            "NoAvailableWorkers": 0,
            "NoTasksClaimed": 80,
            "RanOutOfCapacity": 0,
            "RunningAtCapacity": 0,
            "PoolFilled": 20
          }
        },
        "drift": {
          "p50": 99,
          "p90": 1245,
          "p95": 1845,
          "p99": 2878
        },
        "load": {
          "p50": 0,
          "p90": 0,
          "p95": 10,
          "p99": 20
        },
        "execution": {
          "duration": {
            "alerting:.index-threshold": {
              "p50": 95,
              "p90": 1725,
              "p95": 2761,
              "p99": 2761
            },
            "alerting:xpack.uptime.alerts.monitorStatus": {
              "p50": 149,
              "p90": 1071,
              "p95": 1171,
              "p99": 1171
            },
            "actions:.index": {
              "p50": 166,
              "p90": 166,
              "p95": 166,
              "p99": 166
            }
          },
          "result_frequency_percent_as_number": {
            "alerting:.index-threshold": {
              "Success": 100,
              "RetryScheduled": 0,
              "Failed": 0,
              "status": "OK"
            },
            "alerting:xpack.uptime.alerts.monitorStatus": {
              "Success": 100,
              "RetryScheduled": 0,
              "Failed": 0,
              "status": "OK"
            },
            "actions:.index": {
              "Success": 10,
              "RetryScheduled": 0,
              "Failed": 90,
              "status": "error"
            }
          }
        }
      },
      "status": "OK"
    },
    "workload": {
      "timestamp": "2021-02-16T11:38:05.826Z",
      "value": {
        "count": 26,
        "task_types": {
          "alerting:.index-threshold": {
            "count": 2,
            "status": {
              "idle": 2
            }
          },
          "actions:.index": {
            "count": 14,
            "status": {
              "idle": 2,
              "running": 2,
              "failed": 10
            }
          },
          "alerting:xpack.uptime.alerts.monitorStatus": {
            "count": 10,
            "status": {
              "idle": 10
            }
          },
        },
        "schedule": [
          ["10s", 2],
          ["1m", 2],
          ["60s", 2],
          ["5m", 2],
          ["60m", 4]
        ],
        "overdue": 0,
        "estimated_schedule_density": [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 3, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0]
      },
      "status": "OK"
    }
  }
}
--------------------------------------------------

[float]
[[task-manager-health-evaluate-the-configuration]]
==== Evaluate the Configuration

*Theory*:
Perhaps {kib} is configured to poll for tasks at a reduced rate?

*Diagnosis*:
Evaluating the health stats above, we can see the following output under `stats.configuration.value`:

[source,json]
--------------------------------------------------
{
    "request_capacity": 1000,
    "max_poll_inactivity_cycles": 10,
    "monitored_aggregated_stats_refresh_rate": 60000,
    "monitored_stats_running_average_window": 50,
    "monitored_task_execution_thresholds": {
        "default": {
            "error_threshold": 90,
            "warn_threshold": 80
        },
        "custom": {}
    },
    "poll_interval": 3000, # <1>
    "max_workers": 10 # <2>
}
--------------------------------------------------
<1> the `poll_interval` setting is configured to the default value of 3000 milliseconds
<2> the `max_workers` setting is configured to the default value of 10 workers

We can infer from this output that the {kib} instance is polling for work every 3 seconds and has the capacity to run 10 concurrent tasks.

Hypothetically, lets suppose the output under `stats.configuration.value` was the following:

[source,json]
--------------------------------------------------
{
    "request_capacity": 1000,
    "max_poll_inactivity_cycles": 10,
    "monitored_aggregated_stats_refresh_rate": 60000,
    "monitored_stats_running_average_window": 50,
    "monitored_task_execution_thresholds": {
        "default": {
            "error_threshold": 90,
            "warn_threshold": 80
        },
        "custom": {}
    },
    "poll_interval": 60000, # <1>
    "max_workers": 1 # <2>
}
--------------------------------------------------
<1> the `poll_interval` setting is configured to a value of 60000 milliseconds, far higher than the default
<2> the `max_workers` setting is configured to a value of 1 worker, far lower than the default

We can infer from this output that the {kib} instance is only polling for work once a minute and even then, it will only pick up one task at a time. This throughput is unlikely to support mission critical services such as Alerting or Reporting, as it means tasks will usually run late.

There are two possible reasons for such a configuration:

The first, is that these settings have been configured manually, which can be resolved by reconfiguring these settings.
For details on reconfiguring these settings, see <<task-manager-settings-kb, Task Manager Settings>>.

The second, is that {kib} has reduced its own throughput in reaction to excessive load on the {es} cluster.
{kib} Task Manager is equipped with a reactive self-healing mechanism, where by it reduces the rate at which it polls for work in response to an increase in errors caused by queries against {es}.

This scenario can be validated by evaluating the {kib} Server Log and looking for messages such as:

[source]
--------------------------------------------------
Max workers configuration is temporarily reduced after Elasticsearch returned 25 "too many request" error(s).
--------------------------------------------------

In such a case a deeper investigation into the high error rate experienced by the {es} cluster is required.

[float]
[[task-manager-health-evaluate-the-runtime]]
==== Evaluate the Runtime

[[task-manager-health-evaluate-the-runtime-polling]]
*Theory*:
Perhaps {kib} is not actually polling as frequently as it should?

*Diagnosis*:
Evaluating the health stats above, we can see the following output under `stats.runtime.value.polling`:

[source,json]
--------------------------------------------------
{
    "last_successful_poll": "2021-02-16T11:38:09.934Z", # <1>
    "last_polling_delay": "2021-02-16T11:29:05.053Z",
    "duration": { # <2>
        "p50": 13,
        "p90": 128,
        "p95": 143,
        "p99": 168
    },
    "claim_conflicts": { # <3>
        "p50": 0,
        "p90": 0,
        "p95": 0,
        "p99": 2
    },
    "claim_mismatches": {
        "p50": 0,
        "p90": 0,
        "p95": 0,
        "p99": 0
    },
    "result_frequency_percent_as_number": { # <4>
        "Failed": 0,
        "NoAvailableWorkers": 0,
        "NoTasksClaimed": 80,
        "RanOutOfCapacity": 0,
        "RunningAtCapacity": 0,
        "PoolFilled": 20
    }
}
--------------------------------------------------
<1> ensure the last successful polling cycle was completed recently, no more than a couple of multiples of `poll_interval` in the past
<2> ensure the duration of polling cycles is usually below 100ms or so, longer durations are possible, but unexpected
<3> ensure {kib} instances in the cluster are not encountering a high rate of version conflicts
<4> ensure the majority of polling cycles result in positive outcomes, such as `RunningAtCapacity` or `PoolFilled`

We can infer from this output that the {kib} instance is polling regularly.

We can assess this by comparing the `last_successful_poll` to the `timestamp` (value of `2021-02-16T11:38:10.077Z`) at the root, where we can see the last polling cycle took place 1 second before the monitoring stats were exposed by the Health Monitoring endpoint.
Additionally, the `p50` of the `duration`, shows us that at least 50% of polling cycles take, at most, 13 millisconds to complete.

Evaluating the `result_frequency_percent_as_number`, we can tell that 80% of the polling cycles completed without claiming any tasks (suggesting that there aren't any overdue tasks) and 20% completed with Task manager claiming tasks which were then executed.
We can also see that none of the polling cycles have ended up occupying all of the available workers, as `RunningAtCapacity` has a frequency of 0%, suggesting there is enough capacity in Task Manager to handle the workload.

It is worth noting that all of these stats are tracked as a running average, which means that they give us a snapshot of a period of time (by default we track up to 50 cycles), rather than a complete history throughout time.

Hypothetically, lets suppose the output under `stats.runtime.value.polling.result_frequency_percent_as_number` was the following:

[source,json]
--------------------------------------------------
{
  "Failed": 30, # <1>
  "NoAvailableWorkers": 20, # <2>
  "NoTasksClaimed": 10,
  "RanOutOfCapacity": 10, # <3>
  "RunningAtCapacity": 10, # <4>
  "PoolFilled": 20
}
--------------------------------------------------
<1> 30% of polling cycles failed, which is a high rate
<2> 20% of polling cycles are skipped as Task Manager has no capacity left to run tasks
<3> 10% of polling cycles result in Task Manager claiming more tasks than it has capacity to run
<4> 10% of polling cycles result in Task Manager claming precisely as many tasks as it has capacity to run

We can infer from this output that {kib} Task Manager is not healthy, as the failure rate is high, and Task Manager is fetching tasks it has no capacity to run.
Analyzing the {kib} Server Log should reveal the underlying issue causing the high error rate and capacity issues.

The high `NoAvailableWorkers` rate of 20% suggests that there are many tasks running for durations longer than the `poll_interval`.
For details on analyzing long task execution durations, see the <<task-manager-health-evaluate-the-runtime-long-running-task,long running tasks>> theory.

[[task-manager-health-evaluate-the-runtime-insufficient-workload]]
*Theory*:
Perhaps {kib} is polling as frequently as it should, but that isn't often enough to keep up with the workload?

*Diagnosis*:
Evaluating the health stats above, we can see the following output of `drift` and `load` under `stats.runtime.value`:

[source,json]
--------------------------------------------------
{
  "drift": { # <1>
    "p50": 99,
    "p90": 1245,
    "p95": 1845,
    "p99": 2878
  },
  "load": { # <2>
    "p50": 0,
    "p90": 0,
    "p95": 10,
    "p99": 20
  },
}
--------------------------------------------------
<1> drift shows us that at least 95% tasks are running within 2 seconds of their scheduled time
<2> load shows us that Task Manager is idle at least 90% of the time, and never uses more than 20% of it's available workers

We can infer from these stats that this {kib} has more capacity than it needs, and hence any delays we might be experiencing are not due to capacity concerns.

Hypothetically, lets suppose the output of `drift` and `load` was the following:

[source,json]
--------------------------------------------------
{
  "drift": { # <1>
    "p50": 2999,
    "p90": 3845,
    "p95": 3845.75,
    "p99": 4078
  },
  "load": { # <2>
    "p50": 80,
    "p90": 100,
    "p95": 100,
    "p99": 100
  }
}
--------------------------------------------------
<1> drift shows us that all tasks are running 3 to 4 seconds after their scheduled time
<2> load shows us that at least half of the time Task Manager is running at a load of 80%

We can infer from these stats that this {kib} is running at capacity much of the time, as indicated by the fact that the `p90` of `load` is at 100%, and the `p50` is also quite high at 80%.
That said, it also reveals that tasks are not being run much after their scheduled time, as a `poll_interval` of `3000` milliseconds would often experience a consistent _drift_ of somewhere in between `0` and `3000` milliseconds.

Seeing a `p50 drift` of `2999` suggests that there is room for improvmenet, and we could benefit from a higher throughput.
This could be achieved by scaling either <<task-manager-health-resolution-scale-horizontally, horizontally>> or <<task-manager-health-resolution-scale-vertically, vertically>>.

In the hypothetical scenario above, it would be worth experimenting with both options.
If your {kib} instances have the capacity for higher resource utilization, for instance, it might be easiest to start by scaling vertically.
If, on the other hand, your {kib} instances are already experiencing high resource utilization, then it might be better to scale horizontally by provisioning an additional {kib} instance.

By <<task-manager-health-evaluate-the-workload, evaluating the Workload>> it is possible to asses the scale that the system is trying to handle.

[[task-manager-health-evaluate-the-runtime-long-running-task]]
*Theory*:
Perhaps tasks aren't "running late" so much as "running for too long"?

*Diagnosis*:

Diagnosing the theory that {kib} Task Manager has <<task-manager-health-evaluate-the-runtime-insufficient-workload,insufficient throughtput to handle the scheduled workload>> theorized a hypothetical scenario where both _drift_ and _load_ are unusually high.

Suppose an alternate scenario, where `drift` is high, but `load` is not, such as the following:

[source,json]
--------------------------------------------------
{
    "drift": { # <1>
        "p50": 32999,
        "p90": 83845,
        "p95": 90328,
        "p99": 123845
    },
    "load": { # <2>
        "p50": 40,
        "p90": 75,
        "p95": 80,
        "p99": 100
    }
}
--------------------------------------------------
<1> drift shows us that most (if not all) tasks are running at least 32 seconds too late
<2> load shows us that, for the most part, we have capacity to run more concurrent tasks than we are

In the scenario above we can see that tasks are in fact being run far too late, but we have sufficient capacity to run more concurrent tasks.
A high capacity allows {kib} to run multiple different tasks concurrently, but it does not allow {kib} to run multiple instances of the same task concurrently.

This means that if a task is configured to run at a specified schedule, but executing the tasks takes longer than the cadence of that schedule, then that task will always overrun its schedule and experience a high _drift_.

Evaluating the health stats in out hypothetical scenario, we can see the following output under `stats.runtime.value.execution.duration`:

[source,json]
--------------------------------------------------
{
  "alerting:.index-threshold": { # <1>
    "p50": 95,
    "p90": 1725,
    "p95": 2761,
    "p99": 2761
  },
  "alerting:.es-query": { # <2>
    "p50": 7149,
    "p90": 40071,
    "p95": 45282,
    "p99": 121845
  },
  "actions:.index": {
    "p50": 166,
    "p90": 166,
    "p95": 166,
    "p99": 166
  }
}
--------------------------------------------------
<1> 50% of the tasks backing `Index Threshold Alerts` complete in less than 100 milliseconds
<2> 50% of the tasks backing `ES Query Alerts` complete in 7 seconds, but at least 10% take longer than 40 seconds!

We can infer from these stats that the high _drift_ the {kib} Task Manager is experiencing is most likely due to long running ES Query Alerts that are running for a long time.

Resolving this issue is context dependent and would change from case to case.
In the hypothetical example above, most likely, this would be resolved by replacing the ES Query in the Alert with a faster one, or improving the {es} throughput to speed up the exiting query.

[[task-manager-health-evaluate-the-runtime-fail-rate]]
*Theory*:
Perhaps tasks aren't "running late" so much as "take multiple attempts to succeed"?

*Diagnosis*:

A high error rate could cause a task to appear to run late, when in fact it runs on time, but experiences a high failure rate.

Evaluating the health stats above, we can see the following output under `stats.runtime.value.execution.result_frequency_percent_as_number`:

[source,json]
--------------------------------------------------
{
  "alerting:.index-threshold": { # <1>
    "Success": 100,
    "RetryScheduled": 0,
    "Failed": 0,
    "status": "OK"
  },
  "alerting:xpack.uptime.alerts.monitorStatus": {
    "Success": 100,
    "RetryScheduled": 0,
    "Failed": 0,
    "status": "OK"
  },
  "actions:.index": { # <2>
    "Success": 8,
    "RetryScheduled": 0,
    "Failed": 92,
    "status": "error" # <3>
  }
}
--------------------------------------------------
<1> 100% of the tasks backing `Index Threshold Alerts` successfully complete
<2> 92% of the tasks backing `ES Index Actions` fail to complete
<3> the tasks backing `ES Index Actions` have exceeded the default `monitored_task_execution_thresholds` _error_ configuration

We can infer from these stats that most `actions:.index` tasks, which back the `ES Index` {kib} action, are failing a lot.
Resolving that would require deeper investigation into the {kib} Server Log, where the exact errors would be logged, and addressing the specific errors identified in the logs.

[float]
[[task-manager-health-evaluate-the-workload]]
==== Evaluate the Workload

Predicting the required throughout a deplyment might need to support {kib} Task Management is difficult, as features can schedule an unpredictable number of tasks at a variety of scheduled cadences.

That said, <<task-manager-health-monitoring>> provies statistics that make it easier to monitor the adequecy of the existing throughput.
By evaluating the workload estimates can be made about the required throughput, and these can be used to estimate the required scale.

Evaluating the health stats above, we can see the following output under `stats.workload.value`:

[source,json]
--------------------------------------------------
{
  "count": 26, # <1>
  "task_types": {
    "alerting:.index-threshold": {
      "count": 2, # <2>
      "status": {
        "idle": 2
      }
    },
    "actions:.index": {
      "count": 14,
      "status": {
        "idle": 2,
        "running": 2,
        "failed": 10 # <3>
      }
    },
    "alerting:xpack.uptime.alerts.monitorStatus": {
      "count": 10,
      "status": {
        "idle": 10
      }
    },
  },
  "schedule": [ # <4>
    ["10s", 2],
    ["1m", 2],
    ["90s", 2],
    ["5m", 8]
  ],
  "overdue": 0, # <5>
  "estimated_schedule_density": [  # <6>
    0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
    0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
    0, 3, 0, 0, 0, 1, 0, 1, 0, 1,
    0, 0, 0, 1, 0, 0, 1, 1, 1, 0
  ]
}
--------------------------------------------------
<1> there are 26 tasks in the system, including regular tasks, recurring tasks and failed tasks
<2> there are 2 of the tasks backing `Index Threshold Alerts` both of which are `idle`, meaning they are scheduled to run at some point in the future
<3> of the 14 tasks backing the `ES Index` Action, 10 have failed and 2 are running at this very moment
<4> a histogram of all scheduled recuring tasks shows that there are 2 tasks runing every 10 seconds, 2 once a minute etc.
<5> there are no tasks _overdue_, which means all that task that _should_ have run by now have ran
<6> a histogram of the tasks scheduled to run in the next 20 polling cycles, across the entire cluster

The `workload` section summarizes the work load across the cluster, listing the tasks in the system, their types, schedules and what their current status is.

We can infer from these stats that the likelyhood of there being more than a handful of tasks per polling cycle is low, suggesting that a default deployment shoud suffice. We can make this assesment based on the fact that the estimated schedule density is low, and there don't seem to be many tasks in the system.

Hypothetically, lets suppose the output of `stats.workload.value` looked something like this:

[source,json]
--------------------------------------------------
{
  "count": 2191, # <1>
  "task_types": {
    "alerting:.index-threshold": {
      "count": 202,
      "status": {
        "idle": 183,
        "claiming": 2,
        "running": 19
      }
    },
    "alerting:.es-query": {
      "count": 225,
      "status": {
        "idle": 225,
      }
    },
    "actions:.index": {
      "count": 89,
      "status": {
        "idle": 24,
        "running": 2,
        "failed": 63
      }
    },
    "alerting:xpack.uptime.alerts.monitorStatus": {
      "count": 87,
      "status": {
        "idle": 74,
        "running": 13
      }
    },
  },
  "schedule": [ # <2>
    ["10s", 38],
    ["1m", 101],
    ["90s", 55],
    ["5m", 89],
    ["20m", 62],
    ["60m", 106],
    ["1d", 61]
  ],
  "overdue": 0, # <5>
  "estimated_schedule_density": [  # <3>
    10, 1, 0, 10, 0, 20, 0, 1, 0, 1,
    9, 0, 3, 10, 0, 0, 10, 10, 7, 0,
    0, 31, 0, 12, 16, 31, 0, 10, 0, 10,
    3, 22, 0, 10, 0, 2, 10, 10, 1, 0
  ]
}
--------------------------------------------------
<1> there are 2191 tasks in the system
<2> the scheduled tasks are distributed across a variety of cadences
<3> the schedule density shows that we predict a need for more than the default 10 concurrent tasks

We can infer several important attributes of our workload from this output.

The first attribute is that there are many tasks in our system and ensuring these tasks run on their scheduled cadence will require attention to the {kib} Task Management throughput.

The second attribute is that if we focus in on the high frequency tasks (tasks that recure at a cadence of a couple of minutes and below), we have a need to support a throughput of _aproximately_ 400 tasks per minute (38 every 10 seconds + 101 every minute + 55 every 90 seconds).
Assessing the medium frequency tasks (tasks that recure at a cadence of an hour or less), we have a need to support an additional throughput of over 2000 tasks per hour (89 every 5 minutes, + 62 every 20 minutes + 106 each hour), which we can naively count as an additional 30 to 40 tasks per minute.
These _rough_ calculations give us a lower bound to the required througput which is that of _at least_ 440 tasks per minute to ensure recurring tasks are excuted, more or less, at their scheduled time. This throughput doesn't account for non recurring tasks that might have been scheduled, nor does it account tasks (recurring or otherwise) that might be scheduled in the future.

The third attribute, based on the estimated schedule density, is that there are cycles that are due to run upwards of 31 tasks concurrently, but in between those scheduled tasks there are cycles where we expect a need for few workers to meet the demand. These cycles with low demand could  likely handle a short spike in fresh tasks that might get scheduled (such as a spike in {kib} Actions scheduled by the Alerting framework).

Given these inferred attributes, it would be safe to assume that a single {kib} instance with default settings would not provide the required throughput, but it is quite possible that scaling horizontally to a few more nodes will.
