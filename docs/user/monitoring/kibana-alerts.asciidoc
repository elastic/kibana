[role="xpack"]
[[kibana-alerts]]
= {kib} Stack Monitoring Alerting

The {stack} {monitor-features} provide out-of-the box
<<alerting-getting-started,{kib} alerting>> to notify you of
potential issues in the {stack}. These rules are preconfigured based on the
best practices recommended by Elastic, but you can still tailor them to meet your 
specific needs.

[role="screenshot"]
image::user/monitoring/images/monitoring-kibana-alerts.png["Kibana alerts in the Stack Monitoring app"]

These preconfigured {kib} rules are created automatically when you open the *{stack-monitor-app}*. 
They are initially configured to detect and notify on various conditions across your monitored clusters. 
You can view notifications for: *Cluster health*, *Resource utilization*, and *Errors and exceptions* for {es} in realtime.

NOTE: The default watcher based "cluster alerts" for {stack-monitor-app} have been recreated as rules in kibana alerting.
This causes the existing watcher email action `monitoring.cluster_alerts.email_notifications.email_address` to no longer work.
The default action for all {stack-monitor-app} rules is to write to {kib} logs and disply a notification in the UI.

[role="screenshot"]
image::user/monitoring/images/monitoring-kibana-alerting-notification.png["Kibana alerting notifications in the Stack Monitoring app"]

NOTE: To review and modify all available rules, use *Enter setup mode* on the *Cluster overview* page in *{stack-monitor-app}*

[role="screenshot"]
image::user/monitoring/images/monitoring-kibana-alerting-setup-mode.png["Kibana alerting modify rules in the Stack Monitoring app"]

[discrete]
[[kibana-alerting-cpu-threshold]]
== CPU usage threshold

This alert is triggered when an {es} node runs a consistently high CPU load. By
default, the trigger condition is set at 85% or more averaged over the last 5
minutes. The alert is grouped across all the nodes of the cluster by running
checks on a schedule time of 1 minute with a re-notify interval of 1 day. 

[discrete]
[[kibana-alerting-disk-usage-threshold]]
== Disk usage threshold

This alert is triggered when an {es} node is nearly at disk capacity. By
default, the trigger condition is set at 80% or more averaged over the last 5
minutes. The alert is grouped across all the nodes of the cluster by running
checks on a schedule time of 1 minute with a re-notify interval of 1 day. 

[discrete]
[[kibana-alerting-jvm-memory-threshold]]
== JVM memory threshold

This alert is triggered when an {es} node uses high amount of JVM memory. By
default, the trigger condition is set at 85% or more averaged over the last 5
minutes. The alert is grouped across all the nodes of the cluster by running
checks on a schedule time of 1 minute with a re-notify interval of 1 day. 

[discrete]
[[kibana-alerting-missing-monitoring-data]]
== Missing monitoring data

This alert is triggered when an {es} node stops sending
monitoring data. By default, the trigger condition is set to missing for 15 minutes
looking back 1 day. The alert is grouped across all the {es} nodes of the cluster by running
checks on a schedule time of 1 minute with a re-notify interval of 6 hours. 

[discrete]
[[kibana-alerting-thread-pool-rejections]]
== Thread pool rejections (search/write)

This alert is triggered when an {es} node experiences thread pool rejections. By
default, the trigger condition is set at 300 or more over the last 5
minutes. The alert is grouped across all the nodes of the cluster by running
checks on a schedule time of 1 minute with a re-notify interval of 1 day. 
Thresholds can be set independently for `search` and `write` type rejections.

[discrete]
[[kibana-alerting-ccr-read-exceptions]]
== CCR read exceptions

This alert is triggered if a read exception has been detected on any of the 
replicated {es} clusters. The trigger condition is met if 1 or more read exceptions 
are detected in the last hour. The alert is grouped across all replicated clusters 
by running checks on a schedule time of 1 minute with a re-notify interval of 6 hours. 

[discrete]
[[kibana-alerting-large-shard-size]]
== Large shard size

This alert is triggered if a large average shard size (across associated primaries) is found on any of the 
specified index patterns in a {es} cluster. The trigger condition is met if an index's average shard size is 
55gb or higher in the last 5 minutes. The alert is grouped across all indices that match 
the default pattern of `-.*` by running checks on a schedule time of 1 minute with a re-notify 
interval of 12 hours.

[discrete]
[[kibana-alerting-cluster-alerting]]
== Cluster alerting

These alerts summarize the current status of your {stack}. You can drill down into the metrics 
to view more information about your cluster and specific nodes, instances, and indices.

An action will be triggered if any of the following conditions are met within the last minute:

* {es} cluster health status is yellow (missing at least one replica)
or red (missing at least one primary).
* {es} version mismatch. You have {es} nodes with
different versions in the same cluster.
* {kib} version mismatch. You have {kib} instances with different
versions running against the same {es} cluster.
* Logstash version mismatch. You have Logstash nodes with different
versions reporting stats to the same monitoring cluster.
* {es} nodes changed. You have {es} nodes that were recently added or removed.
* {es} license expiration. The cluster's license is about to expire.
+
--
If you do not preserve the data directory when upgrading a {kib} or
Logstash node, the instance is assigned a new persistent UUID and shows up
as a new instance
--
* Subscription license expiration. When the expiration date
approaches, you will get notifications with a severity level relative to how
soon the expiration date is:
  ** 60 days: Informational alert
  ** 30 days: Low-level alert
  ** 15 days: Medium-level alert
  ** 7 days: Severe-level alert
+
The 60-day and 30-day thresholds are skipped for Trial licenses, which are only
valid for 30 days.

NOTE: Some action types are subscription features, while others are free.
For a comparison of the Elastic subscription levels, see the alerting section of
the {subscriptions}[Subscriptions page].
