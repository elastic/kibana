[[stability]]
== Stability

Ensure your feature will work under all possible {kib} scenarios.

[discrete]
=== Environmental configuration scenarios

* Cloud
** Does the feature work on *cloud environment*?
** Does it create a setting that needs to be exposed, or configured
differently than the default, on Cloud? (whitelisting of certain
settings/users? Ref:
https://www.elastic.co/guide/en/cloud/current/ec-add-user-settings.html
,
https://www.elastic.co/guide/en/cloud/current/ec-manage-kibana-settings.html)
** Is there a significant performance impact that may affect Cloud
{kib} instances?
** Does it need to be aware of running in a container? (for example
monitoring)
* Multiple {kib} instances
** Pointing to the same index
** Pointing to different indexes
*** Should make sure that the {kib} index is not hardcoded anywhere.
*** Should not be storing a bunch of stuff in {kib} memory.
*** Should emulate a high availability deployment.
*** Anticipating different timing related issues due to shared resource
access.
*** We need to make sure security is set up in a specific way for
non-standard {kib} indices. (create their own custom roles)
* {kib} running behind a reverse proxy or load balancer, without sticky
sessions. (we have had many discuss/SDH tickets around this)
* If a proxy/loadbalancer is running between ES and {kib}

[discrete]
=== Kibana.yml settings

* Using a custom {kib} index alias
* When optional dependencies are disabled
** Ensure all your required dependencies are listed in kibana.json
dependency list!

[discrete]
=== Test coverage

Testing UI code is hard. Here are our heuristics for maintaining Kibana's quality with tests.

**Priority 1:** Our first priority is to minimize the total number of poor user experiences caused by bugs, and to prevent bugs that can cause harm to the user. "Harm" includes direct problems like data loss and data entering a bad state, as well as indirect problems like making a poor business decision based on misinformation presented by the UI.

**Priority 2:** Our second priority is to define all correct UX behavior and guard against any UX regressions.

**We are constrained by time.** We don’t have enough time to test all possible input and output permutations of the UX. Just think of the different ways a form can be interacted with and respond to input. We must spend our time prudently to address the above priorities.

With each feature we ship, we have a **hard requirement** for test coverage of common library code, popular UIs, critical code (e.g. payload de/serialization, server-side APIs), and critical features (e.g. visualization, security). We have a **soft requirement** for test coverage of everything else.

**We can make an exception** for the soft requirement if there’s a business case for shipping a feature on time. This requires buy-in from a lead and a PM, and a written explanation in the PR to support forensics. Any time we make an exception we immediately follow up with a separate PR to add the missing test coverage. 

**Bug fixes:** With each bug we fix, we add a test to cover it.

**Measuring quality during development:** During development, test coverage measurement is subjective and manual, based on our understanding of the feature. Code coverage reports indicate possible gaps, but they don’t reliably measure UX coverage -- which is what we really care about.

**Measuring quality after code ships:** After code ships, subsequent bug reports indicate we need to improve our sense of gauging adequate test coverage. If severe or many bug reports arise, then we’ll retro and identify the gaps in our culture and process.

==== Guiding questions for adding test coverage

* Is core library code and critical code covered by unit tests?
* Are server-side APIs covered by API integration tests?
* Have you identified critical features and covered their expected behavior with functional and integration tests?
* Is the expected UX covered by functional and integration tests, including happy paths and failure cases?

[discrete]
=== Browser coverage

Refer to the list of browsers and OS {kib} supports:
https://www.elastic.co/support/matrix

Does the feature work efficiently on the list of supported browsers? 

[discrete]
=== Upgrade and Migration scenarios

* Does the feature affect old indices or saved objects?
* Has the feature been tested with {kib} aliases?
* Read/Write privileges of the indices before and after the
upgrade?